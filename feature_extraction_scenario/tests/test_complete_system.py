#!/usr/bin/env python3
"""Complete System Test for Multi-modal Anomaly Detection"""

import torch
import torch.nn as nn
import numpy as np
import importlib
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
# Current script: .../feature_extraction_scenario/tests/test_complete_system.py
# Project root for imports (InProject): .../feature_extraction_scenario/../../
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

# æ¸…é™¤æ¨¡å—ç¼“å­˜
def clear_cache():
    modules_to_remove = []
    for module_name in sys.modules:
        if 'models' in module_name:
            modules_to_remove.append(module_name)
    
    for module_name in modules_to_remove:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    importlib.invalidate_caches()

def test_individual_modules():
    """æµ‹è¯•å„ä¸ªæ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("ğŸ”§ Testing Individual Modules...")
    
    clear_cache()
    
    # 1. æµ‹è¯• Transformer Encoder
    from feature_extraction_scenario.core_logic.models.base_model.transformer_encoder import TransformerEncoder
    transformer = TransformerEncoder(input_dim=128, hidden_dim=256)
    x = torch.randn(8, 32, 128)
    output = transformer(x)
    print(f"âœ… TransformerEncoder: {x.shape} -> {output.shape}")
    
    # 2. æµ‹è¯• User GNN
    from feature_extraction_scenario.core_logic.models.base_model.user_gnn import UserGNN
    gnn = UserGNN(input_dim=10, output_dim=128)
    node_features = torch.randn(50, 10)
    adj_matrix = torch.randint(0, 2, (50, 50)).float()
    gnn_output = gnn(node_features, adj_matrix)
    print(f"âœ… UserGNN: {node_features.shape} -> {gnn_output.shape}")
    
    # 3. æµ‹è¯• BERT Encoder
    from feature_extraction_scenario.core_logic.models.text_encoder.bert_module import BERTTextEncoder
    bert = BERTTextEncoder(output_dim=128)
    texts = ["Test email content"] * 8
    bert_output = bert(texts)
    print(f"âœ… BERTTextEncoder: {len(texts)} texts -> {bert_output.shape}")
    
    # 4. æµ‹è¯• LightGBM Branch
    from feature_extraction_scenario.core_logic.models.structure_encoder.lightgbm_branch import LightGBMBranch
    lgbm = LightGBMBranch(input_dim=20, output_dim=128)
    struct_features = torch.randn(8, 20)
    lgbm_output = lgbm(struct_features)
    print(f"âœ… LightGBMBranch: {struct_features.shape} -> {lgbm_output.shape}")
    
    # 5. æµ‹è¯• Attention Fusion
    from feature_extraction_scenario.core_logic.models.fusion.attention_fusion import AttentionFusion
    fusion = AttentionFusion(input_dims=[256, 128, 128, 128], embed_dim=128)
    modality_features = [
        torch.randn(8, 256),  # Transformer
        torch.randn(8, 128),  # GNN
        torch.randn(8, 128),  # BERT  
        torch.randn(8, 128),  # LightGBM
    ]
    fusion_output = fusion(modality_features)
    print(f"âœ… AttentionFusion: 4 modalities -> {fusion_output['fused_features'].shape}")
    
    # 6. æµ‹è¯• Classification Head
    from feature_extraction_scenario.core_logic.models.base_model.head import ClassificationHead
    head = ClassificationHead(input_dim=128, num_classes=2)
    head_output = head(fusion_output['fused_features'])
    print(f"âœ… ClassificationHead: {fusion_output['fused_features'].shape} -> logits: {head_output.shape}")
    
    return True

def test_simplified_multimodal_model():
    """æµ‹è¯•ç®€åŒ–çš„å¤šæ¨¡æ€æ¨¡å‹"""
    print("\nğŸš€ Testing Simplified Multi-modal Model...")
    
    clear_cache()
    
    # æ‰‹åŠ¨æ„å»ºç®€åŒ–çš„å¤šæ¨¡æ€æ¨¡å‹
    class SimplifiedMultiModalModel(nn.Module):
        def __init__(self):
            super().__init__()
            
            # å„æ¨¡æ€ç¼–ç å™¨
            from feature_extraction_scenario.core_logic.models.base_model.transformer_encoder import TransformerEncoder
            from feature_extraction_scenario.core_logic.models.base_model.user_gnn import UserGNN
            from feature_extraction_scenario.core_logic.models.text_encoder.bert_module import BERTTextEncoder
            from feature_extraction_scenario.core_logic.models.structure_encoder.lightgbm_branch import LightGBMBranch
            from feature_extraction_scenario.core_logic.models.fusion.attention_fusion import AttentionFusion
            from feature_extraction_scenario.core_logic.models.base_model.head import ClassificationHead
            
            self.transformer = TransformerEncoder(input_dim=128, hidden_dim=128)
            self.gnn = UserGNN(input_dim=10, output_dim=128)
            self.bert = BERTTextEncoder(output_dim=128)
            self.lgbm = LightGBMBranch(input_dim=20, output_dim=128)
            self.fusion = AttentionFusion(input_dims=[128, 128, 128, 128], embed_dim=128)
            self.head = ClassificationHead(input_dim=128, num_classes=2)
        
        def forward(self, inputs):
            # ç¼–ç å„æ¨¡æ€
            behavior_features = self.transformer(inputs['behavior_sequences'])  # [batch, 128]
            
            user_features = self.gnn(inputs['node_features'], inputs['adjacency_matrix'])
            user_features = user_features[:inputs['behavior_sequences'].shape[0]]  # åŒ¹é…batch_size
            
            text_features = self.bert(inputs['text_content'])  # [batch, 128]
            struct_features = self.lgbm(inputs['structured_features'])  # [batch, 128]
            
            # èåˆ
            modality_features = [behavior_features, user_features, text_features, struct_features]
            fusion_output = self.fusion(modality_features)
            
            # åˆ†ç±»
            head_output = self.head(fusion_output['fused_features'])
            
            return {
                'logits': head_output,
                'probabilities': torch.softmax(head_output, dim=1),
                'anomaly_scores': torch.softmax(head_output, dim=1)[:, 1],
                'confidence': torch.max(torch.softmax(head_output, dim=1), dim=1)[0],
                'fused_features': fusion_output['fused_features'],
                'fusion_weights': fusion_output.get('final_weights')
            }
    
    # åˆ›å»ºæ¨¡å‹å’Œæµ‹è¯•æ•°æ®
    model = SimplifiedMultiModalModel()
    
    batch_size = 8
    inputs = {
        'behavior_sequences': torch.randn(batch_size, 32, 128),
        'node_features': torch.randn(50, 10),
        'adjacency_matrix': torch.randint(0, 2, (50, 50)).float(),
        'text_content': [f"Test email {i}" for i in range(batch_size)],
        'structured_features': torch.randn(batch_size, 20)
    }
    
    print(f"  è¾“å…¥æ•°æ®:")
    print(f"    è¡Œä¸ºåºåˆ—: {inputs['behavior_sequences'].shape}")
    print(f"    ç”¨æˆ·ç‰¹å¾: {inputs['node_features'].shape}")
    print(f"    æ–‡æœ¬æ•°é‡: {len(inputs['text_content'])}")
    print(f"    ç»“æ„ç‰¹å¾: {inputs['structured_features'].shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(inputs)
    
    print(f"\n  è¾“å‡ºç»“æœ:")
    print(f"    logits: {outputs['logits'].shape}")
    print(f"    probabilities: {outputs['probabilities'].shape}")
    print(f"    anomaly_scores: {outputs['anomaly_scores'].shape}")
    print(f"    confidence: {outputs['confidence'].shape}")
    print(f"    fused_features: {outputs['fused_features'].shape}")
    
    # é¢„æµ‹ç»“æœ
    print(f"\n  é¢„æµ‹ç»“æœ:")
    print(f"    å¹³å‡å¼‚å¸¸åˆ†æ•°: {outputs['anomaly_scores'].mean().item():.4f}")
    print(f"    å¹³å‡ç½®ä¿¡åº¦: {outputs['confidence'].mean().item():.4f}")
    
    anomaly_count = (outputs['anomaly_scores'] > 0.5).sum().item()
    print(f"    å¼‚å¸¸æ£€æµ‹æ•°é‡: {anomaly_count}/{batch_size}")
    
    if outputs['fusion_weights'] is not None:
        weights = outputs['fusion_weights'].mean(dim=0)
        print(f"    æ¨¡æ€æƒé‡: Transformer={weights[0]:.3f}, GNN={weights[1]:.3f}, BERT={weights[2]:.3f}, LGBM={weights[3]:.3f}")
    
    # æµ‹è¯•æ¢¯åº¦
    model.train()
    outputs = model(inputs)
    loss = outputs['logits'].sum()
    loss.backward()
    print(f"    æ¢¯åº¦æµ‹è¯•: âœ… é€šè¿‡")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿå®Œæ•´æµ‹è¯•")
    print("=" * 50)
    
    try:
        # æµ‹è¯•å„ä¸ªæ¨¡å—
        if test_individual_modules():
            print("\nâœ… æ‰€æœ‰å•ä¸ªæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        
        # æµ‹è¯•å®Œæ•´ç³»ç»Ÿ
        if test_simplified_multimodal_model():
            print("\nğŸ‰ å®Œæ•´å¤šæ¨¡æ€ç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼")
            
        print("\n" + "=" * 50)
        print("ğŸ† ç³»ç»Ÿæµ‹è¯•å…¨éƒ¨å®Œæˆï¼")
        
        # æ€»ç»“
        print("\nğŸ“‹ ç³»ç»Ÿç»„ä»¶æ€»ç»“:")
        print("  âœ… TransformerEncoder - è¡Œä¸ºåºåˆ—å»ºæ¨¡")
        print("  âœ… UserGNN - ç”¨æˆ·å…³ç³»å›¾å»ºæ¨¡") 
        print("  âœ… BERTTextEncoder - æ–‡æœ¬å†…å®¹ç¼–ç ")
        print("  âœ… LightGBMBranch - ç»“æ„åŒ–ç‰¹å¾å¤„ç†")
        print("  âœ… AttentionFusion - å¤šæ¨¡æ€èåˆ")
        print("  âœ… ClassificationHead - å¼‚å¸¸æ£€æµ‹åˆ†ç±»")
        print("  âœ… SimplifiedMultiModalModel - å®Œæ•´ç³»ç»Ÿé›†æˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 