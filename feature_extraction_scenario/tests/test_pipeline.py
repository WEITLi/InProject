#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æµæ°´çº¿æµ‹è¯•è„šæœ¬
éªŒè¯dataset_pipeline.pyçš„åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""

import os
import sys
import time
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import List
import shutil

# è°ƒæ•´ sys.path ä»¥ä¾¿æ­£ç¡®å¯¼å…¥ core_logic æ¨¡å—
# å½“å‰è„šæœ¬ä½äº tests/ ç›®å½•ä¸‹, æˆ‘ä»¬éœ€è¦æ·»åŠ åˆ° feature_extraction_scenario çš„çˆ¶ç›®å½•
# .../feature_extraction_scenario/tests/test_pipeline.py
# æˆ‘ä»¬éœ€è¦ .../feature_extraction_scenario/../ (å³ InProject ç›®å½•)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(PROJECT_ROOT)

# ä»æ–°çš„ä½ç½®å¯¼å…¥ CERTDatasetPipeline
from feature_extraction_scenario.core_logic.dataset_pipeline import CERTDatasetPipeline

# å®šä¹‰æµ‹è¯•æ–‡ä»¶å’Œç›®å½•å¸¸é‡
TESTS_DIR = os.path.dirname(os.path.abspath(__file__))
SAMPLE_DATA_DIR = os.path.join(TESTS_DIR, 'sample_test_data')
FEATURE_EXTRACTION_SCENARIO_DIR = os.path.dirname(TESTS_DIR)
TEST_OUTPUT_DIR = os.path.join(FEATURE_EXTRACTION_SCENARIO_DIR, 'test_output')
TEST_DATA_MARKER_FILE = os.path.join(SAMPLE_DATA_DIR, '.test_data_created_by_pipeline')
BACKUP_DIR_ROOT = os.path.join(TESTS_DIR, 'backup_data') # Backup directory inside tests

def _ensure_dir(directory_path: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(directory_path, exist_ok=True)

def create_test_data():
    """
    åˆ›å»ºæµ‹è¯•ç”¨çš„CERTæ•°æ®æ–‡ä»¶åˆ° SAMPLE_DATA_DIR
    """
    print(f"ğŸ“ åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶åˆ°: {SAMPLE_DATA_DIR}")
    _ensure_dir(SAMPLE_DATA_DIR)
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨çœŸå®æ•°æ® (æ­¤é€»è¾‘åœ¨æ­¤å¤„å¯èƒ½ä¸å¤ªé€‚ç”¨ï¼Œå› ä¸ºæµ‹è¯•æ•°æ®æ˜¯éš”ç¦»çš„)
    # å¯ä»¥è€ƒè™‘ç§»é™¤æˆ–è°ƒæ•´ï¼Œæš‚æ—¶ä¿ç•™ä½†è¾“å‡ºè°ƒæ•´
    real_data_files = ['email.csv', 'file.csv', 'http.csv', 'logon.csv', 'device.csv']
    # This check should ideally point to a shared, non-test data location if strict separation is needed.
    # For now, let's assume 'real' data is NOT in SAMPLE_DATA_DIR.
    
    # åˆ›å»ºæµ‹è¯•ç”¨æˆ·åˆ—è¡¨
    users = ['ACM2278', 'ACM1796', 'CMP2946', 'BTH8471', 'DYQ9624']
    pcs = ['PC-1234', 'PC-5678', 'PC-9012', 'PC-3456']
    
    # åŸºç¡€æ—¶é—´
    base_date = datetime(2024, 1, 1)
    
    # åˆ›å»ºé‚®ä»¶æ•°æ®
    email_data = []
    for week in range(3):
        for day in range(7):
            for user in users[:3]:
                current_date = base_date + timedelta(weeks=week, days=day, hours=np.random.randint(8, 18))
                email_data.append({
                    'date': current_date.strftime('%m/%d/%Y %H:%M:%S'), 'user': user, 'pc': np.random.choice(pcs),
                    'to': f'{np.random.choice(users)}@dtaa.com', 'cc': '', 'bcc': '', 'from': f'{user}@dtaa.com',
                    'size': np.random.randint(1000, 50000), 'content': f'Test email content from {user}', 'activity': 'Send'
                })
    save_test_data(pd.DataFrame(email_data), 'email.csv')
    print(f"   åˆ›å»º email.csv: {len(email_data)} æ¡è®°å½•")
    
    # åˆ›å»ºæ–‡ä»¶æ•°æ®
    file_data = []
    for week in range(3):
        for day in range(7):
            for user in users:
                current_date = base_date + timedelta(weeks=week, days=day, hours=np.random.randint(8, 18))
                file_data.append({
                    'date': current_date.strftime('%m/%d/%Y %H:%M:%S'), 'user': user, 'pc': np.random.choice(pcs),
                    'filename': f'document_{np.random.randint(1, 100)}.docx', 'content': f'File content from {user}',
                    'activity': np.random.choice(['file open', 'file copy', 'file write'])
                })
    save_test_data(pd.DataFrame(file_data), 'file.csv')
    print(f"   åˆ›å»º file.csv: {len(file_data)} æ¡è®°å½•")
    
    # åˆ›å»ºHTTPæ•°æ®
    http_data = []
    urls = ['https://www.company.com/reports', 'https://www.google.com/search', 'https://www.github.com/projects', 'https://www.stackoverflow.com/questions']
    for week in range(3):
        for day in range(7):
            for user in users:
                current_date = base_date + timedelta(weeks=week, days=day, hours=np.random.randint(8, 18))
                http_data.append({
                    'date': current_date.strftime('%m/%d/%Y %H:%M:%S'), 'user': user, 'pc': np.random.choice(pcs),
                    'url': np.random.choice(urls), 'content': f'Web page content for {user}', 'activity': 'www visit'
                })
    save_test_data(pd.DataFrame(http_data), 'http.csv')
    print(f"   åˆ›å»º http.csv: {len(http_data)} æ¡è®°å½•")
    
    # åˆ›å»ºç™»å½•æ•°æ®
    logon_data = []
    for week in range(3):
        for day in range(7):
            for user in users:
                for _ in range(np.random.randint(1, 3)):
                    current_date = base_date + timedelta(weeks=week, days=day, hours=np.random.randint(7, 9))
                    logon_data.append({
                        'date': current_date.strftime('%m/%d/%Y %H:%M:%S'), 'user': user, 'pc': np.random.choice(pcs), 'activity': 'Logon'
                    })
    save_test_data(pd.DataFrame(logon_data), 'logon.csv')
    print(f"   åˆ›å»º logon.csv: {len(logon_data)} æ¡è®°å½•")
    
    # åˆ›å»ºè®¾å¤‡æ•°æ®
    device_data = []
    for week in range(3):
        for day in range(7):
            for user in users[:2]:
                current_date = base_date + timedelta(weeks=week, days=day, hours=np.random.randint(10, 16))
                device_data.append({
                    'date': current_date.strftime('%m/%d/%Y %H:%M:%S'), 'user': user, 'pc': np.random.choice(pcs),
                    'activity': 'Connect', 'content': f'USB device connected by {user}'
                })
    save_test_data(pd.DataFrame(device_data), 'device.csv')
    print(f"   åˆ›å»º device.csv: {len(device_data)} æ¡è®°å½•")
    
    # åˆ›å»ºå¿ƒç†æµ‹é‡æ•°æ®ï¼ˆå¯é€‰ï¼‰
    psycho_data = []
    for user in users:
        psycho_data.append({
            'user': user, 'O': np.random.uniform(0.3, 0.8), 'C': np.random.uniform(0.3, 0.8),
            'E': np.random.uniform(0.3, 0.8), 'A': np.random.uniform(0.3, 0.8), 'N': np.random.uniform(0.3, 0.8)
        })
    save_test_data(pd.DataFrame(psycho_data), 'psychometric.csv')
    print(f"   åˆ›å»º psychometric.csv: {len(psycho_data)} æ¡è®°å½•")
    
    # åˆ›å»ºanswersç›®å½•å’Œæ¶æ„ç”¨æˆ·æ ‡ç­¾ (åœ¨SAMPLE_DATA_DIRä¸‹)
    answers_dir = os.path.join(SAMPLE_DATA_DIR, 'answers')
    _ensure_dir(answers_dir)
    insiders_data = [
        {'user': 'ACM2278', 'scenario': 1, 'start_week': 1, 'end_week': 2},
        {'user': 'CMP2946', 'scenario': 2, 'start_week': 0, 'end_week': 1}
    ]
    save_test_data(pd.DataFrame(insiders_data), os.path.join('answers', 'insiders.csv')) # filename is relative to SAMPLE_DATA_DIR
    print(f"   åˆ›å»º answers/insiders.csv: {len(insiders_data)} ä¸ªæ¶æ„ç”¨æˆ·")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®æ ‡è®°æ–‡ä»¶
    with open(TEST_DATA_MARKER_FILE, 'w') as f:
        f.write(f"æµ‹è¯•æ•°æ®åˆ›å»ºæ—¶é—´: {datetime.now()}\\n")
        f.write(f"æ­¤æ–‡ä»¶æ ‡è®° SAMPLE_DATA_DIR ({SAMPLE_DATA_DIR}) ä¸­çš„æµ‹è¯•æ•°æ®çš„å­˜åœ¨\\n")
        test_files = ['email.csv', 'file.csv', 'http.csv', 'logon.csv', 'device.csv', 'psychometric.csv', 'answers/insiders.csv']
        for test_file in test_files:
            f.write(f"TEST_FILE:{test_file}\\n") # Store relative paths within SAMPLE_DATA_DIR
    
    print("âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ")
    return True

def save_test_data(df: pd.DataFrame, filename: str):
    """
    ä¿å­˜æµ‹è¯•æ•°æ®åˆ° SAMPLE_DATA_DIR å¹¶æ·»åŠ æ ‡è®°
    filename is relative to SAMPLE_DATA_DIR
    """
    full_path = os.path.join(SAMPLE_DATA_DIR, filename)
    _ensure_dir(os.path.dirname(full_path)) # Ensure subdirectory (like 'answers') exists
    
    df.to_csv(full_path, index=False)
    
    with open(full_path, 'r+') as f: # Open in r+ to prepend
        content = f.read()
        f.seek(0, 0)
        test_marker = f"# TEST_DATA_CREATED_BY_PIPELINE at {datetime.now()}\\n"
        f.write(test_marker + content)

def is_test_data_file(file_path: str) -> bool: # Now takes full path
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæµ‹è¯•æ•°æ®
    """
    try:
        with open(file_path, 'r') as f:
            first_line = f.readline()
            return 'TEST_DATA_CREATED_BY_PIPELINE' in first_line
    except:
        return False

def backup_real_data(files_to_backup: List[str], original_data_source_dir: str): # Added original_data_source_dir
    """
    å¤‡ä»½çœŸå®æ•°æ®æ–‡ä»¶ (if any were found outside SAMPLE_DATA_DIR)
    """
    _ensure_dir(BACKUP_DIR_ROOT)
    backup_dir_name = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    specific_backup_dir = os.path.join(BACKUP_DIR_ROOT, backup_dir_name)
    _ensure_dir(specific_backup_dir)
    
    print(f"ğŸ•µï¸ æ­£åœ¨æ£€æŸ¥ {original_data_source_dir} ä¸­çš„æ–‡ä»¶è¿›è¡Œå¤‡ä»½...")
    backed_up_count = 0
    for file_rel_path in files_to_backup: # files_to_backup are relative paths like 'email.csv'
        original_file_path = os.path.join(original_data_source_dir, file_rel_path)
        if os.path.exists(original_file_path):
            backup_target_path = os.path.join(specific_backup_dir, file_rel_path)
            _ensure_dir(os.path.dirname(backup_target_path))
            shutil.copy2(original_file_path, backup_target_path)
            print(f"   å¤‡ä»½ {original_file_path} -> {backup_target_path}")
            backed_up_count +=1
        else:
            print(f"   æ–‡ä»¶æœªåœ¨æºç›®å½•æ‰¾åˆ°ï¼Œè·³è¿‡å¤‡ä»½: {original_file_path}")

    if backed_up_count > 0:
        print(f"âœ… {backed_up_count} ä¸ªçœŸå®æ•°æ®æ–‡ä»¶å·²å¤‡ä»½åˆ°: {specific_backup_dir}")
    else:
        print("   æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„çœŸå®æ•°æ®æ–‡ä»¶è¿›è¡Œå¤‡ä»½ã€‚")


def get_test_data_files() -> List[str]:
    """
    è·å– SAMPLE_DATA_DIR ä¸­çš„æµ‹è¯•æ•°æ®æ–‡ä»¶åˆ—è¡¨ (full paths)
    """
    test_files_full_paths = []
    if os.path.exists(TEST_DATA_MARKER_FILE):
        with open(TEST_DATA_MARKER_FILE, 'r') as f:
            for line in f:
                if line.startswith('TEST_FILE:'):
                    # Paths in marker file are relative to SAMPLE_DATA_DIR
                    relative_test_file = line.replace('TEST_FILE:', '').strip()
                    full_path = os.path.join(SAMPLE_DATA_DIR, relative_test_file)
                    test_files_full_paths.append(full_path)
    else: # Fallback: scan SAMPLE_DATA_DIR if marker is missing
        if os.path.exists(SAMPLE_DATA_DIR):
            for root, _, files in os.walk(SAMPLE_DATA_DIR):
                for file in files:
                    full_path = os.path.join(root, file)
                    if file.endswith('.csv') and is_test_data_file(full_path):
                         # Add only if it's a marked test data CSV to avoid grabbing other files
                        test_files_full_paths.append(full_path)
    return list(set(test_files_full_paths)) # Ensure unique paths

def test_basic_functionality():
    """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
    print("\\nğŸ§ª æµ‹è¯•åŸºç¡€åŠŸèƒ½...")
    
    # æµ‹è¯•å¯¼å…¥ (already done at top level)
    print("âœ… æ¨¡å—å¯¼å…¥å·²åœ¨è„šæœ¬é¡¶éƒ¨å¤„ç†")
    
    # æµ‹è¯•åˆå§‹åŒ–
    try:
        # Initialize with overridden paths for testing
        pipeline = CERTDatasetPipeline(
            data_version='r4.2_test', 
            feature_dim=128,
            source_dir_override=SAMPLE_DATA_DIR,
            work_dir_override=TEST_OUTPUT_DIR
        )
        print("âœ… æµæ°´çº¿åˆå§‹åŒ–æˆåŠŸ (ä½¿ç”¨æµ‹è¯•è·¯å¾„)")
    except Exception as e:
        print(f"âŒ æµæ°´çº¿åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def test_step_by_step():
    """åˆ†æ­¥æµ‹è¯•æµæ°´çº¿"""
    print("\\nğŸ”¬ åˆ†æ­¥æµ‹è¯•æµæ°´çº¿...")
    
    # Ensure test data exists
    if not os.path.exists(TEST_DATA_MARKER_FILE) and not get_test_data_files():
        print("   æµ‹è¯•æ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
        if not create_test_data():
            print("âŒ åˆ›å»ºæµ‹è¯•æ•°æ®å¤±è´¥ï¼Œä¸­æ­¢åˆ†æ­¥æµ‹è¯•ã€‚")
            return False
            
    _ensure_dir(TEST_OUTPUT_DIR) # Ensure output directory exists
    
    pipeline = CERTDatasetPipeline(
        data_version='r4.2_test', 
        feature_dim=128,
        source_dir_override=SAMPLE_DATA_DIR,
        work_dir_override=TEST_OUTPUT_DIR
    )
    
    try:
        # Step 1: æ•°æ®åˆå¹¶
        print("\\n1ï¸âƒ£ æµ‹è¯•æ•°æ®åˆå¹¶...")
        pipeline.step1_combine_raw_data(start_week=0, end_week=3)
        
        # æ£€æŸ¥è¾“å‡º (paths relative to TEST_OUTPUT_DIR)
        week_files = [os.path.join(TEST_OUTPUT_DIR, f"DataByWeek/{i}.pickle") for i in range(3)]
        existing_files = [f for f in week_files if os.path.exists(f)]
        print(f"   ç”Ÿæˆå‘¨æ–‡ä»¶: {len(existing_files)}/3 in {os.path.join(TEST_OUTPUT_DIR, 'DataByWeek')}")
        
        # Step 2: ç”¨æˆ·æ•°æ®
        print("\\n2ï¸âƒ£ æµ‹è¯•ç”¨æˆ·æ•°æ®åŠ è½½...")
        users_df = pipeline.step2_load_user_data() # This will use psychometric and answers from SAMPLE_DATA_DIR
        print(f"   ç”¨æˆ·æ•°é‡: {len(users_df)}")
        print(f"   æ¶æ„ç”¨æˆ·: {len(users_df[users_df['malscene'] > 0])}")
        
        # Step 3: ç‰¹å¾æå–
        print("\\n3ï¸âƒ£ æµ‹è¯•ç‰¹å¾æå–...")
        pipeline.step3_extract_features(users_df, start_week=0, end_week=3, max_users=3)
        
        # æ£€æŸ¥ç‰¹å¾æ–‡ä»¶ (paths relative to TEST_OUTPUT_DIR)
        feature_files = [os.path.join(TEST_OUTPUT_DIR, f"NumDataByWeek/{i}_features.pickle") for i in range(3)]
        existing_features = [f for f in feature_files if os.path.exists(f)]
        print(f"   ç”Ÿæˆç‰¹å¾æ–‡ä»¶: {len(existing_features)}/3 in {os.path.join(TEST_OUTPUT_DIR, 'NumDataByWeek')}")
        
        # Step 4: å¤šçº§åˆ«åˆ†æ
        print("\\n4ï¸âƒ£ æµ‹è¯•å¤šçº§åˆ«åˆ†æ...")
        pipeline.step4_multi_level_analysis(start_week=0, end_week=3, modes=['week', 'day', 'session'])
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ (paths relative to TEST_OUTPUT_DIR)
        output_files_relative = [
            'WeekLevelFeatures/weeks_0_2.csv',
            'DayLevelFeatures/days_0_2.csv', 
            'SessionLevelFeatures/sessions_0_2.csv'
        ]
        
        for rel_path in output_files_relative:
            output_file_path = os.path.join(TEST_OUTPUT_DIR, rel_path)
            if os.path.exists(output_file_path):
                df = pd.read_csv(output_file_path)
                print(f"   âœ… {output_file_path}: {len(df)} æ¡è®°å½•")
            else:
                print(f"   âŒ {output_file_path}: æ–‡ä»¶ä¸å­˜åœ¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†æ­¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´æµæ°´çº¿"""
    print("\\nğŸš€ æµ‹è¯•å®Œæ•´æµæ°´çº¿...")

    if not os.path.exists(TEST_DATA_MARKER_FILE) and not get_test_data_files():
        print("   æµ‹è¯•æ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
        if not create_test_data():
            print("âŒ åˆ›å»ºæµ‹è¯•æ•°æ®å¤±è´¥ï¼Œä¸­æ­¢å®Œæ•´æµæ°´çº¿æµ‹è¯•ã€‚")
            return False

    _ensure_dir(TEST_OUTPUT_DIR)
    
    pipeline = CERTDatasetPipeline(
        data_version='r4.2_test', 
        feature_dim=64,
        source_dir_override=SAMPLE_DATA_DIR,
        work_dir_override=TEST_OUTPUT_DIR
    )
    
    try:
        start_time = time.time()
        
        pipeline.run_full_pipeline(
            start_week=0,
            end_week=3,
            max_users=3,
            modes=['week', 'day']  # åªæµ‹è¯•å‘¨å’Œæ—¥çº§åˆ«
        )
        
        elapsed_time = (time.time() - start_time) / 60
        print(f"âœ… å®Œæ•´æµæ°´çº¿æµ‹è¯•æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.1f} åˆ†é’Ÿ (è¾“å‡ºåˆ° {TEST_OUTPUT_DIR})")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æµæ°´çº¿æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def analyze_results():
    """åˆ†ææµ‹è¯•ç»“æœ from TEST_OUTPUT_DIR"""
    print("\\nğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
    
    result_files_relative = {
        'week': 'WeekLevelFeatures/weeks_0_2.csv',
        'day': 'DayLevelFeatures/days_0_2.csv',
        'session': 'SessionLevelFeatures/sessions_0_2.csv' # This might not be created if modes in test_full_pipeline is limited
    }
    
    all_results_found = True
    for mode, rel_file_path in result_files_relative.items():
        full_file_path = os.path.join(TEST_OUTPUT_DIR, rel_file_path)
        if os.path.exists(full_file_path):
            df = pd.read_csv(full_file_path)
            print(f"\\nğŸ“ˆ {mode.capitalize()}çº§åˆ«åˆ†æç»“æœ (æ¥è‡ª {full_file_path}):")
            print(f"   è®°å½•æ•°: {len(df)}")
            print(f"   ç”¨æˆ·æ•°: {df['user'].nunique()}")
            if 'n_events' in df.columns: print(f"   å¹³å‡äº‹ä»¶æ•°: {df['n_events'].mean():.1f}")
            if 'malicious_ratio' in df.columns: print(f"   æ¶æ„æ¯”ä¾‹: {df['malicious_ratio'].mean():.3f}")
            
            print(f"   æ ·æœ¬æ•°æ®:")
            display_cols = [col for col in ['user', 'week', 'mode', 'n_events', 'malicious_ratio'] if col in df.columns]
            print(df[display_cols].head(3).to_string(index=False))
        else:
            # Only mark as not found if it was expected based on test_full_pipeline modes
            if mode in ['week', 'day']: # Assuming these are always run by test_full_pipeline
                 print(f"âš ï¸  {mode}çº§åˆ«ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {full_file_path}")
                 all_results_found = False
            else:
                 print(f"â„¹ï¸  {mode}çº§åˆ«ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ (å¯èƒ½æœªåœ¨æµ‹è¯•ä¸­è¿è¡Œ): {full_file_path}")


    if not all_results_found and ('session' not in ['week', 'day']): # Check if session was the missing one
        print("æé†’: 'session' çº§åˆ«æ•°æ®å¯èƒ½æœªç”Ÿæˆï¼Œå› ä¸º `test_full_pipeline` ä¸­ modes=['week', 'day']ã€‚åˆ†æ­¥æµ‹è¯•ä¼šç”Ÿæˆå®ƒã€‚")


def cleanup_test_files():
    """å®‰å…¨æ¸…ç† TEST_OUTPUT_DIR å’Œ SAMPLE_DATA_DIR"""
    print("\\nğŸ§¹ å®‰å…¨æ¸…ç†æµ‹è¯•æ–‡ä»¶å’Œç›®å½•...")
    
    # 1. æ¸…ç†ç”Ÿæˆçš„è¾“å‡ºç›®å½•
    if os.path.exists(TEST_OUTPUT_DIR):
        try:
            shutil.rmtree(TEST_OUTPUT_DIR)
            print(f"   åˆ é™¤æµ‹è¯•è¾“å‡ºç›®å½•: {TEST_OUTPUT_DIR}")
        except Exception as e:
            print(f"   åˆ é™¤æµ‹è¯•è¾“å‡ºç›®å½•å¤±è´¥ {TEST_OUTPUT_DIR}: {e}")
    else:
        print(f"   æµ‹è¯•è¾“å‡ºç›®å½•æœªæ‰¾åˆ°ï¼Œæ— éœ€åˆ é™¤: {TEST_OUTPUT_DIR}")

    # 2. æ¸…ç†æ ·æœ¬æ•°æ®ç›®å½• (including marker file)
    if os.path.exists(SAMPLE_DATA_DIR):
        try:
            shutil.rmtree(SAMPLE_DATA_DIR) # This will remove all files within, including the marker
            print(f"   åˆ é™¤æ ·æœ¬æ•°æ®ç›®å½•: {SAMPLE_DATA_DIR}")
        except Exception as e:
            print(f"   åˆ é™¤æ ·æœ¬æ•°æ®ç›®å½•å¤±è´¥ {SAMPLE_DATA_DIR}: {e}")
    else:
        print(f"   æ ·æœ¬æ•°æ®ç›®å½•æœªæ‰¾åˆ°ï¼Œæ— éœ€åˆ é™¤: {SAMPLE_DATA_DIR}")
        # Explicitly check and remove marker if it somehow exists outside SAMPLE_DATA_DIR (legacy)
        legacy_marker = '.test_data_created_by_pipeline'
        if os.path.exists(legacy_marker) and os.path.isfile(legacy_marker):
             try:
                os.remove(legacy_marker)
                print(f"   åˆ é™¤é—ç•™çš„æ ‡è®°æ–‡ä»¶: {legacy_marker}")
             except Exception as e:
                print(f"   åˆ é™¤é—ç•™æ ‡è®°æ–‡ä»¶å¤±è´¥ {legacy_marker}: {e}")


    # 3. åˆ é™¤å¤‡ä»½ç›®å½•ï¼ˆå¯é€‰, and now they are inside TESTS_DIR/backup_data)
    if os.path.exists(BACKUP_DIR_ROOT):
        backup_sub_dirs = [d for d in os.listdir(BACKUP_DIR_ROOT) if os.path.isdir(os.path.join(BACKUP_DIR_ROOT, d)) and d.startswith('backup_')]
        if backup_sub_dirs:
            print(f"\\nğŸ“¦ å‘ç°æµ‹è¯•ç›¸å…³çš„å¤‡ä»½ç›®å½•äº {BACKUP_DIR_ROOT}: {backup_sub_dirs}")
            # Non-interactive cleanup for automated tests, or prompt if desired
            # For now, let's make it non-interactive:
            print("   è‡ªåŠ¨æ¸…ç†æµ‹è¯•ç›¸å…³çš„å¤‡ä»½ç›®å½•...")
            for backup_dir_name in backup_sub_dirs:
                try:
                    shutil.rmtree(os.path.join(BACKUP_DIR_ROOT, backup_dir_name))
                    print(f"      åˆ é™¤å¤‡ä»½ç›®å½•: {backup_dir_name}")
                except Exception as e:
                    print(f"      åˆ é™¤å¤‡ä»½ç›®å½•å¤±è´¥ {backup_dir_name}: {e}")
            # Optionally remove BACKUP_DIR_ROOT if it's now empty
            if not os.listdir(BACKUP_DIR_ROOT):
                try:
                    os.rmdir(BACKUP_DIR_ROOT)
                    print(f"   åˆ é™¤ç©ºçš„å¤‡ä»½æ ¹ç›®å½•: {BACKUP_DIR_ROOT}")
                except Exception as e:
                    print(f"   åˆ é™¤ç©ºçš„å¤‡ä»½æ ¹ç›®å½•å¤±è´¥ {BACKUP_DIR_ROOT}: {e}")
        elif not os.listdir(BACKUP_DIR_ROOT): # If BACKUP_DIR_ROOT exists but is empty
            try:
                os.rmdir(BACKUP_DIR_ROOT)
                print(f"   åˆ é™¤ç©ºçš„å¤‡ä»½æ ¹ç›®å½•: {BACKUP_DIR_ROOT}")
            except Exception as e:
                print(f"   åˆ é™¤ç©ºçš„å¤‡ä»½æ ¹ç›®å½•å¤±è´¥ {BACKUP_DIR_ROOT}: {e}")
    
    print("âœ… æµ‹è¯•æ–‡ä»¶å’Œç›®å½•æ¸…ç†å®Œæˆ")


def check_data_safety(): # Primarily checks SAMPLE_DATA_DIR now
    """æ£€æŸ¥ SAMPLE_DATA_DIR çš„æ•°æ®å®‰å…¨æ€§"""
    print("\\nğŸ”’ æµ‹è¯•æ•°æ®å®‰å…¨æ€§æ£€æŸ¥...")
    
    if os.path.exists(TEST_DATA_MARKER_FILE):
        print(f"âœ… æ‰¾åˆ°æµ‹è¯•æ•°æ®æ ‡è®°æ–‡ä»¶: {TEST_DATA_MARKER_FILE}")
        # Reading from marker is now primary source for get_test_data_files
        marked_files = get_test_data_files() # These are full paths now
        print(f"   æ ‡è®°çš„æµ‹è¯•æ–‡ä»¶ ({len(marked_files)}):")
        for f_path in marked_files:
            print(f"     - {f_path} (å­˜åœ¨: {os.path.exists(f_path)})")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ ‡è®°æ–‡ä»¶: {TEST_DATA_MARKER_FILE}")
        # Check if there are any CSVs in SAMPLE_DATA_DIR anyway
        if os.path.exists(SAMPLE_DATA_DIR):
            csv_files_in_sample_dir = [os.path.join(SAMPLE_DATA_DIR, f) for f in os.listdir(SAMPLE_DATA_DIR) if f.endswith('.csv')]
            if csv_files_in_sample_dir:
                print(f"   åœ¨ {SAMPLE_DATA_DIR} ä¸­å‘ç°ä»¥ä¸‹æœªæ ‡è®°çš„CSVæ–‡ä»¶:")
                for csv_file in csv_files_in_sample_dir:
                     print(f"     - {csv_file} (æ˜¯å¦ä¸ºæµ‹è¯•æ–‡ä»¶: {is_test_data_file(csv_file)})")
            else:
                print(f"   {SAMPLE_DATA_DIR} ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶ã€‚")
        else:
            print(f"   æ ·æœ¬æ•°æ®ç›®å½• {SAMPLE_DATA_DIR} ä¸å­˜åœ¨ã€‚")
    
    # Backup directory check remains similar but points to BACKUP_DIR_ROOT
    if os.path.exists(BACKUP_DIR_ROOT):
        backup_sub_dirs = [d for d in os.listdir(BACKUP_DIR_ROOT) if os.path.isdir(os.path.join(BACKUP_DIR_ROOT, d)) and d.startswith('backup_')]
        if backup_sub_dirs:
            print(f"\\nğŸ“¦ æµ‹è¯•ç›¸å…³çš„å¤‡ä»½ç›®å½•ä½äº {BACKUP_DIR_ROOT}: {backup_sub_dirs}")
        else:
            print(f"\\nğŸ“¦ {BACKUP_DIR_ROOT} ä¸­æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç›¸å…³çš„å¤‡ä»½å­ç›®å½•ã€‚")
    else:
        print(f"\\nğŸ“¦ æµ‹è¯•ç›¸å…³çš„å¤‡ä»½æ ¹ç›®å½• {BACKUP_DIR_ROOT} ä¸å­˜åœ¨ã€‚")


def restore_real_data(): # This function might need rethinking in the new structure
    """
    ä»å¤‡ä»½æ¢å¤çœŸå®æ•°æ®. 
    NOTE: This function's utility is reduced if test data is strictly isolated.
    It might be used to restore to a *different* location if needed.
    For now, it attempts to restore to where `backup_real_data` might have sourced from,
    which is now more abstract (needs `original_data_source_dir` if used).
    Consider if this function is still needed or how it should behave.
    """
    print(f"âš ï¸  è­¦å‘Š: `restore_real_data` çš„è¡Œä¸ºå·²æ”¹å˜ã€‚å®ƒç°åœ¨ä¼šä» {BACKUP_DIR_ROOT} æ¢å¤ã€‚")
    print("   è¯·ç¡®ä¿æ‚¨äº†è§£æ­¤æ“ä½œçš„ç›®æ ‡æ¢å¤ä½ç½®ã€‚")

    if not os.path.exists(BACKUP_DIR_ROOT):
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ ¹ç›®å½•: {BACKUP_DIR_ROOT}")
        return

    backup_sub_dirs = [d for d in os.listdir(BACKUP_DIR_ROOT) if os.path.isdir(os.path.join(BACKUP_DIR_ROOT, d)) and d.startswith('backup_')]
    if not backup_sub_dirs:
        print(f"âŒ {BACKUP_DIR_ROOT} ä¸­æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½å­ç›®å½•ã€‚")
        return
    
    print(f"ğŸ“¦ å‘ç°å¤‡ä»½ç›®å½•äº {BACKUP_DIR_ROOT}: {backup_sub_dirs}")
    
    # Simplistic: choose the latest by name. A more robust way might involve parsing dates.
    latest_backup_subdir_name = max(backup_sub_dirs)
    latest_backup_full_path = os.path.join(BACKUP_DIR_ROOT, latest_backup_subdir_name)
    print(f"ä½¿ç”¨æœ€æ–°å¤‡ä»½: {latest_backup_full_path}")
    
    # Define a *TARGET* directory for restoration.
    # This is CRITICAL. Where should these files go?
    # For safety, let's default to a NEW directory unless specified.
    # This part needs careful consideration based on actual use case.
    # Let's assume for now we're restoring to a hypothetical 'restored_data' dir in TESTS_DIR
    restore_target_dir = os.path.join(TESTS_DIR, f"restored_from_{latest_backup_subdir_name}")
    _ensure_dir(restore_target_dir)
    print(f" æ–‡ä»¶å°†æ¢å¤åˆ°: {restore_target_dir}")
    
    restored_count = 0
    for root, _, files in os.walk(latest_backup_full_path):
        for file in files:
            backup_file_path = os.path.join(root, file)
            # Path relative to the specific backup sub-directory (e.g., 'email.csv')
            relative_path_in_backup = os.path.relpath(backup_file_path, latest_backup_full_path)
            target_restore_path = os.path.join(restore_target_dir, relative_path_in_backup)
            
            _ensure_dir(os.path.dirname(target_restore_path))
            shutil.copy2(backup_file_path, target_restore_path)
            print(f"   æ¢å¤: {relative_path_in_backup} -> {target_restore_path}")
            restored_count += 1
            
    if restored_count > 0:
        print(f"âœ… {restored_count} ä¸ªæ–‡ä»¶å·²æ¢å¤åˆ° {restore_target_dir}")
    else:
        print(f"âš ï¸  å¤‡ä»½ç›®å½• {latest_backup_full_path} ä¸ºç©ºæˆ–æ— æ³•æ¢å¤æ–‡ä»¶ã€‚")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ CERTæ•°æ®é›†æµæ°´çº¿æµ‹è¯• (æ–°ç»“æ„)")
    print("=" * 50)
    
    _ensure_dir(TEST_OUTPUT_DIR) # Ensure output directory for pipeline runs
    _ensure_dir(SAMPLE_DATA_DIR) # Ensure sample data dir
    _ensure_dir(BACKUP_DIR_ROOT) # Ensure backup root dir

    check_data_safety()
    
    print(f"\\nğŸ“‹ å¯ç”¨é€‰é¡¹:")
    print("1. åˆ›å»ºæµ‹è¯•æ•°æ® (åœ¨ sample_test_data/)")
    print("2. è¿è¡Œå®Œæ•´æµ‹è¯• (ä½¿ç”¨ sample_test_data/ å’Œ test_output/)")
    print("3. æ£€æŸ¥æ•°æ®å®‰å…¨æ€§ (ä¸»è¦é’ˆå¯¹ sample_test_data/)")
    print("4. æ¸…ç†æµ‹è¯•æ–‡ä»¶å’Œç›®å½• (sample_test_data/, test_output/, backups)")
    print("5. ä»å¤‡ä»½æ¢å¤æ•°æ® (åˆ° tests/restored_from_backup_xxx)")
    print("6. é€€å‡º")
    
    while True:
        choice = input("\\nè¯·é€‰æ‹©æ“ä½œ (1-6): ").strip()
        
        if choice == '1':
            if create_test_data(): # Now creates in SAMPLE_DATA_DIR
                print("âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
            else:
                print("âŒ æµ‹è¯•æ•°æ®åˆ›å»ºå¤±è´¥")
            # break # Allow further actions after creating data
            
        elif choice == '2':
            run_full_test() # Uses SAMPLE_DATA_DIR and TEST_OUTPUT_DIR
            # break
            
        elif choice == '3':
            check_data_safety() # Checks SAMPLE_DATA_DIR
            
        elif choice == '4':
            response = input(f"ç¡®è®¤æ¸…ç† {SAMPLE_DATA_DIR}, {TEST_OUTPUT_DIR} å’Œ {BACKUP_DIR_ROOT} ä¸­çš„ç›¸å…³å†…å®¹? (y/n): ")
            if response.lower() == 'y':
                cleanup_test_files()
            # break
            
        elif choice == '5':
            restore_real_data() # Restores from BACKUP_DIR_ROOT to a new dir
            
        elif choice == '6':
            print("ğŸ‘‹ é€€å‡º")
            break
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-6")

def run_full_test():
    """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
    print("\\nğŸš€ è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹...")
    
    test_results = {}
    
    try:
        if not get_test_data_files(): # Checks SAMPLE_DATA_DIR
            print("âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®ï¼Œå…ˆåˆ›å»ºæµ‹è¯•æ•°æ®...")
            if not create_test_data():
                print("âŒ æµ‹è¯•æ•°æ®åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•ã€‚")
                return
        
        _ensure_dir(TEST_OUTPUT_DIR) # Ensure output dir for this run

        test_results['basic_functionality'] = test_basic_functionality()
        if not test_results['basic_functionality']: return # Stop if basic init fails
        
        test_results['step_by_step'] = test_step_by_step()
        if not test_results['step_by_step']: print("âš ï¸  åˆ†æ­¥æµ‹è¯•ä¸­å‡ºç°é—®é¢˜ï¼Œä½†å°†ç»§ç»­å°è¯•å®Œæ•´æµæ°´çº¿æµ‹è¯•ã€‚")
        
        test_results['full_pipeline'] = test_full_pipeline()
        
        analyze_results() # Analyzes from TEST_OUTPUT_DIR
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\\nğŸ“‹ æµ‹è¯•æ‘˜è¦:")
    print("=" * 30)
    
    all_passed = True
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        if not result: all_passed = False
        print(f"{test_name}: {status}")
    
    if test_results:
        # success_rate = sum(1 for res in test_results.values() if res) / len(test_results) * 100
        # print(f"\\nğŸ¯ æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}%")
        if all_passed:
            print("\\nğŸ¯ æ‰€æœ‰ä¸»è¦æµ‹è¯•æ­¥éª¤é€šè¿‡ï¼")
        else:
            print("\\nğŸ¯ éƒ¨åˆ†æµ‹è¯•æ­¥éª¤å¤±è´¥ã€‚")

    
    print(f"\\nğŸ§¹ æµ‹è¯•å®Œæˆã€‚")
    print("   è¾“å‡ºæ–‡ä»¶ä½äº: " + os.path.abspath(TEST_OUTPUT_DIR))
    print("   æµ‹è¯•æ•°æ®ä½äº: " + os.path.abspath(SAMPLE_DATA_DIR))
    print("   å¤‡ä»½æ–‡ä»¶ä½äº: " + os.path.abspath(BACKUP_DIR_ROOT))
    print("\\n   è¯·æ ¹æ®éœ€è¦é€‰æ‹©æ¸…ç†é€‰é¡¹ï¼ˆä¸»èœå•ä¸­çš„é€‰é¡¹4ï¼‰ã€‚")


if __name__ == "__main__":
    main() 