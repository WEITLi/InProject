#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹ä¸»è®­ç»ƒè„šæœ¬
æ•´åˆåŸæœ‰é¡¹ç›®æ¡†æ¶ï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼å’Œé…ç½®
"""

import os
import sys
import argparse
import json
import time
import warnings
from datetime import datetime
from pathlib import Path

import torch
import numpy as np
from typing import Dict, List, Optional

warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'core_logic'))

try:
    # å°è¯•ä»core_logicåŒ…å¯¼å…¥
    from core_logic.multimodal_pipeline import MultiModalDataPipeline
    from core_logic.train_pipeline_multimodal.multimodal_trainer import MultiModalTrainer
    from core_logic.config import Config, ModelConfig, TrainingConfig, DataConfig
except ImportError:
    # å¦‚æœåŒ…å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
    from multimodal_pipeline import MultiModalDataPipeline
    from train_pipeline.multimodal_trainer import MultiModalTrainer
    from config import Config, ModelConfig, TrainingConfig, DataConfig

def set_seed(seed: int = 42):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯é‡å¤æ€§"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def create_config_from_args(args) -> Config:
    """ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®"""
    config = Config()
    
    # æ¨¡å‹é…ç½®
    config.model.hidden_dim = args.hidden_dim
    config.model.num_heads = args.num_heads
    config.model.num_layers = args.num_layers
    config.model.sequence_length = args.sequence_length
    config.model.enable_gnn = args.enable_gnn
    config.model.enable_bert = args.enable_bert
    config.model.enable_lgbm = args.enable_lgbm
    config.model.enable_transformer = args.enable_transformer
    
    # è®­ç»ƒé…ç½®
    config.training.batch_size = args.batch_size
    config.training.learning_rate = args.learning_rate
    config.training.num_epochs = args.num_epochs
    config.training.patience = args.patience
    config.training.test_split = args.test_split
    config.training.val_split = args.val_split
    config.training.device = args.device
    
    # æ•°æ®é…ç½®
    config.data.data_version = args.data_version
    config.data.feature_dim = args.feature_dim
    config.data.start_week = args.start_week
    config.data.end_week = args.end_week
    config.data.max_users = args.max_users
    
    # ç¯å¢ƒé…ç½®
    config.seed = args.seed
    config.num_workers = args.num_workers
    config.output_dir = args.output_dir
    config.experiment_name = args.experiment_name
    config.debug = args.debug
    
    return config

def train_multimodal_model(config: Config) -> Dict[str, any]:
    """
    è®­ç»ƒå¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹
    
    Args:
        config: é…ç½®å¯¹è±¡
        
    Returns:
        è®­ç»ƒç»“æœå­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"å¼€å§‹å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
    print(f"å®éªŒåç§°: {config.experiment_name}")
    print(f"{'='*80}")
    
    start_time = time.time()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = os.path.join(config.output_dir, f"{config.experiment_name}_{timestamp}")
    os.makedirs(experiment_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(experiment_dir, 'config.json')
    with open(config_path, 'w') as f:
        # è½¬æ¢é…ç½®ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        config_dict = {}
        for key, value in config.__dict__.items():
            if hasattr(value, '__dict__'):
                config_dict[key] = value.__dict__
            else:
                config_dict[key] = value
        json.dump(config_dict, f, indent=4, ensure_ascii=False)
    
    print(f"å®éªŒç›®å½•: {experiment_dir}")
    print(f"é…ç½®ä¿å­˜åˆ°: {config_path}")
    
    try:
        # Step 1: åˆ›å»ºå¤šæ¨¡æ€æ•°æ®æµæ°´çº¿
        print(f"\n{'='*60}")
        print(f"Step 1: åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®æµæ°´çº¿")
        print(f"{'='*60}")
        
        pipeline = MultiModalDataPipeline(
            config=config,
            data_version=config.data.data_version,
            feature_dim=config.data.feature_dim,
            num_cores=config.num_workers
        )
        
        # Step 2: è¿è¡Œæ•°æ®å¤„ç†æµæ°´çº¿
        print(f"\n{'='*60}")
        print(f"Step 2: è¿è¡Œæ•°æ®å¤„ç†æµæ°´çº¿")
        print(f"{'='*60}")
        
        training_data = pipeline.run_full_multimodal_pipeline(
            start_week=config.data.start_week,
            end_week=config.data.end_week,
            max_users=config.data.max_users,
            sequence_length=config.model.sequence_length
        )
        
        # ä¿å­˜è®­ç»ƒæ•°æ®ä¿¡æ¯
        data_info = {
            'total_samples': len(training_data['labels']),
            'normal_samples': int(np.sum(training_data['labels'] == 0)),
            'anomaly_samples': int(np.sum(training_data['labels'] == 1)),
            'behavior_sequences_shape': training_data['behavior_sequences'].shape,
            'node_features_shape': training_data['node_features'].shape,
            'adjacency_matrix_shape': training_data['adjacency_matrix'].shape,
            'structured_features_shape': training_data['structured_features'].shape,
            'text_samples': len(training_data['text_content'])
        }
        
        data_info_path = os.path.join(experiment_dir, 'data_info.json')
        with open(data_info_path, 'w') as f:
            json.dump(data_info, f, indent=4)
        
        print(f"æ•°æ®ä¿¡æ¯ä¿å­˜åˆ°: {data_info_path}")
        
        # Step 3: åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
        print(f"\n{'='*60}")
        print(f"Step 3: åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ")
        print(f"{'='*60}")
        
        trainer = MultiModalTrainer(config=config, output_dir=experiment_dir)
        model = trainer.train(training_data)
        
        # Step 4: ä¿å­˜æœ€ç»ˆç»“æœ
        total_time = time.time() - start_time
        
        final_results = {
            'experiment_name': config.experiment_name,
            'experiment_dir': experiment_dir,
            'total_time': total_time,
            'data_info': data_info,
            'config': config_dict,
            'success': True
        }
        
        results_path = os.path.join(experiment_dir, 'final_results.json')
        with open(results_path, 'w') as f:
            json.dump(final_results, f, indent=4)
        
        print(f"\n{'='*80}")
        print(f"å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"å®éªŒç»“æœä¿å­˜åˆ°: {experiment_dir}")
        print(f"{'='*80}")
        
        return final_results
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        error_results = {
            'experiment_name': config.experiment_name,
            'experiment_dir': experiment_dir,
            'error': str(e),
            'success': False
        }
        
        error_path = os.path.join(experiment_dir, 'error_results.json')
        with open(error_path, 'w') as f:
            json.dump(error_results, f, indent=4)
        
        raise e

def run_experiment_comparison(base_config: Config, experiment_configs: List[Dict]) -> Dict[str, any]:
    """
    è¿è¡Œå¯¹æ¯”å®éªŒ
    
    Args:
        base_config: åŸºç¡€é…ç½®
        experiment_configs: å®éªŒé…ç½®åˆ—è¡¨
        
    Returns:
        å¯¹æ¯”å®éªŒç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"å¼€å§‹è¿è¡Œå¯¹æ¯”å®éªŒ")
    print(f"å®éªŒæ•°é‡: {len(experiment_configs)}")
    print(f"{'='*80}")
    
    comparison_results = {}
    
    for i, exp_config in enumerate(experiment_configs):
        print(f"\n{'='*60}")
        print(f"è¿è¡Œå®éªŒ {i+1}/{len(experiment_configs)}: {exp_config['name']}")
        print(f"{'='*60}")
        
        # å¤åˆ¶åŸºç¡€é…ç½®
        config = Config()
        config.__dict__.update(base_config.__dict__)
        
        # åº”ç”¨å®éªŒç‰¹å®šé…ç½®
        for key, value in exp_config.items():
            if key != 'name':
                if hasattr(config, key):
                    setattr(config, key, value)
                elif hasattr(config.model, key):
                    setattr(config.model, key, value)
                elif hasattr(config.training, key):
                    setattr(config.training, key, value)
                elif hasattr(config.data, key):
                    setattr(config.data, key, value)
        
        # è®¾ç½®å®éªŒåç§°
        config.experiment_name = exp_config['name']
        
        try:
            # è¿è¡Œè®­ç»ƒ
            results = train_multimodal_model(config)
            comparison_results[exp_config['name']] = results
            
        except Exception as e:
            print(f"å®éªŒ {exp_config['name']} å¤±è´¥: {e}")
            comparison_results[exp_config['name']] = {
                'success': False,
                'error': str(e)
            }
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_dir = os.path.join(base_config.output_dir, 'comparison_results')
    os.makedirs(comparison_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    comparison_path = os.path.join(comparison_dir, f'comparison_{timestamp}.json')
    
    with open(comparison_path, 'w') as f:
        json.dump(comparison_results, f, indent=4)
    
    print(f"\n{'='*80}")
    print(f"å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åˆ°: {comparison_path}")
    print(f"{'='*80}")
    
    return comparison_results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_version', type=str, default='r4.2',
                       help='æ•°æ®é›†ç‰ˆæœ¬ (é»˜è®¤: r4.2)')
    parser.add_argument('--feature_dim', type=int, default=256,
                       help='ç‰¹å¾ç»´åº¦ (é»˜è®¤: 256)')
    parser.add_argument('--start_week', type=int, default=0,
                       help='å¼€å§‹å‘¨æ•° (é»˜è®¤: 0)')
    parser.add_argument('--end_week', type=int, default=None,
                       help='ç»“æŸå‘¨æ•° (é»˜è®¤: None, ä½¿ç”¨å…¨éƒ¨æ•°æ®)')
    parser.add_argument('--max_users', type=int, default=None,
                       help='æœ€å¤§ç”¨æˆ·æ•° (é»˜è®¤: None, ä½¿ç”¨å…¨éƒ¨ç”¨æˆ·)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_dim', type=int, default=256,
                       help='éšè—å±‚ç»´åº¦ (é»˜è®¤: 256)')
    parser.add_argument('--num_heads', type=int, default=8,
                       help='æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 8)')
    parser.add_argument('--num_layers', type=int, default=6,
                       help='Transformerå±‚æ•° (é»˜è®¤: 6)')
    parser.add_argument('--sequence_length', type=int, default=128,
                       help='åºåˆ—é•¿åº¦ (é»˜è®¤: 128)')
    
    # æ¨¡å—å¯ç”¨æ§åˆ¶
    parser.add_argument('--enable_gnn', action='store_true', default=True,
                       help='å¯ç”¨GNNç”¨æˆ·å›¾åµŒå…¥')
    parser.add_argument('--enable_bert', action='store_true', default=True,
                       help='å¯ç”¨BERTæ–‡æœ¬ç¼–ç ')
    parser.add_argument('--enable_lgbm', action='store_true', default=True,
                       help='å¯ç”¨LightGBMç»“æ„åŒ–ç‰¹å¾')
    parser.add_argument('--enable_transformer', action='store_true', default=True,
                       help='å¯ç”¨Transformeråºåˆ—å»ºæ¨¡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹å¤§å° (é»˜è®¤: 32)')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 1e-4)')
    parser.add_argument('--num_epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 100)')
    parser.add_argument('--patience', type=int, default=10,
                       help='æ—©åœpatience (é»˜è®¤: 10)')
    parser.add_argument('--test_split', type=float, default=0.2,
                       help='æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    parser.add_argument('--val_split', type=float, default=0.2,
                       help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    
    # ç¯å¢ƒå‚æ•°
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ç±»å‹ (é»˜è®¤: cuda)')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 4)')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ (é»˜è®¤: 42)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default='./outputs',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ./outputs)')
    parser.add_argument('--experiment_name', type=str, default='multimodal_anomaly_detection',
                       help='å®éªŒåç§° (é»˜è®¤: multimodal_anomaly_detection)')
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument('--mode', type=str, choices=['train', 'experiment', 'comparison'], 
                       default='train', help='è¿è¡Œæ¨¡å¼ (é»˜è®¤: train)')
    parser.add_argument('--config_file', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (å¯é€‰)')
    
    # è°ƒè¯•æ¨¡å¼
    parser.add_argument('--debug', action='store_true',
                       help='è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--fast_dev_run', action='store_true',
                       help='å¿«é€Ÿå¼€å‘æ¨¡å¼ï¼ˆåªè¿è¡Œå°‘é‡æ•°æ®ï¼‰')
    
    args = parser.parse_args()
    
    # å¿«é€Ÿå¼€å‘æ¨¡å¼è®¾ç½®
    if args.fast_dev_run:
        args.end_week = 3
        args.max_users = 50
        args.num_epochs = 5
        args.debug = True
        print("ğŸš€ å¿«é€Ÿå¼€å‘æ¨¡å¼å¯ç”¨")
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœæä¾›ï¼‰
    if args.config_file and os.path.exists(args.config_file):
        with open(args.config_file, 'r') as f:
            config_dict = json.load(f)
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config_file}")
    
    # åˆ›å»ºé…ç½®
    config = create_config_from_args(args)
    
    print(f"\n{'='*80}")
    print(f"å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
    print(f"{'='*80}")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"æ•°æ®ç‰ˆæœ¬: {config.data.data_version}")
    print(f"ç‰¹å¾ç»´åº¦: {config.data.feature_dim}")
    print(f"éšè—ç»´åº¦: {config.model.hidden_dim}")
    print(f"è®¾å¤‡: {config.training.device}")
    print(f"è¾“å‡ºç›®å½•: {config.output_dir}")
    if args.debug:
        print("ğŸ› è°ƒè¯•æ¨¡å¼å¯ç”¨")
    print(f"{'='*80}")
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == 'train':
        # å•æ¬¡è®­ç»ƒ
        results = train_multimodal_model(config)
        
    elif args.mode == 'experiment':
        # è¿è¡Œé¢„å®šä¹‰å®éªŒ
        experiment_configs = [
            {'name': 'baseline', 'enable_gnn': False, 'enable_bert': False, 'enable_lgbm': False},
            {'name': 'transformer_only', 'enable_gnn': False, 'enable_bert': False, 'enable_lgbm': False},
            {'name': 'transformer_gnn', 'enable_bert': False, 'enable_lgbm': False},
            {'name': 'transformer_bert', 'enable_gnn': False, 'enable_lgbm': False},
            {'name': 'transformer_lgbm', 'enable_gnn': False, 'enable_bert': False},
            {'name': 'full_multimodal', 'enable_gnn': True, 'enable_bert': True, 'enable_lgbm': True}
        ]
        
        results = run_experiment_comparison(config, experiment_configs)
        
    elif args.mode == 'comparison':
        # è¿è¡Œè¶…å‚æ•°å¯¹æ¯”å®éªŒ
        comparison_configs = [
            {'name': 'small_model', 'hidden_dim': 128, 'num_layers': 4},
            {'name': 'medium_model', 'hidden_dim': 256, 'num_layers': 6},
            {'name': 'large_model', 'hidden_dim': 512, 'num_layers': 8},
        ]
        
        results = run_experiment_comparison(config, comparison_configs)
    
    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main() 