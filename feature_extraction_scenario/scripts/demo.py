#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†…éƒ¨å¨èƒæ£€æµ‹ç‰¹å¾æå–ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬

å±•ç¤ºå¦‚ä½•ä½¿ç”¨è¯¥ç³»ç»Ÿè¿›è¡Œå®Œæ•´çš„ç‰¹å¾æå–å’Œå¼‚å¸¸æ£€æµ‹æµç¨‹
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List

# å¯¼å…¥ç‰¹å¾æå–æ¨¡å—
from encoder import EventEncoder
from utils import FeatureEncoder
from temporal import encode_session_temporal_features
from user_context import encode_behavioral_risk_profile
from email import detect_suspicious_email_patterns
from file import detect_suspicious_file_patterns
from http import detect_suspicious_http_patterns
from device import detect_suspicious_device_patterns

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    print("ğŸ”§ åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    # ç¤ºä¾‹äº‹ä»¶æ•°æ®
    events_data = [
        # æ­£å¸¸ç”¨æˆ·æ´»åŠ¨
        {
            'type': 'logon',
            'date': '01/15/2024 08:30:00',
            'user': 'ACM2278',
            'pc': 'PC-1234'
        },
        {
            'type': 'email',
            'date': '01/15/2024 09:15:00',
            'user': 'ACM2278',
            'from': 'ACM2278@dtaa.com',
            'to': 'colleague@dtaa.com',
            'cc': '',
            'bcc': '',
            'size': '1024',
            'content': 'Meeting reminder for tomorrow',
            'activity': 'Send'
        },
        {
            'type': 'http',
            'date': '01/15/2024 10:30:00',
            'user': 'ACM2278',
            'url/fname': 'https://www.company-intranet.com/reports',
            'content': 'Internal company reports page',
            'activity': 'www visit'
        },
        {
            'type': 'file',
            'date': '01/15/2024 11:45:00',
            'user': 'ACM2278',
            'pc': 'PC-1234',
            'url/fname': 'C:\\Users\\ACM2278\\Documents\\report.docx',
            'content': 'Monthly financial report content...',
            'activity': 'file open',
            'to': 'false',
            'from': 'false'
        },
        
        # å¯ç–‘ç”¨æˆ·æ´»åŠ¨
        {
            'type': 'email',
            'date': '01/15/2024 18:30:00',  # ä¸‹ç­å
            'user': 'ACM2278',
            'from': 'ACM2278@dtaa.com',
            'to': 'external@gmail.com;competitor@rival.com',  # å¤–éƒ¨é‚®ä»¶
            'cc': '',
            'bcc': 'personal@gmail.com',  # å¯†é€å¤–éƒ¨
            'size': '5120000',  # å¤§é‚®ä»¶
            'content': 'Confidential company strategy document attached',
            'activity': 'Send',
            'att': 'strategy.pdf(2048000);financial_data.xlsx(1024000)'  # å¤§é™„ä»¶
        },
        {
            'type': 'http',
            'date': '01/15/2024 19:15:00',
            'user': 'ACM2278',
            'url/fname': 'https://www.wikileaks.org/documents',  # é«˜é£é™©åŸŸå
            'content': 'Document leak website content',
            'activity': 'www upload'  # ä¸Šä¼ æ´»åŠ¨
        },
        {
            'type': 'file',
            'date': '01/15/2024 19:45:00',
            'user': 'ACM2278',
            'pc': 'PC-1234',
            'url/fname': 'C:\\Users\\ACM2278\\Desktop\\sensitive_employee_data.xlsx',
            'content': 'Employee personal information, salaries, SSN...',
            'activity': 'file copy',
            'to': 'true',  # USBä¼ è¾“
            'from': 'false'
        },
        {
            'type': 'device',
            'date': '01/15/2024 20:00:00',
            'user': 'ACM2278',
            'activity': 'USB Connect',
            'content': 'External USB drive detected - 64GB capacity',
            'file_tree_len': '1500'
        }
    ]
    
    # ç”¨æˆ·ä¸Šä¸‹æ–‡æ•°æ®
    users_data = [
        {
            'user_id': 'ACM2278',
            'role': 'Manager',
            'dept': 'Finance',
            'ITAdmin': 0,
            'O': 0.6,  # å¼€æ”¾æ€§
            'C': 0.9,  # è´£ä»»å¿ƒ
            'E': 0.7,  # å¤–å‘æ€§
            'A': 0.8,  # å®œäººæ€§
            'N': 0.2,  # ç¥ç»è´¨
            'pc_type': 0,
            'sharedpc': None,
            'npc': 1
        }
    ]
    
    events_df = pd.DataFrame(events_data)
    users_df = pd.DataFrame(users_data).set_index('user_id')
    
    return events_df, users_df

def demonstrate_basic_usage():
    """æ¼”ç¤ºåŸºç¡€ä½¿ç”¨æµç¨‹"""
    print("\nğŸ“ åŸºç¡€ä½¿ç”¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®
    events_df, users_df = create_sample_data()
    
    # 1. åˆå§‹åŒ–ç¼–ç å™¨
    print("1ï¸âƒ£ åˆå§‹åŒ–ç¼–ç å™¨")
    encoder = EventEncoder(feature_dim=256, data_version='r5.2')
    
    # 2. æ‹Ÿåˆç¼–ç å™¨
    print("2ï¸âƒ£ æ‹Ÿåˆç¼–ç å™¨")
    encoder.fit(events_df, users_df)
    
    # 3. ç¼–ç å•ä¸ªäº‹ä»¶
    print("3ï¸âƒ£ ç¼–ç å•ä¸ªäº‹ä»¶")
    event_dict = events_df.iloc[1].to_dict()  # é‚®ä»¶äº‹ä»¶
    user_context = users_df.loc['ACM2278'].to_dict()
    
    features, mask = encoder.encode_event(event_dict, user_context)
    
    print(f"   ğŸ“Š ç‰¹å¾å‘é‡ç»´åº¦: {features.shape}")
    print(f"   âœ… æœ‰æ•ˆç‰¹å¾æ•°: {mask.sum()}/{len(mask)} ({mask.mean():.1%})")
    print(f"   ğŸ“ˆ ç‰¹å¾å€¼èŒƒå›´: [{features.min():.3f}, {features.max():.3f}]")
    
    # 4. ç¼–ç äº‹ä»¶åºåˆ—
    print("4ï¸âƒ£ ç¼–ç äº‹ä»¶åºåˆ—")
    user_events = events_df[events_df['user'] == 'ACM2278'].to_dict('records')
    seq_features, seq_mask = encoder.encode_event_sequence(
        user_events, user_context, max_sequence_length=10
    )
    
    print(f"   ğŸ“Š åºåˆ—ç‰¹å¾çŸ©é˜µ: {seq_features.shape}")
    print(f"   âœ… åºåˆ—æœ‰æ•ˆç‡: {seq_mask.mean():.1%}")
    
    return encoder, events_df, users_df

def demonstrate_risk_analysis():
    """æ¼”ç¤ºé£é™©åˆ†æåŠŸèƒ½"""
    print("\nğŸš¨ é£é™©åˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    events_df, users_df = create_sample_data()
    
    # æŒ‰äº‹ä»¶ç±»å‹åˆ†ç»„
    user_events = events_df[events_df['user'] == 'ACM2278'].to_dict('records')
    user_context = users_df.loc['ACM2278'].to_dict()
    
    email_events = [e for e in user_events if e['type'] == 'email']
    http_events = [e for e in user_events if e['type'] == 'http']
    file_events = [e for e in user_events if e['type'] == 'file']
    device_events = [e for e in user_events if e['type'] == 'device']
    
    print("1ï¸âƒ£ ç”¨æˆ·ä¸Šä¸‹æ–‡é£é™©åˆ†æ")
    context_risk = encode_behavioral_risk_profile(user_context)
    print(f"   ğŸ¯ æ€»ä½“é£é™©è¯„åˆ†: {context_risk['overall_risk']:.3f}")
    print(f"   ğŸ”‘ æƒé™é£é™©: {context_risk['privilege_risk']:.3f}")
    print(f"   ğŸ“Š æ•°æ®è®¿é—®é£é™©: {context_risk['data_access_risk']:.3f}")
    print(f"   ğŸ§  å¿ƒç†é£é™©: {context_risk['psychological_risk']:.3f}")
    
    print("\n2ï¸âƒ£ é‚®ä»¶è¡Œä¸ºé£é™©åˆ†æ")
    if email_events:
        email_risks = detect_suspicious_email_patterns(email_events)
        for pattern, score in email_risks.items():
            if score > 0.1:
                print(f"   âš ï¸  {pattern}: {score:.3f}")
    
    print("\n3ï¸âƒ£ HTTPæµè§ˆé£é™©åˆ†æ")
    if http_events:
        http_risks = detect_suspicious_http_patterns(http_events)
        for pattern, score in http_risks.items():
            if score > 0.1:
                print(f"   âš ï¸  {pattern}: {score:.3f}")
    
    print("\n4ï¸âƒ£ æ–‡ä»¶æ“ä½œé£é™©åˆ†æ")
    if file_events:
        file_risks = detect_suspicious_file_patterns(file_events)
        for pattern, score in file_risks.items():
            if score > 0.1:
                print(f"   âš ï¸  {pattern}: {score:.3f}")
    
    print("\n5ï¸âƒ£ è®¾å¤‡ä½¿ç”¨é£é™©åˆ†æ")
    if device_events:
        device_risks = detect_suspicious_device_patterns(device_events)
        for pattern, score in device_risks.items():
            if score > 0.1:
                print(f"   âš ï¸  {pattern}: {score:.3f}")
    
    # ç»¼åˆé£é™©è¯„åˆ†
    all_risks = []
    if email_events:
        all_risks.extend(email_risks.values())
    if http_events:
        all_risks.extend(http_risks.values())
    if file_events:
        all_risks.extend(file_risks.values())
    if device_events:
        all_risks.extend(device_risks.values())
    
    overall_risk = (
        np.mean(list(email_risks.values()) if email_events else [0]) * 0.3 +
        np.mean(list(http_risks.values()) if http_events else [0]) * 0.3 +
        np.mean(list(file_risks.values()) if file_events else [0]) * 0.2 +
        context_risk['overall_risk'] * 0.2
    )
    
    print(f"\nğŸ¯ ç”¨æˆ·ç»¼åˆé£é™©è¯„åˆ†: {overall_risk:.3f}")
    
    # é£é™©ç­‰çº§åˆ¤æ–­
    if overall_risk > 0.7:
        risk_level = "ğŸ”´ é«˜é£é™©"
    elif overall_risk > 0.4:
        risk_level = "ğŸŸ¡ ä¸­é£é™©"
    else:
        risk_level = "ğŸŸ¢ ä½é£é™©"
    
    print(f"ğŸ·ï¸  é£é™©ç­‰çº§: {risk_level}")

def demonstrate_time_analysis():
    """æ¼”ç¤ºæ—¶é—´ç‰¹å¾åˆ†æ"""
    print("\nâ° æ—¶é—´ç‰¹å¾åˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    events_df, users_df = create_sample_data()
    encoder = EventEncoder(feature_dim=256, data_version='r5.2')
    encoder.fit(events_df, users_df)
    
    # ä¼šè¯çº§æ—¶é—´åˆ†æ
    user_events = events_df[events_df['user'] == 'ACM2278'].to_dict('records')
    
    session_temporal, _ = encode_session_temporal_features(user_events, encoder.feature_encoder)
    
    print("1ï¸âƒ£ ä¼šè¯æ—¶é—´ç‰¹å¾")
    print(f"   ğŸ“… ä¼šè¯æŒç»­æ—¶é•¿: {session_temporal[0]*480:.1f} åˆ†é’Ÿ")
    print(f"   â±ï¸  å¹³å‡äº‹ä»¶é—´éš”: {session_temporal[1]*60:.1f} åˆ†é’Ÿ")
    print(f"   â° å¼€å§‹æ—¶é—´: {session_temporal[3]*24:.1f} ç‚¹")
    print(f"   ğŸ•˜ å·¥ä½œæ—¶é—´æ´»åŠ¨æ¯”ä¾‹: {session_temporal[6]:.1%}")
    print(f"   ğŸŒ™ å¤œé—´æ´»åŠ¨æ¯”ä¾‹: {session_temporal[7]:.1%}")
    print(f"   ğŸ“… å‘¨æœ«æ´»åŠ¨æ¯”ä¾‹: {session_temporal[8]:.1%}")
    print(f"   ğŸ”„ æ˜¯å¦è·¨å¤©: {'æ˜¯' if session_temporal[9] > 0 else 'å¦'}")
    
    # æ£€æµ‹å¼‚å¸¸æ—¶é—´æ¨¡å¼
    print("\n2ï¸âƒ£ å¼‚å¸¸æ—¶é—´æ¨¡å¼æ£€æµ‹")
    
    # å¤œé—´æ´»åŠ¨æ£€æµ‹
    if session_temporal[7] > 0.3:  # è¶…è¿‡30%å¤œé—´æ´»åŠ¨
        print("   âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å¤œé—´æ´»åŠ¨")
    
    # å‘¨æœ«å·¥ä½œæ£€æµ‹
    if session_temporal[8] > 0.5:  # è¶…è¿‡50%å‘¨æœ«æ´»åŠ¨
        print("   âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å‘¨æœ«å·¥ä½œ")
    
    # é•¿ä¼šè¯æ£€æµ‹
    if session_temporal[0] > 0.5:  # è¶…è¿‡4å°æ—¶
        print("   âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸é•¿æ—¶é—´ä¼šè¯")
    
    # éå·¥ä½œæ—¶é—´æ£€æµ‹
    if session_temporal[6] < 0.5:  # å·¥ä½œæ—¶é—´æ´»åŠ¨å°‘äº50%
        print("   âš ï¸  æ£€æµ‹åˆ°å¤§é‡éå·¥ä½œæ—¶é—´æ´»åŠ¨")

def demonstrate_feature_visualization():
    """æ¼”ç¤ºç‰¹å¾å¯è§†åŒ–"""
    print("\nğŸ“Š ç‰¹å¾å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    events_df, users_df = create_sample_data()
    encoder = EventEncoder(feature_dim=256, data_version='r5.2')
    encoder.fit(events_df, users_df)
    
    # ç¼–ç æ‰€æœ‰äº‹ä»¶
    user_context = users_df.loc['ACM2278'].to_dict()
    all_features = []
    all_masks = []
    event_types = []
    
    for _, event in events_df.iterrows():
        if event['user'] == 'ACM2278':
            event_dict = event.to_dict()
            features, mask = encoder.encode_event(event_dict, user_context)
            all_features.append(features)
            all_masks.append(mask)
            event_types.append(event['type'])
    
    all_features = np.array(all_features)
    all_masks = np.array(all_masks)
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ç‰¹å¾æå–ç³»ç»Ÿå¯è§†åŒ–åˆ†æ', fontsize=16, fontweight='bold')
    
    # 1. ç‰¹å¾åˆ†å¸ƒç›´æ–¹å›¾
    axes[0, 0].hist(all_features[all_masks], bins=50, alpha=0.7, color='skyblue')
    axes[0, 0].set_title('æœ‰æ•ˆç‰¹å¾å€¼åˆ†å¸ƒ')
    axes[0, 0].set_xlabel('ç‰¹å¾å€¼')
    axes[0, 0].set_ylabel('é¢‘æ•°')
    
    # 2. maskæ¨¡å¼
    mask_pattern = all_masks.mean(axis=0)
    axes[0, 1].plot(mask_pattern, color='green', linewidth=2)
    axes[0, 1].set_title('ç‰¹å¾æœ‰æ•ˆæ€§æ¨¡å¼')
    axes[0, 1].set_xlabel('ç‰¹å¾ç´¢å¼•')
    axes[0, 1].set_ylabel('æœ‰æ•ˆç‡')
    axes[0, 1].set_ylim([0, 1])
    
    # 3. äº‹ä»¶ç±»å‹ç‰¹å¾çƒ­å›¾
    feature_by_type = {}
    for i, event_type in enumerate(event_types):
        if event_type not in feature_by_type:
            feature_by_type[event_type] = []
        valid_features = all_features[i][all_masks[i]]
        if len(valid_features) > 0:
            feature_by_type[event_type].append(valid_features.mean())
    
    type_names = list(feature_by_type.keys())
    type_scores = [np.mean(feature_by_type[t]) for t in type_names]
    
    bars = axes[1, 0].bar(type_names, type_scores, color=['red', 'blue', 'green', 'orange'])
    axes[1, 0].set_title('ä¸åŒäº‹ä»¶ç±»å‹çš„å¹³å‡ç‰¹å¾å€¼')
    axes[1, 0].set_ylabel('å¹³å‡ç‰¹å¾å€¼')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 4. æ—¶é—´åºåˆ—ç‰¹å¾
    time_features = []
    for _, event in events_df.iterrows():
        if event['user'] == 'ACM2278':
            timestamp = pd.to_datetime(event['date'])
            time_features.append(timestamp.hour + timestamp.minute/60.0)
    
    axes[1, 1].plot(time_features, 'o-', color='purple', linewidth=2, markersize=8)
    axes[1, 1].set_title('äº‹ä»¶æ—¶é—´åˆ†å¸ƒ')
    axes[1, 1].set_xlabel('äº‹ä»¶åºå·')
    axes[1, 1].set_ylabel('æ—¶é—´ (å°æ—¶)')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('feature_analysis.png', dpi=300, bbox_inches='tight')
    print("ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ä¸º 'feature_analysis.png'")
    plt.show()

def demonstrate_batch_processing():
    """æ¼”ç¤ºæ‰¹é‡å¤„ç†"""
    print("\nğŸ”„ æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ›´å¤§çš„ç¤ºä¾‹æ•°æ®é›†
    print("1ï¸âƒ£ åˆ›å»ºå¤§å‹æ•°æ®é›†")
    events_df, users_df = create_sample_data()
    
    # æ¨¡æ‹Ÿå¤šç”¨æˆ·æ•°æ®
    users = ['ACM2278', 'ACM1796', 'CMP2946', 'BTH8471', 'DYQ9624']
    expanded_events = []
    
    for user in users:
        for _, event in events_df.iterrows():
            new_event = event.copy()
            new_event['user'] = user
            # éšæœºè°ƒæ•´æ—¶é—´
            base_time = pd.to_datetime(event['date'])
            time_offset = timedelta(hours=np.random.randint(-5, 5))
            new_event['date'] = (base_time + time_offset).strftime('%m/%d/%Y %H:%M:%S')
            expanded_events.append(new_event)
    
    large_events_df = pd.DataFrame(expanded_events)
    print(f"   ğŸ“Š æ•°æ®é›†å¤§å°: {len(large_events_df)} æ¡äº‹ä»¶, {len(users)} ä¸ªç”¨æˆ·")
    
    # 2. æ‰¹é‡ç‰¹å¾æå–
    print("2ï¸âƒ£ æ‰¹é‡ç‰¹å¾æå–")
    encoder = EventEncoder(feature_dim=256, data_version='r5.2')
    encoder.fit(large_events_df, users_df)
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºéšæœºç”¨æˆ·ä¸Šä¸‹æ–‡
    user_contexts = {}
    for user in users:
        user_contexts[user] = {
            'user_id': user,
            'role': np.random.choice(['Employee', 'Manager', 'Director']),
            'dept': np.random.choice(['IT', 'Finance', 'HR', 'Marketing']),
            'ITAdmin': np.random.choice([0, 1]),
            'O': np.random.uniform(0.2, 0.8),
            'C': np.random.uniform(0.2, 0.8),
            'E': np.random.uniform(0.2, 0.8),
            'A': np.random.uniform(0.2, 0.8),
            'N': np.random.uniform(0.2, 0.8)
        }
    
    # æ‰¹é‡å¤„ç†
    batch_size = 10
    user_features = {}
    
    for user in users:
        user_events = large_events_df[large_events_df['user'] == user].to_dict('records')
        user_context = user_contexts[user]
        
        # åˆ†æ‰¹å¤„ç†äº‹ä»¶
        all_features = []
        for i in range(0, len(user_events), batch_size):
            batch_events = user_events[i:i+batch_size]
            for event in batch_events:
                features, mask = encoder.encode_event(event, user_context)
                all_features.append(features)
        
        user_features[user] = np.array(all_features)
    
    print(f"   âœ… å®Œæˆ {len(users)} ä¸ªç”¨æˆ·çš„ç‰¹å¾æå–")
    
    # 3. ç»Ÿè®¡åˆ†æ
    print("3ï¸âƒ£ æ‰¹é‡ç»Ÿè®¡åˆ†æ")
    
    # è®¡ç®—å„ç”¨æˆ·çš„å¹³å‡ç‰¹å¾
    user_avg_features = {}
    for user, features in user_features.items():
        user_avg_features[user] = features.mean(axis=0)
    
    # ç”¨æˆ·ç›¸ä¼¼åº¦åˆ†æ
    print("   ğŸ“Š ç”¨æˆ·ç›¸ä¼¼åº¦åˆ†æ:")
    users_list = list(user_avg_features.keys())
    for i in range(len(users_list)):
        for j in range(i+1, len(users_list)):
            user1, user2 = users_list[i], users_list[j]
            similarity = np.corrcoef(
                user_avg_features[user1], 
                user_avg_features[user2]
            )[0, 1]
            print(f"     {user1} â†” {user2}: {similarity:.3f}")
    
    # 4. ä¿å­˜ç»“æœ
    print("4ï¸âƒ£ ä¿å­˜å¤„ç†ç»“æœ")
    import pickle
    
    # ä¿å­˜ç‰¹å¾æ•°æ®
    with open('batch_features.pkl', 'wb') as f:
        pickle.dump(user_features, f)
    
    # ä¿å­˜ç¼–ç å™¨
    encoder.save_encoder('batch_encoder.pkl')
    
    print("   ğŸ’¾ ç»“æœå·²ä¿å­˜: batch_features.pkl, batch_encoder.pkl")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ å†…éƒ¨å¨èƒæ£€æµ‹ç‰¹å¾æå–ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    print("è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºç³»ç»Ÿçš„ä¸»è¦åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•")
    
    try:
        # åŸºç¡€ä½¿ç”¨æ¼”ç¤º
        encoder, events_df, users_df = demonstrate_basic_usage()
        
        # é£é™©åˆ†ææ¼”ç¤º
        demonstrate_risk_analysis()
        
        # æ—¶é—´ç‰¹å¾åˆ†ææ¼”ç¤º
        demonstrate_time_analysis()
        
        # ç‰¹å¾å¯è§†åŒ–æ¼”ç¤º
        demonstrate_feature_visualization()
        
        # æ‰¹é‡å¤„ç†æ¼”ç¤º
        demonstrate_batch_processing()
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("ğŸ“š æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ README.md å’Œ API_REFERENCE.md")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…ï¼Œæˆ–æŸ¥çœ‹æ•…éšœæ’é™¤æŒ‡å—")

if __name__ == "__main__":
    main() 