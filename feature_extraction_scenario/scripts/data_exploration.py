#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CERT r4.2 æ•°æ®é›†æ¢ç´¢å’Œé¢„å¤„ç†ç­–ç•¥
ç”¨äºåä¸ºå¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿçš„æ•°æ®åˆ†æ
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
from typing import Dict, List, Tuple, Optional
import json
from collections import Counter, defaultdict

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾æ ·å¼
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8')

class CERTDataExplorer:
    """CERTæ•°æ®é›†æ¢ç´¢å™¨"""
    
    def __init__(self, data_version: str = 'r4.2'):
        """
        åˆå§‹åŒ–æ•°æ®æ¢ç´¢å™¨
        
        Args:
            data_version: æ•°æ®ç‰ˆæœ¬
        """
        self.data_version = data_version
        self.data_dir = f'../data/{data_version}/'
        
        # æ•°æ®æ–‡ä»¶é…ç½®
        self.data_files = {
            'device': 'device.csv',
            'email': 'email.csv', 
            'file': 'file.csv',
            'http': 'http.csv',
            'logon': 'logon.csv',
            'psychometric': 'psychometric.csv'
        }
        
        # æ¶æ„ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶
        self.insiders_file = os.path.join(self.data_dir, 'answers', 'insiders.csv')
        self.ldap_dir = os.path.join(self.data_dir, 'LDAP')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = './exploration_results'
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸ” CERT {data_version} æ•°æ®æ¢ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {os.path.abspath(self.data_dir)}")
        print(f"ğŸ“Š è¾“å‡ºç›®å½•: {os.path.abspath(self.output_dir)}")
    
    def check_data_availability(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ•°æ®æ–‡ä»¶å¯ç”¨æ€§"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ•°æ®æ–‡ä»¶å¯ç”¨æ€§æ£€æŸ¥")
        print("="*60)
        
        availability = {}
        total_size = 0
        
        for data_type, filename in self.data_files.items():
            file_path = os.path.join(self.data_dir, filename)
            exists = os.path.exists(file_path)
            availability[data_type] = exists
            
            if exists:
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                total_size += size_mb
                status = f"âœ… å­˜åœ¨ ({size_mb:.1f} MB)"
            else:
                status = "âŒ ç¼ºå¤±"
            
            print(f"  {data_type:12s}: {status}")
        
        # æ£€æŸ¥å…¶ä»–é‡è¦æ–‡ä»¶
        other_files = {
            'insiders': self.insiders_file,
            'LDAPç›®å½•': self.ldap_dir
        }
        
        for name, path in other_files.items():
            exists = os.path.exists(path)
            availability[name] = exists
            status = "âœ… å­˜åœ¨" if exists else "âŒ ç¼ºå¤±"
            print(f"  {name:12s}: {status}")
        
        print(f"\nğŸ“Š æ€»æ•°æ®å¤§å°: {total_size:.1f} MB")
        print(f"ğŸ“ˆ æ•°æ®å®Œæ•´æ€§: {sum(availability.values())}/{len(availability)} ä¸ªæ–‡ä»¶å¯ç”¨")
        
        return availability
    
    def load_sample_data(self, sample_size: int = 10000) -> Dict[str, pd.DataFrame]:
        """åŠ è½½æ ·æœ¬æ•°æ®è¿›è¡Œå¿«é€Ÿæ¢ç´¢"""
        print(f"\nğŸ“¥ åŠ è½½æ ·æœ¬æ•°æ® (æ¯ä¸ªæ–‡ä»¶ {sample_size:,} è¡Œ)")
        print("-" * 40)
        
        sample_data = {}
        
        for data_type, filename in self.data_files.items():
            file_path = os.path.join(self.data_dir, filename)
            
            if not os.path.exists(file_path):
                print(f"âš ï¸  è·³è¿‡ {data_type}: æ–‡ä»¶ä¸å­˜åœ¨")
                continue
            
            try:
                # è¯»å–æ ·æœ¬æ•°æ®
                df = pd.read_csv(file_path, nrows=sample_size)
                sample_data[data_type] = df
                
                print(f"âœ… {data_type:8s}: {len(df):,} è¡Œ x {len(df.columns)} åˆ—")
                
            except Exception as e:
                print(f"âŒ {data_type:8s}: è¯»å–å¤±è´¥ - {e}")
        
        return sample_data
    
    def analyze_data_schema(self, sample_data: Dict[str, pd.DataFrame]) -> Dict:
        """åˆ†ææ•°æ®æ¨¡å¼å’Œç»“æ„"""
        print(f"\nğŸ” æ•°æ®æ¨¡å¼åˆ†æ")
        print("="*60)
        
        schema_info = {}
        
        for data_type, df in sample_data.items():
            print(f"\nğŸ“Š {data_type.upper()} æ•°æ®ç»“æ„:")
            print("-" * 30)
            
            # åŸºæœ¬ä¿¡æ¯
            info = {
                'shape': df.shape,
                'columns': list(df.columns),
                'dtypes': df.dtypes.to_dict(),
                'missing_values': df.isnull().sum().to_dict(),
                'unique_counts': df.nunique().to_dict()
            }
            
            schema_info[data_type] = info
            
            # æ‰“å°åˆ—ä¿¡æ¯
            for col in df.columns:
                dtype = df[col].dtype
                missing = df[col].isnull().sum()
                unique = df[col].nunique()
                missing_pct = (missing / len(df)) * 100
                
                print(f"  {col:15s}: {str(dtype):10s} | "
                      f"ç¼ºå¤±: {missing:4d} ({missing_pct:5.1f}%) | "
                      f"å”¯ä¸€å€¼: {unique:6d}")
            
            # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
            print(f"\nğŸ“‹ {data_type} æ ·æœ¬æ•°æ®:")
            print(df.head(3).to_string())
        
        return schema_info
    
    def analyze_temporal_patterns(self, sample_data: Dict[str, pd.DataFrame]) -> Dict:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        print(f"\nâ° æ—¶é—´æ¨¡å¼åˆ†æ")
        print("="*60)
        
        temporal_info = {}
        
        for data_type, df in sample_data.items():
            if 'date' not in df.columns:
                print(f"âš ï¸  {data_type}: æ— æ—¥æœŸåˆ—ï¼Œè·³è¿‡æ—¶é—´åˆ†æ")
                continue
            
            print(f"\nğŸ“… {data_type.upper()} æ—¶é—´åˆ†æ:")
            print("-" * 30)
            
            # è½¬æ¢æ—¥æœŸ
            df['date'] = pd.to_datetime(df['date'])
            
            # æ—¶é—´èŒƒå›´
            date_range = {
                'start_date': df['date'].min(),
                'end_date': df['date'].max(),
                'duration_days': (df['date'].max() - df['date'].min()).days
            }
            
            print(f"  æ—¶é—´èŒƒå›´: {date_range['start_date']} åˆ° {date_range['end_date']}")
            print(f"  æŒç»­å¤©æ•°: {date_range['duration_days']} å¤©")
            
            # æŒ‰å‘¨ç»Ÿè®¡
            df['week'] = ((df['date'] - df['date'].min()).dt.days // 7)
            week_counts = df['week'].value_counts().sort_index()
            
            print(f"  å‘¨æ•°èŒƒå›´: ç¬¬ {week_counts.index.min()} å‘¨ åˆ° ç¬¬ {week_counts.index.max()} å‘¨")
            print(f"  å¹³å‡æ¯å‘¨äº‹ä»¶: {week_counts.mean():.1f}")
            
            # æŒ‰å°æ—¶ç»Ÿè®¡
            df['hour'] = df['date'].dt.hour
            hour_dist = df['hour'].value_counts().sort_index()
            
            print(f"  æ´»è·ƒæ—¶é—´: {hour_dist.index.min()}:00 - {hour_dist.index.max()}:00")
            print(f"  å³°å€¼æ—¶é—´: {hour_dist.idxmax()}:00 ({hour_dist.max()} äº‹ä»¶)")
            
            temporal_info[data_type] = {
                'date_range': date_range,
                'week_distribution': week_counts.to_dict(),
                'hour_distribution': hour_dist.to_dict()
            }
        
        return temporal_info
    
    def analyze_user_patterns(self, sample_data: Dict[str, pd.DataFrame]) -> Dict:
        """åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼"""
        print(f"\nğŸ‘¥ ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ")
        print("="*60)
        
        user_info = {}
        all_users = set()
        
        for data_type, df in sample_data.items():
            if 'user' not in df.columns:
                print(f"âš ï¸  {data_type}: æ— ç”¨æˆ·åˆ—ï¼Œè·³è¿‡ç”¨æˆ·åˆ†æ")
                continue
            
            print(f"\nğŸ‘¤ {data_type.upper()} ç”¨æˆ·åˆ†æ:")
            print("-" * 30)
            
            users = df['user'].dropna()
            user_counts = users.value_counts()
            all_users.update(users.unique())
            
            print(f"  ç”¨æˆ·æ€»æ•°: {len(user_counts)}")
            print(f"  äº‹ä»¶æ€»æ•°: {len(users)}")
            print(f"  å¹³å‡æ¯ç”¨æˆ·äº‹ä»¶: {len(users) / len(user_counts):.1f}")
            print(f"  æœ€æ´»è·ƒç”¨æˆ·: {user_counts.index[0]} ({user_counts.iloc[0]} äº‹ä»¶)")
            print(f"  äº‹ä»¶åˆ†å¸ƒ:")
            print(f"    å‰10%ç”¨æˆ·å äº‹ä»¶: {user_counts.head(len(user_counts)//10).sum() / len(users) * 100:.1f}%")
            
            user_info[data_type] = {
                'total_users': len(user_counts),
                'total_events': len(users),
                'top_users': user_counts.head(10).to_dict(),
                'user_distribution': user_counts.describe().to_dict()
            }
        
        print(f"\nğŸŒ è·¨æ•°æ®æºç”¨æˆ·åˆ†æ:")
        print(f"  æ€»å”¯ä¸€ç”¨æˆ·æ•°: {len(all_users)}")
        
        return user_info
    
    def load_insider_labels(self) -> pd.DataFrame:
        """åŠ è½½å†…éƒ¨å¨èƒè€…æ ‡ç­¾"""
        print(f"\nğŸš¨ å†…éƒ¨å¨èƒè€…æ ‡ç­¾åˆ†æ")
        print("="*60)
        
        if not os.path.exists(self.insiders_file):
            print(f"âŒ å†…éƒ¨å¨èƒè€…æ–‡ä»¶ä¸å­˜åœ¨: {self.insiders_file}")
            return pd.DataFrame()
        
        try:
            insiders_df = pd.read_csv(self.insiders_file)
            print(f"âœ… æˆåŠŸåŠ è½½å†…éƒ¨å¨èƒè€…æ•°æ®: {len(insiders_df)} ä¸ªå¨èƒè€…")
            
            print(f"\nğŸ“‹ å¨èƒè€…ä¿¡æ¯:")
            print("-" * 30)
            
            for _, row in insiders_df.iterrows():
                user = row['user']
                scenario = row.get('scenario', 'Unknown')
                start = row.get('start', 'Unknown')
                end = row.get('end', 'Unknown')
                
                print(f"  {user}: åœºæ™¯ {scenario}, æ—¶é—´ {start} - {end}")
            
            # åˆ†æå¨èƒåœºæ™¯åˆ†å¸ƒ
            if 'scenario' in insiders_df.columns:
                scenario_counts = insiders_df['scenario'].value_counts()
                print(f"\nğŸ“Š å¨èƒåœºæ™¯åˆ†å¸ƒ:")
                for scenario, count in scenario_counts.items():
                    print(f"  åœºæ™¯ {scenario}: {count} ä¸ªç”¨æˆ·")
            
            return insiders_df
            
        except Exception as e:
            print(f"âŒ åŠ è½½å†…éƒ¨å¨èƒè€…æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def analyze_data_quality(self, sample_data: Dict[str, pd.DataFrame]) -> Dict:
        """åˆ†ææ•°æ®è´¨é‡"""
        print(f"\nğŸ” æ•°æ®è´¨é‡åˆ†æ")
        print("="*60)
        
        quality_report = {}
        
        for data_type, df in sample_data.items():
            print(f"\nğŸ“Š {data_type.upper()} æ•°æ®è´¨é‡:")
            print("-" * 30)
            
            # ç¼ºå¤±å€¼åˆ†æ
            missing_analysis = {}
            total_cells = len(df) * len(df.columns)
            missing_cells = df.isnull().sum().sum()
            
            print(f"  æ€»å•å…ƒæ ¼æ•°: {total_cells:,}")
            print(f"  ç¼ºå¤±å•å…ƒæ ¼: {missing_cells:,} ({missing_cells/total_cells*100:.2f}%)")
            
            # æŒ‰åˆ—åˆ†æç¼ºå¤±å€¼
            missing_by_col = df.isnull().sum()
            missing_cols = missing_by_col[missing_by_col > 0]
            
            if len(missing_cols) > 0:
                print(f"  ç¼ºå¤±å€¼åˆ—æ•°: {len(missing_cols)}/{len(df.columns)}")
                for col, missing_count in missing_cols.items():
                    pct = missing_count / len(df) * 100
                    print(f"    {col}: {missing_count} ({pct:.1f}%)")
            else:
                print(f"  âœ… æ— ç¼ºå¤±å€¼")
            
            # é‡å¤å€¼åˆ†æ
            duplicates = df.duplicated().sum()
            print(f"  é‡å¤è¡Œæ•°: {duplicates} ({duplicates/len(df)*100:.2f}%)")
            
            # æ•°æ®ç±»å‹ä¸€è‡´æ€§
            type_issues = []
            for col in df.columns:
                if df[col].dtype == 'object':
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜¯æ•°å€¼ç±»å‹
                    try:
                        pd.to_numeric(df[col].dropna())
                        type_issues.append(f"{col} (å¯èƒ½åº”ä¸ºæ•°å€¼å‹)")
                    except:
                        pass
            
            if type_issues:
                print(f"  ç±»å‹é—®é¢˜: {', '.join(type_issues)}")
            else:
                print(f"  âœ… æ•°æ®ç±»å‹ä¸€è‡´")
            
            quality_report[data_type] = {
                'missing_percentage': missing_cells/total_cells*100,
                'duplicate_percentage': duplicates/len(df)*100,
                'missing_columns': missing_by_col.to_dict(),
                'type_issues': type_issues
            }
        
        return quality_report
    
    def create_data_summary_report(self, schema_info: Dict, temporal_info: Dict, 
                                 user_info: Dict, quality_report: Dict, 
                                 insiders_df: pd.DataFrame) -> str:
        """åˆ›å»ºæ•°æ®æ‘˜è¦æŠ¥å‘Š"""
        print(f"\nğŸ“ ç”Ÿæˆæ•°æ®æ‘˜è¦æŠ¥å‘Š")
        print("="*60)
        
        report = []
        report.append("# CERT r4.2 æ•°æ®é›†æ¢ç´¢æŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # æ•°æ®æ¦‚è§ˆ
        report.append("## ğŸ“Š æ•°æ®æ¦‚è§ˆ")
        report.append("")
        total_events = sum(info['shape'][0] for info in schema_info.values())
        total_columns = sum(len(info['columns']) for info in schema_info.values())
        
        report.append(f"- **æ•°æ®æºæ•°é‡**: {len(schema_info)} ä¸ª")
        report.append(f"- **æ€»äº‹ä»¶æ•°**: {total_events:,} (æ ·æœ¬)")
        report.append(f"- **æ€»å­—æ®µæ•°**: {total_columns}")
        report.append(f"- **å†…éƒ¨å¨èƒè€…**: {len(insiders_df)} ä¸ª")
        report.append("")
        
        # å„æ•°æ®æºè¯¦æƒ…
        report.append("## ğŸ“‹ æ•°æ®æºè¯¦æƒ…")
        report.append("")
        
        for data_type, info in schema_info.items():
            report.append(f"### {data_type.upper()}")
            report.append(f"- **ç»´åº¦**: {info['shape'][0]:,} è¡Œ x {info['shape'][1]} åˆ—")
            report.append(f"- **å­—æ®µ**: {', '.join(info['columns'])}")
            
            if data_type in quality_report:
                quality = quality_report[data_type]
                report.append(f"- **æ•°æ®è´¨é‡**: ç¼ºå¤±ç‡ {quality['missing_percentage']:.2f}%, é‡å¤ç‡ {quality['duplicate_percentage']:.2f}%")
            
            if data_type in temporal_info:
                temporal = temporal_info[data_type]
                date_range = temporal['date_range']
                report.append(f"- **æ—¶é—´èŒƒå›´**: {date_range['start_date'].date()} åˆ° {date_range['end_date'].date()} ({date_range['duration_days']} å¤©)")
            
            if data_type in user_info:
                user = user_info[data_type]
                report.append(f"- **ç”¨æˆ·æ•°**: {user['total_users']:,} ä¸ªç”¨æˆ·, å¹³å‡æ¯ç”¨æˆ· {user['total_events']/user['total_users']:.1f} äº‹ä»¶")
            
            report.append("")
        
        # æ•°æ®è´¨é‡æ€»ç»“
        report.append("## ğŸ” æ•°æ®è´¨é‡æ€»ç»“")
        report.append("")
        
        avg_missing = np.mean([q['missing_percentage'] for q in quality_report.values()])
        avg_duplicate = np.mean([q['duplicate_percentage'] for q in quality_report.values()])
        
        report.append(f"- **å¹³å‡ç¼ºå¤±ç‡**: {avg_missing:.2f}%")
        report.append(f"- **å¹³å‡é‡å¤ç‡**: {avg_duplicate:.2f}%")
        
        # ä¸»è¦å‘ç°
        report.append("## ğŸ¯ ä¸»è¦å‘ç°")
        report.append("")
        report.append("### æ•°æ®ç‰¹ç‚¹")
        
        # æ‰¾å‡ºæœ€å¤§çš„æ•°æ®æº
        largest_source = max(schema_info.items(), key=lambda x: x[1]['shape'][0])
        report.append(f"- **æœ€å¤§æ•°æ®æº**: {largest_source[0]} ({largest_source[1]['shape'][0]:,} äº‹ä»¶)")
        
        # æ‰¾å‡ºç”¨æˆ·æœ€å¤šçš„æ•°æ®æº
        if user_info:
            most_users_source = max(user_info.items(), key=lambda x: x[1]['total_users'])
            report.append(f"- **ç”¨æˆ·æœ€å¤š**: {most_users_source[0]} ({most_users_source[1]['total_users']:,} ç”¨æˆ·)")
        
        # æ•°æ®è´¨é‡é—®é¢˜
        report.append("")
        report.append("### æ•°æ®è´¨é‡é—®é¢˜")
        
        high_missing = [name for name, q in quality_report.items() if q['missing_percentage'] > 10]
        if high_missing:
            report.append(f"- **é«˜ç¼ºå¤±ç‡æ•°æ®æº**: {', '.join(high_missing)}")
        
        high_duplicate = [name for name, q in quality_report.items() if q['duplicate_percentage'] > 5]
        if high_duplicate:
            report.append(f"- **é«˜é‡å¤ç‡æ•°æ®æº**: {', '.join(high_duplicate)}")
        
        # é¢„å¤„ç†å»ºè®®
        report.append("")
        report.append("## ğŸ’¡ é¢„å¤„ç†å»ºè®®")
        report.append("")
        report.append("### æ•°æ®æ¸…æ´—")
        report.append("- å¤„ç†ç¼ºå¤±å€¼ï¼šæ ¹æ®ä¸šåŠ¡é€»è¾‘å¡«å……æˆ–åˆ é™¤")
        report.append("- å»é™¤é‡å¤è®°å½•ï¼šä¿ç•™æœ€æ–°æˆ–æœ€å®Œæ•´çš„è®°å½•")
        report.append("- æ•°æ®ç±»å‹è½¬æ¢ï¼šç¡®ä¿æ—¥æœŸã€æ•°å€¼ç±»å‹æ­£ç¡®")
        report.append("")
        
        report.append("### ç‰¹å¾å·¥ç¨‹")
        report.append("- æ—¶é—´ç‰¹å¾ï¼šæå–å°æ—¶ã€æ˜ŸæœŸã€æœˆä»½ç­‰æ—¶é—´ç‰¹å¾")
        report.append("- ç”¨æˆ·ç‰¹å¾ï¼šè®¡ç®—ç”¨æˆ·æ´»åŠ¨é¢‘ç‡ã€æ¨¡å¼ç­‰")
        report.append("- è¡Œä¸ºåºåˆ—ï¼šæ„å»ºç”¨æˆ·è¡Œä¸ºæ—¶é—´åºåˆ—")
        report.append("- å¼‚å¸¸æ ‡æ³¨ï¼šåŸºäºå†…éƒ¨å¨èƒè€…æ ‡ç­¾åˆ›å»ºè®­ç»ƒæ ‡ç­¾")
        report.append("")
        
        report.append("### å¤šæ¨¡æ€èåˆ")
        report.append("- æ–‡æœ¬ç‰¹å¾ï¼šé‚®ä»¶å†…å®¹ã€æ–‡ä»¶åç­‰æ–‡æœ¬ä¿¡æ¯")
        report.append("- ç»“æ„åŒ–ç‰¹å¾ï¼šç™»å½•æ—¶é—´ã€è®¾å¤‡ä¿¡æ¯ç­‰")
        report.append("- å›¾ç‰¹å¾ï¼šç”¨æˆ·å…³ç³»ç½‘ç»œã€é€šä¿¡å›¾ç­‰")
        report.append("- æ—¶åºç‰¹å¾ï¼šè¡Œä¸ºåºåˆ—ã€æ—¶é—´æ¨¡å¼ç­‰")
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report)
        report_file = os.path.join(self.output_dir, 'data_exploration_report.md')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report_text
    
    def create_visualization_dashboard(self, sample_data: Dict[str, pd.DataFrame],
                                     temporal_info: Dict, user_info: Dict,
                                     schema_info: Dict) -> None:
        """åˆ›å»ºå¯è§†åŒ–ä»ªè¡¨æ¿"""
        print(f"\nğŸ“Š åˆ›å»ºå¯è§†åŒ–ä»ªè¡¨æ¿")
        print("="*60)
        
        # è®¾ç½®å›¾å½¢å¤§å°
        fig = plt.figure(figsize=(20, 15))
        
        # 1. æ•°æ®æºå¤§å°å¯¹æ¯”
        plt.subplot(2, 3, 1)
        if schema_info: # ç¡®ä¿ schema_info å­˜åœ¨
            data_sizes = [info['shape'][0] for info in schema_info.values()]
            data_names = list(schema_info.keys())

            plt.bar(data_names, data_sizes, color='skyblue', alpha=0.7)
            plt.title('Data Source Sizes (Sample)', fontsize=14, fontweight='bold')
            plt.xlabel('Data Source')
            plt.ylabel('Number of Records')
            plt.xticks(rotation=45)
        else:
            print("âš ï¸ æ— æ³•ç”Ÿæˆæ•°æ®æºå¤§å°å¯¹æ¯”å›¾ï¼šç¼ºå°‘ schema_info æ•°æ®")
        
        # 2. æ—¶é—´åˆ†å¸ƒï¼ˆä»¥ç¬¬ä¸€ä¸ªæœ‰æ—¶é—´æ•°æ®çš„æºä¸ºä¾‹ï¼‰
        plt.subplot(2, 3, 2)
        if temporal_info:
            first_temporal = list(temporal_info.values())[0]
            weeks = list(first_temporal['week_distribution'].keys())
            counts = list(first_temporal['week_distribution'].values())
            
            plt.plot(weeks, counts, marker='o', linewidth=2, markersize=4)
            plt.title('Weekly Activity Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Week Number')
            plt.ylabel('Number of Events')
            plt.grid(True, alpha=0.3)
        
        # 3. ç”¨æˆ·æ´»åŠ¨åˆ†å¸ƒ
        plt.subplot(2, 3, 3)
        if user_info:
            # åˆå¹¶æ‰€æœ‰ç”¨æˆ·äº‹ä»¶æ•°
            all_user_events = []
            for info in user_info.values():
                all_user_events.extend(info['top_users'].values())
            
            plt.hist(all_user_events, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')
            plt.title('User Activity Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Events per User')
            plt.ylabel('Number of Users')
            plt.yscale('log')
        
        # 4. å°æ—¶æ´»åŠ¨æ¨¡å¼
        plt.subplot(2, 3, 4)
        if temporal_info:
            # åˆå¹¶æ‰€æœ‰å°æ—¶åˆ†å¸ƒ
            hour_totals = defaultdict(int)
            for info in temporal_info.values():
                for hour, count in info['hour_distribution'].items():
                    hour_totals[hour] += count
            
            hours = sorted(hour_totals.keys())
            counts = [hour_totals[h] for h in hours]
            
            plt.bar(hours, counts, color='orange', alpha=0.7)
            plt.title('Hourly Activity Pattern', fontsize=14, fontweight='bold')
            plt.xlabel('Hour of Day')
            plt.ylabel('Number of Events')
            plt.xticks(range(0, 24, 2))
        
        # 5. æ•°æ®è´¨é‡çƒ­å›¾
        plt.subplot(2, 3, 5)
        quality_data = []
        quality_labels = []
        
        for data_type, df in sample_data.items():
            missing_rates = (df.isnull().sum() / len(df) * 100).values
            quality_data.append(missing_rates)
            quality_labels.append(data_type)
        
        if quality_data:
            # ç¡®ä¿æ‰€æœ‰æ•°ç»„é•¿åº¦ä¸€è‡´
            max_cols = max(len(row) for row in quality_data)
            quality_matrix = np.zeros((len(quality_data), max_cols))
            
            for i, row in enumerate(quality_data):
                quality_matrix[i, :len(row)] = row
            
            im = plt.imshow(quality_matrix, cmap='Reds', aspect='auto')
            plt.title('Missing Data Heatmap (%)', fontsize=14, fontweight='bold')
            plt.xlabel('Column Index')
            plt.ylabel('Data Source')
            plt.yticks(range(len(quality_labels)), quality_labels)
            plt.colorbar(im, shrink=0.8)
        
        # 6. æ•°æ®æºå…³ç³»ï¼ˆç”¨æˆ·é‡å ï¼‰
        plt.subplot(2, 3, 6)
        if len(sample_data) >= 2:
            # è®¡ç®—ç”¨æˆ·é‡å 
            user_sets = {}
            for data_type, df in sample_data.items():
                if 'user' in df.columns:
                    user_sets[data_type] = set(df['user'].dropna().unique())
            
            if len(user_sets) >= 2:
                # åˆ›å»ºé‡å çŸ©é˜µ
                sources = list(user_sets.keys())
                overlap_matrix = np.zeros((len(sources), len(sources)))
                
                for i, source1 in enumerate(sources):
                    for j, source2 in enumerate(sources):
                        if i == j:
                            overlap_matrix[i, j] = len(user_sets[source1])
                        else:
                            overlap = len(user_sets[source1] & user_sets[source2])
                            overlap_matrix[i, j] = overlap
                
                im = plt.imshow(overlap_matrix, cmap='Blues', aspect='auto')
                plt.title('User Overlap Between Sources', fontsize=14, fontweight='bold')
                plt.xlabel('Data Source')
                plt.ylabel('Data Source')
                plt.xticks(range(len(sources)), sources, rotation=45)
                plt.yticks(range(len(sources)), sources)
                plt.colorbar(im, shrink=0.8)
                
                # æ·»åŠ æ•°å€¼æ ‡æ³¨
                for i in range(len(sources)):
                    for j in range(len(sources)):
                        plt.text(j, i, f'{int(overlap_matrix[i, j])}', 
                               ha='center', va='center', fontsize=10)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        dashboard_file = os.path.join(self.output_dir, 'data_exploration_dashboard.png')
        plt.savefig(dashboard_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å¯è§†åŒ–ä»ªè¡¨æ¿å·²ä¿å­˜: {dashboard_file}")
    
    def run_full_exploration(self, sample_size: int = 10000) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ¢ç´¢æµç¨‹"""
        print("ğŸš€ å¯åŠ¨CERTæ•°æ®é›†å®Œæ•´æ¢ç´¢æµç¨‹")
        print("="*80)
        
        start_time = datetime.now()
        
        try:
            # 1. æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
            availability = self.check_data_availability()
            
            # 2. åŠ è½½æ ·æœ¬æ•°æ®
            sample_data = self.load_sample_data(sample_size)
            
            if not sample_data:
                print("âŒ æ— æ³•åŠ è½½ä»»ä½•æ•°æ®ï¼Œç»ˆæ­¢æ¢ç´¢")
                return {}
            
            # 3. åˆ†ææ•°æ®æ¨¡å¼
            schema_info = self.analyze_data_schema(sample_data)
            
            # 4. åˆ†ææ—¶é—´æ¨¡å¼
            temporal_info = self.analyze_temporal_patterns(sample_data)
            
            # 5. åˆ†æç”¨æˆ·æ¨¡å¼
            user_info = self.analyze_user_patterns(sample_data)
            
            # 6. åŠ è½½å¨èƒè€…æ ‡ç­¾
            insiders_df = self.load_insider_labels()
            
            # 7. åˆ†ææ•°æ®è´¨é‡
            quality_report = self.analyze_data_quality(sample_data)
            
            # 8. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            report = self.create_data_summary_report(
                schema_info, temporal_info, user_info, quality_report, insiders_df
            )
            
            # 9. åˆ›å»ºå¯è§†åŒ–ä»ªè¡¨æ¿
            self.create_visualization_dashboard(sample_data, temporal_info, user_info, schema_info)
            
            # 10. ä¿å­˜æ¢ç´¢ç»“æœ
            exploration_results = {
                'availability': availability,
                'schema_info': schema_info,
                'temporal_info': temporal_info,
                'user_info': user_info,
                'quality_report': quality_report,
                'insiders_count': len(insiders_df),
                'exploration_time': (datetime.now() - start_time).total_seconds()
            }
            
            results_file = os.path.join(self.output_dir, 'exploration_results.json')
            with open(results_file, 'w', encoding='utf-8') as f:
                # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
                serializable_results = {}
                for key, value in exploration_results.items():
                    if key == 'temporal_info':
                        # è½¬æ¢æ—¥æœŸæ—¶é—´å¯¹è±¡
                        serializable_value = {}
                        for data_type, info in value.items():
                            serializable_value[data_type] = {}
                            for k, v in info.items():
                                if k == 'date_range':
                                    serializable_value[data_type][k] = {
                                        'start_date': v['start_date'].isoformat(),
                                        'end_date': v['end_date'].isoformat(),
                                        'duration_days': v['duration_days']
                                    }
                                else:
                                    serializable_value[data_type][k] = v
                        serializable_results[key] = serializable_value
                    elif key == 'schema_info':
                         # è½¬æ¢ dtype å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        serializable_value = {}
                        for data_type, info in value.items():
                            serializable_value[data_type] = {
                                'shape': info['shape'],
                                'columns': info['columns'],
                                # å°† dtype å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                'dtypes': {col: str(dtype) for col, dtype in info['dtypes'].items()},
                                'missing_values': info['missing_values'],
                                'unique_counts': info['unique_counts']
                            }
                        serializable_results[key] = serializable_value
                    else:
                        serializable_results[key] = value
                
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            
            total_time = (datetime.now() - start_time).total_seconds()
            
            print(f"\nğŸ‰ æ•°æ®æ¢ç´¢å®Œæˆ!")
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {os.path.abspath(self.output_dir)}")
            print(f"ğŸ“Š æ¢ç´¢äº† {len(sample_data)} ä¸ªæ•°æ®æºï¼Œå…± {sum(len(df) for df in sample_data.values()):,} æ¡è®°å½•")
            
            return exploration_results
            
        except Exception as e:
            print(f"âŒ æ•°æ®æ¢ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” CERT r4.2 æ•°æ®é›†æ¢ç´¢å’Œé¢„å¤„ç†ç­–ç•¥")
    print("="*80)
    
    # åˆ›å»ºæ¢ç´¢å™¨
    explorer = CERTDataExplorer(data_version='r4.2')
    
    # è¿è¡Œå®Œæ•´æ¢ç´¢
    results = explorer.run_full_exploration(sample_size=50000)  # å¢åŠ æ ·æœ¬å¤§å°ä»¥è·å¾—æ›´å¥½çš„ç»Ÿè®¡
    
    if results:
        print("\nğŸ“‹ æ¢ç´¢ç»“æœæ‘˜è¦:")
        print(f"  - æ•°æ®æºæ•°é‡: {len(results.get('schema_info', {}))}")
        print(f"  - æ•°æ®è´¨é‡å¹³å‡åˆ†: {np.mean([q['missing_percentage'] for q in results.get('quality_report', {}).values()]):.2f}% ç¼ºå¤±ç‡")
        print(f"  - å†…éƒ¨å¨èƒè€…æ•°é‡: {results.get('insiders_count', 0)}")
        print(f"  - æ¢ç´¢è€—æ—¶: {results.get('exploration_time', 0):.1f} ç§’")

if __name__ == "__main__":
    main() 