#!/usr/bin/env python3
"""Simplified Attention Fusion for Multi-modal Features"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union

class AttentionFusion(nn.Module):
    """
    ç®€åŒ–çš„æ³¨æ„åŠ›èåˆæ¨¡å—
    
    å®ç°å¤šæ¨¡æ€ç‰¹å¾çš„èåˆç­–ç•¥ï¼š
    1. ç‰¹å¾æŠ•å½±åˆ°ç»Ÿä¸€ç©ºé—´
    2. æ¨¡æ€é—¨æ§æœºåˆ¶
    3. åŠ æƒèåˆ
    """
    
    def __init__(
        self,
        input_dims: List[int],
        embed_dim: int = 256,
        dropout: float = 0.1,
        use_gating: bool = True
    ):
        super().__init__()
        
        self.input_dims = input_dims
        self.embed_dim = embed_dim
        self.num_modalities = len(input_dims)
        self.use_gating = use_gating
        
        # æ¨¡æ€æŠ•å½±å±‚
        self.modality_projections = nn.ModuleList([
            nn.Sequential(
                nn.Linear(dim, embed_dim),
                nn.LayerNorm(embed_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ) for dim in input_dims
        ])
        
        # æ¨¡æ€é—¨æ§ç½‘ç»œï¼ˆå¯é€‰ï¼‰
        if use_gating:
            self.gate_network = nn.Sequential(
                nn.Linear(embed_dim * self.num_modalities, embed_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(embed_dim, self.num_modalities),
                nn.Softmax(dim=-1)
            )
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        self.attention_weights = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.ReLU(),
            nn.Linear(embed_dim // 2, 1)
        )
        
        # æœ€ç»ˆèåˆå±‚
        self.final_fusion = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim)
        )
        
    def forward(self, modality_features: List[torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            modality_features: å„æ¨¡æ€ç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [batch_size, feature_dim]
            
        Returns:
            èåˆç»“æœå­—å…¸
        """
        batch_size = modality_features[0].shape[0]
        
        # 1. æŠ•å½±åˆ°ç»Ÿä¸€ç©ºé—´
        projected_features = []
        for i, features in enumerate(modality_features):
            projected = self.modality_projections[i](features)
            projected_features.append(projected)
        
        # å †å ç‰¹å¾ [batch_size, num_modalities, embed_dim]
        modality_stack = torch.stack(projected_features, dim=1)
        
        # 2. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_scores = []
        for i in range(self.num_modalities):
            score = self.attention_weights(projected_features[i])  # [batch_size, 1]
            attention_scores.append(score)
        
        attention_scores = torch.cat(attention_scores, dim=1)  # [batch_size, num_modalities]
        attention_weights = F.softmax(attention_scores, dim=1)
        
        # 3. æ¨¡æ€é—¨æ§ï¼ˆå¯é€‰ï¼‰
        if self.use_gating:
            # è¿æ¥æ‰€æœ‰æ¨¡æ€ç‰¹å¾
            concatenated = torch.cat(projected_features, dim=1)  # [batch_size, embed_dim * num_modalities]
            gate_weights = self.gate_network(concatenated)  # [batch_size, num_modalities]
            
            # ç»“åˆæ³¨æ„åŠ›æƒé‡å’Œé—¨æ§æƒé‡
            final_weights = attention_weights * gate_weights
            final_weights = final_weights / (final_weights.sum(dim=1, keepdim=True) + 1e-8)
        else:
            final_weights = attention_weights
            gate_weights = torch.ones_like(attention_weights)
        
        # 4. åŠ æƒèåˆ
        weighted_features = torch.sum(
            modality_stack * final_weights.unsqueeze(-1), 
            dim=1
        )
        
        # 5. æœ€ç»ˆå˜æ¢
        fused_features = self.final_fusion(weighted_features)
        
        return {
            "fused_features": fused_features,           # [batch_size, embed_dim]
            "modality_features": modality_stack,        # [batch_size, num_modalities, embed_dim]
            "attention_weights": attention_weights,     # [batch_size, num_modalities]
            "gate_weights": gate_weights if self.use_gating else None,  # [batch_size, num_modalities]
            "final_weights": final_weights             # [batch_size, num_modalities]
        }

def test_attention_fusion():
    """æµ‹è¯•æ³¨æ„åŠ›èåˆæ¨¡å—"""
    print("ğŸ§ª Testing Attention Fusion...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿå¤šæ¨¡æ€ç‰¹å¾
    batch_size = 16
    modality_features = [
        torch.randn(batch_size, 256),  # Transformerç‰¹å¾
        torch.randn(batch_size, 128),  # GNNç‰¹å¾  
        torch.randn(batch_size, 512),  # BERTç‰¹å¾
        torch.randn(batch_size, 64),   # LGBMç‰¹å¾
    ]
    
    input_dims = [256, 128, 512, 64]
    
    print(f"  è¾“å…¥æ¨¡æ€æ•°é‡: {len(modality_features)}")
    print(f"  è¾“å…¥ç‰¹å¾å½¢çŠ¶: {[f.shape for f in modality_features]}")
    
    # åˆ›å»ºæ³¨æ„åŠ›èåˆæ¨¡å‹
    model = AttentionFusion(
        input_dims=input_dims,
        embed_dim=256,
        use_gating=True
    )
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(modality_features)
    
    print(f"  èåˆç‰¹å¾å½¢çŠ¶: {outputs['fused_features'].shape}")
    print(f"  æ¨¡æ€ç‰¹å¾å½¢çŠ¶: {outputs['modality_features'].shape}")
    print(f"  æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {outputs['attention_weights'].shape}")
    
    # æ˜¾ç¤ºæƒé‡
    attention_weights = outputs['attention_weights'].mean(dim=0)
    final_weights = outputs['final_weights'].mean(dim=0)
    print(f"  å¹³å‡æ³¨æ„åŠ›æƒé‡: {attention_weights.numpy()}")
    print(f"  æœ€ç»ˆèåˆæƒé‡: {final_weights.numpy()}")
    
    # æµ‹è¯•æ¢¯åº¦
    model.train()
    outputs = model(modality_features)
    loss = outputs['fused_features'].sum()
    loss.backward()
    
    print("  âœ… Attention Fusion æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•ä¸åŒé…ç½®
    print("\nğŸ§ª Testing Different Configurations...")
    
    # ç®€åŒ–é…ç½®ï¼ˆæ— é—¨æ§ï¼‰
    simple_model = AttentionFusion(
        input_dims=input_dims,
        embed_dim=128,
        use_gating=False
    )
    
    with torch.no_grad():
        simple_outputs = simple_model(modality_features)
    
    print(f"  ç®€åŒ–æ¨¡å‹èåˆç‰¹å¾å½¢çŠ¶: {simple_outputs['fused_features'].shape}")
    print(f"  é—¨æ§æƒé‡: {simple_outputs['gate_weights']}")
    print("  âœ… ç®€åŒ–é…ç½®æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    test_attention_fusion() 