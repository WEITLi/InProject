#!/usr/bin/env python3
"""Base Fusion Module for Multi-modal Feature Integration"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional

class BaseFusion(nn.Module):
    """
    åŸºç¡€èåˆæ¨¡å—
    
    æ”¯æŒå¤šç§èåˆç­–ç•¥ï¼šæ‹¼æ¥ã€åŠ æƒå¹³å‡ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰
    """
    
    def __init__(
        self,
        input_dims: List[int],
        output_dim: int = 256,
        fusion_type: str = "concat",
        hidden_dim: int = 128,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.input_dims = input_dims
        self.output_dim = output_dim
        self.fusion_type = fusion_type
        self.num_modalities = len(input_dims)
        
        if fusion_type == "concat":
            self._build_concat_fusion()
        elif fusion_type == "weighted":
            self._build_weighted_fusion()
        elif fusion_type == "attention":
            self._build_attention_fusion(hidden_dim)
        else:
            raise ValueError(f"Unsupported fusion type: {fusion_type}")
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(output_dim)
        
    def _build_concat_fusion(self):
        """æ„å»ºæ‹¼æ¥èåˆ"""
        total_input_dim = sum(self.input_dims)
        self.projection = nn.Sequential(
            nn.Linear(total_input_dim, self.output_dim * 2),
            nn.ReLU(),
            nn.Linear(self.output_dim * 2, self.output_dim)
        )
    
    def _build_weighted_fusion(self):
        """æ„å»ºåŠ æƒèåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        self.projections = nn.ModuleList([
            nn.Linear(dim, self.output_dim) for dim in self.input_dims
        ])
        
        # å­¦ä¹ æƒé‡
        self.fusion_weights = nn.Parameter(torch.ones(self.num_modalities))
        
    def _build_attention_fusion(self, hidden_dim: int):
        """æ„å»ºæ³¨æ„åŠ›èåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        self.projections = nn.ModuleList([
            nn.Linear(dim, self.output_dim) for dim in self.input_dims
        ])
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(self.output_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features: å„æ¨¡æ€ç‰¹å¾åˆ—è¡¨ [modality1, modality2, ...]
                     æ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [batch_size, feature_dim]
        
        Returns:
            fused_features: [batch_size, output_dim] èåˆåçš„ç‰¹å¾
        """
        if len(features) != self.num_modalities:
            raise ValueError(f"Expected {self.num_modalities} modalities, got {len(features)}")
        
        if self.fusion_type == "concat":
            return self._concat_fusion(features)
        elif self.fusion_type == "weighted":
            return self._weighted_fusion(features)
        elif self.fusion_type == "attention":
            return self._attention_fusion(features)
    
    def _concat_fusion(self, features: List[torch.Tensor]) -> torch.Tensor:
        """æ‹¼æ¥èåˆ"""
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        concatenated = torch.cat(features, dim=-1)  # [batch_size, sum(input_dims)]
        
        # æŠ•å½±åˆ°è¾“å‡ºç»´åº¦
        output = self.projection(concatenated)
        output = self.dropout(output)
        output = self.layer_norm(output)
        
        return output
    
    def _weighted_fusion(self, features: List[torch.Tensor]) -> torch.Tensor:
        """åŠ æƒèåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        projected_features = []
        for i, feature in enumerate(features):
            projected = self.projections[i](feature)
            projected_features.append(projected)
        
        # å †å ç‰¹å¾
        stacked_features = torch.stack(projected_features, dim=1)  # [batch_size, num_modalities, output_dim]
        
        # åº”ç”¨æƒé‡ï¼ˆä½¿ç”¨softmaxå½’ä¸€åŒ–ï¼‰
        weights = F.softmax(self.fusion_weights, dim=0)  # [num_modalities]
        weights = weights.view(1, -1, 1)  # [1, num_modalities, 1]
        
        # åŠ æƒå¹³å‡
        weighted_features = (stacked_features * weights).sum(dim=1)  # [batch_size, output_dim]
        
        output = self.dropout(weighted_features)
        output = self.layer_norm(output)
        
        return output
    
    def _attention_fusion(self, features: List[torch.Tensor]) -> torch.Tensor:
        """æ³¨æ„åŠ›èåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        projected_features = []
        for i, feature in enumerate(features):
            projected = self.projections[i](feature)
            projected_features.append(projected)
        
        # å †å ç‰¹å¾
        stacked_features = torch.stack(projected_features, dim=1)  # [batch_size, num_modalities, output_dim]
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_scores = self.attention(stacked_features)  # [batch_size, num_modalities, 1]
        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, num_modalities, 1]
        
        # æ³¨æ„åŠ›åŠ æƒ
        attended_features = (stacked_features * attention_weights).sum(dim=1)  # [batch_size, output_dim]
        
        output = self.dropout(attended_features)
        output = self.layer_norm(output)
        
        return output
    
    def get_fusion_weights(self) -> Dict[str, torch.Tensor]:
        """è·å–èåˆæƒé‡ï¼ˆç”¨äºåˆ†æï¼‰"""
        if self.fusion_type == "weighted":
            return {"weights": F.softmax(self.fusion_weights, dim=0)}
        elif self.fusion_type == "attention":
            # æ³¨æ„åŠ›æƒé‡éœ€è¦åœ¨å‰å‘ä¼ æ’­æ—¶è®¡ç®—
            return {"type": "attention", "message": "weights computed dynamically"}
        else:
            return {"type": self.fusion_type, "message": "no explicit weights"}

class ModalityGate(nn.Module):
    """æ¨¡æ€é—¨æ§æœºåˆ¶"""
    
    def __init__(self, input_dim: int, num_modalities: int):
        super().__init__()
        self.gate = nn.Sequential(
            nn.Linear(input_dim, num_modalities),
            nn.Sigmoid()
        )
    
    def forward(self, context: torch.Tensor, features: List[torch.Tensor]) -> List[torch.Tensor]:
        """
        åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹æ¨¡æ€è¿›è¡Œé—¨æ§
        
        Args:
            context: [batch_size, input_dim] ä¸Šä¸‹æ–‡ä¿¡æ¯
            features: å„æ¨¡æ€ç‰¹å¾åˆ—è¡¨
            
        Returns:
            gated_features: é—¨æ§åçš„ç‰¹å¾åˆ—è¡¨
        """
        gates = self.gate(context)  # [batch_size, num_modalities]
        
        gated_features = []
        for i, feature in enumerate(features):
            gate_weight = gates[:, i:i+1]  # [batch_size, 1]
            gated_feature = feature * gate_weight
            gated_features.append(gated_feature)
        
        return gated_features

def test_base_fusion():
    """æµ‹è¯•åŸºç¡€èåˆæ¨¡å—"""
    print("ğŸ§ª Testing Base Fusion...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 32
    features = [
        torch.randn(batch_size, 256),  # Transformerç‰¹å¾
        torch.randn(batch_size, 128),  # GNNç‰¹å¾
        torch.randn(batch_size, 512),  # BERTç‰¹å¾
        torch.randn(batch_size, 64),   # LGBMç‰¹å¾
    ]
    
    input_dims = [256, 128, 512, 64]
    output_dim = 256
    
    # æµ‹è¯•ä¸åŒèåˆç±»å‹
    fusion_types = ["concat", "weighted", "attention"]
    
    for fusion_type in fusion_types:
        print(f"\n  Testing {fusion_type} fusion:")
        
        model = BaseFusion(
            input_dims=input_dims,
            output_dim=output_dim,
            fusion_type=fusion_type
        )
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(features)
        
        print(f"    Input shapes: {[f.shape for f in features]}")
        print(f"    Output shape: {output.shape}")
        
        # æ£€æŸ¥æ¢¯åº¦
        model.train()
        output = model(features)
        loss = output.sum()
        loss.backward()
        
        print(f"    âœ… {fusion_type} fusion test passed")
        
        # æ˜¾ç¤ºæƒé‡ä¿¡æ¯
        weights_info = model.get_fusion_weights()
        if "weights" in weights_info:
            print(f"    Fusion weights: {weights_info['weights'].detach().numpy()}")
    
    # æµ‹è¯•æ¨¡æ€é—¨æ§
    print("\n  Testing Modality Gate:")
    context = torch.randn(batch_size, 100)
    gate = ModalityGate(input_dim=100, num_modalities=len(features))
    
    with torch.no_grad():
        gated_features = gate(context, features)
    
    print(f"    Gated features shapes: {[f.shape for f in gated_features]}")
    print("    âœ… Modality gate test passed")
    
    print("\n  âœ… All Base Fusion tests passed")

if __name__ == "__main__":
    test_base_fusion() 