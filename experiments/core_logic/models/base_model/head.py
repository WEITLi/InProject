#!/usr/bin/env python3
"""Classification Head for Anomaly Detection"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple

class ClassificationHead(nn.Module):
    """
    åˆ†ç±»å¤´æ¨¡å—
    
    ç”¨äºå¼‚å¸¸æ£€æµ‹çš„äºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ä»»åŠ¡
    """
    
    def __init__(
        self,
        input_dim: int = 256,
        num_classes: int = 2,
        hidden_dims: list = [128, 64],
        dropout: float = 0.1,
        activation: str = "relu"
    ):
        super().__init__()
        
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.hidden_dims = hidden_dims
        
        # æ¿€æ´»å‡½æ•°
        if activation == "relu":
            self.activation = nn.ReLU()
        elif activation == "gelu":
            self.activation = nn.GELU()
        elif activation == "tanh":
            self.activation = nn.Tanh()
        else:
            raise ValueError(f"Unsupported activation: {activation}")
        
        # æ„å»ºåˆ†ç±»å™¨
        self.classifier = self._build_classifier(dropout)
        
        # ç”¨äºè®¡ç®—ä¸ç¡®å®šæ€§çš„é¢å¤–å¤´ï¼ˆå¯é€‰ï¼‰
        self.uncertainty_head = None
        
    def _build_classifier(self, dropout: float) -> nn.Module:
        """æ„å»ºåˆ†ç±»å™¨ç½‘ç»œ"""
        layers = []
        
        # è¾“å…¥å±‚
        prev_dim = self.input_dim
        
        # éšè—å±‚
        for hidden_dim in self.hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                self.activation,
                nn.Dropout(dropout),
                nn.LayerNorm(hidden_dim)
            ])
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, self.num_classes))
        
        return nn.Sequential(*layers)
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            
        Returns:
            logits: [batch_size, num_classes] åˆ†ç±»logits
        """
        logits = self.classifier(features)
        return logits
    
    def predict_proba(self, features: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹æ¦‚ç‡
        
        Args:
            features: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            
        Returns:
            probs: [batch_size, num_classes] åˆ†ç±»æ¦‚ç‡
        """
        logits = self.forward(features)
        probs = F.softmax(logits, dim=1)
        return probs
    
    def predict(self, features: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹ç±»åˆ«
        
        Args:
            features: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            
        Returns:
            predictions: [batch_size] é¢„æµ‹ç±»åˆ«
        """
        logits = self.forward(features)
        predictions = torch.argmax(logits, dim=1)
        return predictions

class AnomalyDetectionHead(nn.Module):
    """
    å¼‚å¸¸æ£€æµ‹å¤´
    
    ä¸“é—¨ç”¨äºå¼‚å¸¸æ£€æµ‹ï¼ŒåŒ…å«å¼‚å¸¸åˆ†æ•°å’Œç½®ä¿¡åº¦
    """
    
    def __init__(
        self,
        input_dim: int = 256,
        hidden_dims: list = [128, 64],
        dropout: float = 0.1,
        temperature: float = 1.0
    ):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.temperature = temperature
        
        # å¼‚å¸¸åˆ†æ•°åˆ†æ”¯
        self.anomaly_branch = self._build_branch(input_dim, hidden_dims, 1, dropout)
        
        # ç½®ä¿¡åº¦åˆ†æ”¯
        self.confidence_branch = self._build_branch(input_dim, hidden_dims, 1, dropout)
        
    def _build_branch(self, input_dim: int, hidden_dims: list, output_dim: int, dropout: float) -> nn.Module:
        """æ„å»ºåˆ†æ”¯ç½‘ç»œ"""
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.LayerNorm(hidden_dim)
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        return nn.Sequential(*layers)
    
    def forward(self, features: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            
        Returns:
            outputs: åŒ…å«å¼‚å¸¸åˆ†æ•°å’Œç½®ä¿¡åº¦çš„å­—å…¸
        """
        # å¼‚å¸¸åˆ†æ•° (0-1, è¶Šé«˜è¶Šå¼‚å¸¸)
        anomaly_score = torch.sigmoid(self.anomaly_branch(features) / self.temperature)
        
        # ç½®ä¿¡åº¦ (0-1, è¶Šé«˜è¶Šç¡®ä¿¡)
        confidence = torch.sigmoid(self.confidence_branch(features))
        
        return {
            "anomaly_score": anomaly_score.squeeze(-1),  # [batch_size]
            "confidence": confidence.squeeze(-1),        # [batch_size]
        }
    
    def predict(self, features: torch.Tensor, threshold: float = 0.5) -> Dict[str, torch.Tensor]:
        """
        é¢„æµ‹å¼‚å¸¸
        
        Args:
            features: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            threshold: å¼‚å¸¸åˆ¤å®šé˜ˆå€¼
            
        Returns:
            predictions: åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        outputs = self.forward(features)
        
        # äºŒå€¼åŒ–é¢„æµ‹
        is_anomaly = (outputs["anomaly_score"] > threshold).float()
        
        return {
            "is_anomaly": is_anomaly,
            "anomaly_score": outputs["anomaly_score"],
            "confidence": outputs["confidence"]
        }

class MultiTaskHead(nn.Module):
    """
    å¤šä»»åŠ¡å¤´
    
    åŒæ—¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œé£é™©ç­‰çº§åˆ†ç±»
    """
    
    def __init__(
        self,
        input_dim: int = 256,
        hidden_dims: list = [128, 64],
        num_risk_levels: int = 4,  # ä½ã€ä¸­ã€é«˜ã€æé«˜
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_risk_levels = num_risk_levels
        
        # å…±äº«ç‰¹å¾æå–
        self.shared_layers = self._build_shared_layers(input_dim, hidden_dims, dropout)
        
        # å¼‚å¸¸æ£€æµ‹å¤´
        self.anomaly_head = nn.Linear(hidden_dims[-1], 1)
        
        # é£é™©ç­‰çº§åˆ†ç±»å¤´
        self.risk_head = nn.Linear(hidden_dims[-1], num_risk_levels)
        
        # ä¸¥é‡æ€§å›å½’å¤´
        self.severity_head = nn.Linear(hidden_dims[-1], 1)
        
    def _build_shared_layers(self, input_dim: int, hidden_dims: list, dropout: float) -> nn.Module:
        """æ„å»ºå…±äº«å±‚"""
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.LayerNorm(hidden_dim)
            ])
            prev_dim = hidden_dim
        
        return nn.Sequential(*layers)
    
    def forward(self, features: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features: [batch_size, input_dim] è¾“å…¥ç‰¹å¾
            
        Returns:
            outputs: åŒ…å«å¤šä¸ªä»»åŠ¡è¾“å‡ºçš„å­—å…¸
        """
        # å…±äº«ç‰¹å¾
        shared_features = self.shared_layers(features)
        
        # å¼‚å¸¸æ£€æµ‹
        anomaly_logits = self.anomaly_head(shared_features)
        anomaly_probs = torch.sigmoid(anomaly_logits.squeeze(-1))
        
        # é£é™©ç­‰çº§åˆ†ç±»
        risk_logits = self.risk_head(shared_features)
        risk_probs = F.softmax(risk_logits, dim=1)
        
        # ä¸¥é‡æ€§å›å½’
        severity_scores = torch.sigmoid(self.severity_head(shared_features).squeeze(-1))
        
        return {
            "anomaly_logits": anomaly_logits.squeeze(-1),
            "anomaly_probs": anomaly_probs,
            "risk_logits": risk_logits,
            "risk_probs": risk_probs,
            "severity_scores": severity_scores
        }
    
    def predict(self, features: torch.Tensor, anomaly_threshold: float = 0.5) -> Dict[str, torch.Tensor]:
        """é¢„æµ‹å¤šä¸ªä»»åŠ¡"""
        outputs = self.forward(features)
        
        # å¼‚å¸¸æ£€æµ‹é¢„æµ‹
        is_anomaly = (outputs["anomaly_probs"] > anomaly_threshold).float()
        
        # é£é™©ç­‰çº§é¢„æµ‹
        risk_predictions = torch.argmax(outputs["risk_logits"], dim=1)
        
        return {
            "is_anomaly": is_anomaly,
            "anomaly_score": outputs["anomaly_probs"],
            "risk_level": risk_predictions,
            "risk_probs": outputs["risk_probs"],
            "severity_score": outputs["severity_scores"]
        }

def test_classification_heads():
    """æµ‹è¯•åˆ†ç±»å¤´æ¨¡å—"""
    print("ğŸ§ª Testing Classification Heads...")
    
    batch_size = 32
    input_dim = 256
    features = torch.randn(batch_size, input_dim)
    
    # æµ‹è¯•åŸºç¡€åˆ†ç±»å¤´
    print("\n  Testing ClassificationHead:")
    classifier = ClassificationHead(
        input_dim=input_dim,
        num_classes=2,
        hidden_dims=[128, 64]
    )
    
    with torch.no_grad():
        logits = classifier(features)
        probs = classifier.predict_proba(features)
        predictions = classifier.predict(features)
    
    print(f"    Input shape: {features.shape}")
    print(f"    Logits shape: {logits.shape}")
    print(f"    Probs shape: {probs.shape}")
    print(f"    Predictions shape: {predictions.shape}")
    print("    âœ… ClassificationHead test passed")
    
    # æµ‹è¯•å¼‚å¸¸æ£€æµ‹å¤´
    print("\n  Testing AnomalyDetectionHead:")
    anomaly_detector = AnomalyDetectionHead(
        input_dim=input_dim,
        hidden_dims=[128, 64]
    )
    
    with torch.no_grad():
        outputs = anomaly_detector(features)
        predictions = anomaly_detector.predict(features, threshold=0.5)
    
    print(f"    Anomaly scores shape: {outputs['anomaly_score'].shape}")
    print(f"    Confidence shape: {outputs['confidence'].shape}")
    print(f"    Binary predictions shape: {predictions['is_anomaly'].shape}")
    print("    âœ… AnomalyDetectionHead test passed")
    
    # æµ‹è¯•å¤šä»»åŠ¡å¤´
    print("\n  Testing MultiTaskHead:")
    multitask_head = MultiTaskHead(
        input_dim=input_dim,
        hidden_dims=[128, 64],
        num_risk_levels=4
    )
    
    with torch.no_grad():
        outputs = multitask_head(features)
        predictions = multitask_head.predict(features)
    
    print(f"    Anomaly probs shape: {outputs['anomaly_probs'].shape}")
    print(f"    Risk probs shape: {outputs['risk_probs'].shape}")
    print(f"    Severity scores shape: {outputs['severity_scores'].shape}")
    print(f"    Risk predictions shape: {predictions['risk_level'].shape}")
    print("    âœ… MultiTaskHead test passed")
    
    # æµ‹è¯•æ¢¯åº¦
    print("\n  Testing gradient computation:")
    classifier.train()
    anomaly_detector.train()
    multitask_head.train()
    
    # åˆ†ç±»å¤´æ¢¯åº¦
    logits = classifier(features)
    loss1 = logits.sum()
    loss1.backward()
    
    # å¼‚å¸¸æ£€æµ‹å¤´æ¢¯åº¦
    outputs = anomaly_detector(features)
    loss2 = outputs["anomaly_score"].sum()
    loss2.backward()
    
    # å¤šä»»åŠ¡å¤´æ¢¯åº¦
    outputs = multitask_head(features)
    loss3 = (outputs["anomaly_probs"].sum() + 
             outputs["risk_probs"].sum() + 
             outputs["severity_scores"].sum())
    loss3.backward()
    
    print("    âœ… All gradient computations successful")
    print("\n  âœ… All Classification Head tests passed")

if __name__ == "__main__":
    test_classification_heads() 