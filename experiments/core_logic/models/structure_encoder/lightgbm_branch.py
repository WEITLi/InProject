#!/usr/bin/env python3
"""LightGBM Branch for Structured Features Processing"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Union

class LightGBMBranch(nn.Module):
    """
    ç®€åŒ–çš„LightGBMåˆ†æ”¯
    å¤„ç†ç»“æ„åŒ–ç‰¹å¾ï¼Œå¦‚ç”¨æˆ·å±æ€§ã€è®¾å¤‡ä¿¡æ¯ã€æ—¶é—´ç‰¹å¾ç­‰
    """
    
    def __init__(
        self,
        input_dim: int = 50,
        output_dim: int = 128,
        dropout: float = 0.1,
        # LightGBM specific parameters
        num_leaves: int = 31,
        max_depth: int = -1,
        learning_rate: float = 0.05,
        n_estimators: int = 100, # Added n_estimators for lgb.train
        feature_fraction: float = 0.9,
        bagging_fraction: float = 0.8, # Added bagging_fraction
        bagging_freq: int = 5 # Added bagging_freq
    ):
        super().__init__()
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.dropout_rate = dropout
        
        # Store LightGBM parameters
        self.lgbm_params = {
            'objective': 'multiclass', # Or 'binary' if only 2 classes in a different context
            'metric': 'multi_logloss', # Or 'binary_logloss'
            'num_class': output_dim, # Treating output_dim as num_classes for feature transformation
            'num_leaves': num_leaves,
            'max_depth': max_depth,
            'learning_rate': learning_rate,
            'feature_fraction': feature_fraction,
            'bagging_fraction': bagging_fraction,
            'bagging_freq': bagging_freq,
            'verbose': -1,
            'n_jobs': -1,
            'seed': 42
        }
        self.n_estimators = n_estimators
        
        self.model = None  # LightGBM model will be trained in forward pass or separately
        self.fc = nn.Linear(input_dim, output_dim) # Fallback or initial projection
        self.dropout = nn.Dropout(dropout)
        self.is_fitted = False
        
        # ç‰¹å¾å¤„ç†ç½‘ç»œ
        self.feature_processor = nn.Sequential(
            nn.Linear(input_dim, input_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim * 2, output_dim),
            nn.LayerNorm(output_dim)
        )
        
    def forward(self, features: Union[torch.Tensor, pd.DataFrame]) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features: ç»“æ„åŒ–ç‰¹å¾
            
        Returns:
            structure_embeddings: [batch_size, output_dim] ç»“æ„åŒ–ç‰¹å¾åµŒå…¥
        """
        # è½¬æ¢ä¸ºå¼ é‡
        if isinstance(features, pd.DataFrame):
            # ç®€å•çš„æ•°å€¼åŒ–å¤„ç†
            X = features.copy()
            for col in X.columns:
                if X[col].dtype == 'object':
                    # ç®€å•çš„æ ‡ç­¾ç¼–ç 
                    unique_vals = X[col].unique()
                    mapping = {val: i for i, val in enumerate(unique_vals)}
                    X[col] = X[col].map(mapping)
            
            X = X.fillna(0).values.astype(np.float32)
            features_tensor = torch.tensor(X, dtype=torch.float32)
        else:
            features_tensor = features.float()
        
        # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…input_dim
        batch_size = features_tensor.shape[0]
        current_dim = features_tensor.shape[1]
        
        if current_dim != self.input_dim:
            if current_dim > self.input_dim:
                # æˆªæ–­
                features_tensor = features_tensor[:, :self.input_dim]
            else:
                # å¡«å……
                padding = torch.zeros(batch_size, self.input_dim - current_dim)
                features_tensor = torch.cat([features_tensor, padding], dim=1)
        
        # ç‰¹å¾å¤„ç†
        output = self.feature_processor(features_tensor)
        
        return output

class StructuredFeatureExtractor:
    """ç»“æ„åŒ–ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.user_features = [
            'dept', 'role', 'seniority', 'ITAdmin', 'login_frequency',
            'avg_session_length', 'off_hours_access', 'weekend_access'
        ]
        
        self.temporal_features = [
            'hour', 'day_of_week', 'is_weekend', 'is_holiday',
            'time_since_last_login', 'session_duration'
        ]
        
        self.behavioral_features = [
            'email_count', 'file_access_count', 'network_access_count',
            'failed_login_attempts', 'suspicious_activity_score'
        ]
    
    def extract_features(self, data: Dict) -> pd.DataFrame:
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        features = {}
        
        # ç”¨æˆ·ç‰¹å¾
        for feature in self.user_features:
            features[feature] = data.get(feature, 0)
        
        # æ—¶é—´ç‰¹å¾
        for feature in self.temporal_features:
            features[feature] = data.get(feature, 0)
        
        # è¡Œä¸ºç‰¹å¾
        for feature in self.behavioral_features:
            features[feature] = data.get(feature, 0)
        
        return pd.DataFrame([features])

def test_lightgbm_branch():
    """æµ‹è¯•LightGBMåˆ†æ”¯"""
    print("ğŸ§ª Testing LightGBM Branch...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 100
    n_features = 15
    
    # åˆ›å»ºæ•°å€¼ç‰¹å¾
    numerical_data = np.random.randn(n_samples, n_features - 3)
    
    # åˆ›å»ºåˆ†ç±»ç‰¹å¾
    categorical_data = {
        'dept': np.random.choice(['IT', 'Finance', 'HR', 'Marketing'], n_samples),
        'role': np.random.choice(['Employee', 'Manager', 'Director'], n_samples),
        'is_admin': np.random.choice([0, 1], n_samples)
    }
    
    # åˆå¹¶ä¸ºDataFrame
    df = pd.DataFrame(numerical_data, columns=[f'feature_{i}' for i in range(numerical_data.shape[1])])
    for col, values in categorical_data.items():
        df[col] = values
    
    print(f"  è¾“å…¥æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"  ç‰¹å¾åˆ—: {list(df.columns)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = LightGBMBranch(
        input_dim=20,  # è®¾ç½®è¾ƒå¤§çš„è¾“å…¥ç»´åº¦
        output_dim=64
    )
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        embeddings = model(df)
    
    print(f"  è¾“å‡ºåµŒå…¥å½¢çŠ¶: {embeddings.shape}")
    print(f"  åµŒå…¥å‡å€¼: {embeddings.mean().item():.4f}")
    print(f"  åµŒå…¥æ ‡å‡†å·®: {embeddings.std().item():.4f}")
    
    # æµ‹è¯•æ¢¯åº¦
    model.train()
    embeddings = model(df)
    loss = embeddings.sum()
    loss.backward()
    
    print("  âœ… LightGBM Branch æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•ç‰¹å¾æå–å™¨
    print("\nğŸ§ª Testing Structured Feature Extractor...")
    
    extractor = StructuredFeatureExtractor()
    sample_data = {
        'dept': 'IT',
        'role': 'Manager',
        'seniority': 5,
        'ITAdmin': 1,
        'login_frequency': 10.5,
        'hour': 14,
        'day_of_week': 2,
        'email_count': 25
    }
    
    extracted_features = extractor.extract_features(sample_data)
    print(f"  æå–ç‰¹å¾å½¢çŠ¶: {extracted_features.shape}")
    print(f"  ç‰¹å¾åˆ—: {list(extracted_features.columns)}")
    
    # æµ‹è¯•æå–çš„ç‰¹å¾
    with torch.no_grad():
        embeddings = model(extracted_features)
    
    print(f"  å•æ ·æœ¬åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
    print("  âœ… Structured Feature Extractor æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    test_lightgbm_branch() 