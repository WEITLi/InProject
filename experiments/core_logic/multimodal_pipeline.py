#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ•°æ®å¤„ç†æµæ°´çº¿
æ•´åˆç°æœ‰çš„CERTæ•°æ®é›†å¤„ç†èƒ½åŠ›å’Œå¤šæ¨¡æ€æ¨¡å‹æ¶æ„
"""

import os
import sys
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from datetime import datetime, timedelta
import multiprocessing as mp
from typing import Dict, List, Tuple, Optional, Any, Union
import pickle
import warnings
from joblib import Parallel, delayed
import json
from pathlib import Path
from sklearn.preprocessing import StandardScaler
import dask.dataframe as dd
import logging # ç¡®ä¿å¯¼å…¥

warnings.filterwarnings('ignore')

# å¯¼å…¥ç°æœ‰æ¨¡å—
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from .dataset_pipeline import CERTDatasetPipeline
    from .encoder import EventEncoder
    from .config import Config, ModelConfig, TrainingConfig, DataConfig
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œæ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„å¹¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    
    from dataset_pipeline import CERTDatasetPipeline
    from encoder import EventEncoder
    from config import Config, ModelConfig, TrainingConfig, DataConfig

# æ³¨æ„ï¼šMultiModalAnomalyDetectoråœ¨train_pipelineä¸­å®šä¹‰ï¼Œè¿™é‡Œä¸éœ€è¦å¯¼å…¥

class MultiModalDataPipeline:
    """
    å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ•°æ®å¤„ç†æµæ°´çº¿
    
    æ•´åˆåŠŸèƒ½ï¼š
    1. ç°æœ‰çš„CERTæ•°æ®é›†ç‰¹å¾æå–
    2. å¤šæ¨¡æ€æ•°æ®å‡†å¤‡å’Œç¼–ç 
    3. ç”¨æˆ·å…³ç³»å›¾æ„å»º
    4. æ–‡æœ¬å†…å®¹æå–å’Œé¢„å¤„ç†
    5. ç»“æ„åŒ–ç‰¹å¾å·¥ç¨‹
    6. è®­ç»ƒæ•°æ®ç”Ÿæˆ
    """
    
    def __init__(self, 
                 config: Config = None,
                 data_version: str = 'r4.2',
                 feature_dim: int = 256,
                 num_cores: int = 8,
                 source_dir_override: Optional[str] = None,
                 work_dir_override: Optional[str] = None):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®æµæ°´çº¿
        
        Args:
            config: å®Œæ•´é…ç½®å¯¹è±¡
            data_version: æ•°æ®é›†ç‰ˆæœ¬ (ä¼šè¢« config.data.data_version è¦†ç›–)
            feature_dim: ç‰¹å¾å‘é‡ç»´åº¦ (ä¼šè¢« config.data.feature_dim è¦†ç›–)
            num_cores: CPUæ ¸å¿ƒæ•° (ä¼šè¢« config.data.num_cores è¦†ç›–)
            source_dir_override: æºæ•°æ®ç›®å½•è¦†ç›– (ä¼šè¢« config.data.source_dir è¦†ç›–)
            work_dir_override: å·¥ä½œç›®å½•è¦†ç›– (ä¼šè¢« CERTDatasetPipeline çš„ work_dir_override ä½¿ç”¨)
        """
        self.config = config or Config()
        
        # Values from MultiModalDataPipeline's own constructor arguments are defaults
        mmp_constructor_data_version = data_version
        mmp_constructor_feature_dim = feature_dim
        mmp_constructor_num_cores = num_cores
        mmp_constructor_source_dir_override = source_dir_override
        mmp_constructor_work_dir_override = work_dir_override

        # Determine effective values for CERTDatasetPipeline initialization
        # by prioritizing values from the config object if they are set.
        
        effective_data_version_for_base = mmp_constructor_data_version
        if self.config and hasattr(self.config.data, 'data_version') and self.config.data.data_version is not None:
            effective_data_version_for_base = self.config.data.data_version
        
        effective_feature_dim_for_base = mmp_constructor_feature_dim
        if self.config and hasattr(self.config.data, 'feature_dim') and self.config.data.feature_dim is not None:
            effective_feature_dim_for_base = self.config.data.feature_dim

        effective_num_cores_for_base = mmp_constructor_num_cores
        if self.config and hasattr(self.config.data, 'num_cores') and self.config.data.num_cores is not None:
            effective_num_cores_for_base = self.config.data.num_cores

        effective_source_dir_override_for_base = mmp_constructor_source_dir_override
        if self.config and hasattr(self.config.data, 'source_dir') and self.config.data.source_dir is not None:
            effective_source_dir_override_for_base = self.config.data.source_dir
        
        # For CERTDatasetPipeline's work_dir_override, use the one passed to MMP's constructor.
        effective_work_dir_override_for_cert = mmp_constructor_work_dir_override

        # Get seed for CERTDatasetPipeline from the main Config object
        effective_seed_for_base = getattr(self.config, 'seed', 42) # Default to 42 if not in config
        
        # åˆå§‹åŒ–åŸºç¡€æ•°æ®æµæ°´çº¿ (CERTDatasetPipeline)
        self.base_pipeline = CERTDatasetPipeline(
            data_version=effective_data_version_for_base,
            feature_dim=effective_feature_dim_for_base,
            num_cores=effective_num_cores_for_base,
            source_dir_override=effective_source_dir_override_for_base,
            work_dir_override=effective_work_dir_override_for_cert,
            seed=effective_seed_for_base # ä¼ é€’ç§å­
        )
        
        # Set MultiModalDataPipeline's own attributes to reflect effective values.
        self.data_version = effective_data_version_for_base
        self.feature_dim = effective_feature_dim_for_base
        self.num_cores = effective_num_cores_for_base # Corrected assignment
        
        # MMP's own work_dir and source_data_dir
        self.work_dir = self.base_pipeline.work_dir 
        self.source_data_dir = self.base_pipeline.source_data_dir
        
        # å¤šæ¨¡æ€æ•°æ®ç›®å½• (remains based on self.work_dir which is now CERT's work_dir)
        self.multimodal_dir = os.path.join(self.work_dir, "MultiModalData")
        self._create_multimodal_directories()
        
        # æ•°æ®ç¼“å­˜
        self._data_cache = {}
        self._user_graph_cache = None
        self._text_data_cache = None
        
        print(f"åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®æµæ°´çº¿")
        print(f"  æ•°æ®ç‰ˆæœ¬ (effective for base): {self.data_version}") 
        print(f"  ç‰¹å¾ç»´åº¦ (effective for base): {self.feature_dim}") 
        print(f"  å¤šæ¨¡æ€æ•°æ®ç›®å½•: {os.path.abspath(self.multimodal_dir)}")
    
    def _create_multimodal_directories(self):
        """åˆ›å»ºå¤šæ¨¡æ€æ•°æ®ç›®å½•ç»“æ„"""
        directories = [
            "MultiModalData",
            "MultiModalData/UserGraphs",
            "MultiModalData/TextData", 
            "MultiModalData/StructuredFeatures",
            "MultiModalData/BehaviorSequences",
            "MultiModalData/TrainingData",
            "MultiModalData/Models"
        ]
        for directory in directories:
            work_path = os.path.join(self.work_dir, directory)
            os.makedirs(work_path, exist_ok=True)
    
    def run_base_feature_extraction(self, 
                                  start_week: int = 0, 
                                  end_week: int = None,
                                  max_users: int = None,
                                  sample_ratio: float = None):
        """
        è¿è¡ŒåŸºç¡€ç‰¹å¾æå–æµæ°´çº¿
        
        Args:
            start_week: å¼€å§‹å‘¨æ•°
            end_week: ç»“æŸå‘¨æ•°  
            max_users: æœ€å¤§ç”¨æˆ·æ•°
            sample_ratio: æ•°æ®é‡‡æ ·æ¯”ä¾‹ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
        """
        print(f"\n{'='*60}")
        print(f"Step 1: è¿è¡ŒåŸºç¡€ç‰¹å¾æå–æµæ°´çº¿")
        print(f"{'='*60}")

        force_regen_combined_weeks = getattr(self.config.data, 'force_regenerate_combined_weeks', False)
        force_regen_analysis_levels = getattr(self.config.data, 'force_regenerate_analysis_levels', False)
        print(f"   åŸºç¡€æµæ°´çº¿æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆå¹¶å‘¨æ•°æ®: {force_regen_combined_weeks}")
        print(f"   åŸºç¡€æµæ°´çº¿æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆåˆ†æçº§åˆ«CSV: {force_regen_analysis_levels}")
        
        # è¿è¡Œå®Œæ•´çš„åŸºç¡€æµæ°´çº¿
        self.base_pipeline.run_full_pipeline(
            start_week=start_week,
            end_week=end_week,
            max_users=max_users,
            modes=['week', 'day', 'session'],
            sample_ratio=sample_ratio,
            force_regenerate_combined_weeks=force_regen_combined_weeks,
            force_regenerate_analysis_levels=force_regen_analysis_levels
        )
        
        print("âœ… åŸºç¡€ç‰¹å¾æå–å®Œæˆ")
    
    def _apply_max_users_filter(self, users_df: pd.DataFrame, max_users: Optional[int]) -> pd.DataFrame:
        """
        è¾…åŠ©å‡½æ•°ï¼šå¯¹ç”¨æˆ·DataFrameåº”ç”¨max_usersé™åˆ¶ã€‚
        ä¼˜å…ˆä¿ç•™æ¶æ„ç”¨æˆ·ã€‚
        """
        if max_users and len(users_df) > max_users:
            print(f"   ç»Ÿä¸€åº”ç”¨ç”¨æˆ·æ•°é‡é™åˆ¶: {len(users_df)} -> {max_users}")
            if 'malscene' in users_df.columns:
                malicious_users_df = users_df[users_df['malscene'] > 0]
                normal_users_df = users_df[users_df['malscene'] == 0]

                if len(malicious_users_df) >= max_users:
                    final_users_df = malicious_users_df.sample(n=max_users, random_state=self.config.seed) # ä½¿ç”¨ self.config.seed
                else:
                    remaining_slots = max_users - len(malicious_users_df)
                    if remaining_slots > 0 and not normal_users_df.empty:
                        selected_normal_df = normal_users_df.sample(
                            n=min(remaining_slots, len(normal_users_df)), random_state=self.config.seed # ä½¿ç”¨ self.config.seed
                        )
                        final_users_df = pd.concat([malicious_users_df, selected_normal_df])
                    else:
                        final_users_df = malicious_users_df
                
                if final_users_df.empty and not users_df.empty:
                     print("    âš ï¸ ä¼˜å…ˆé€‰æ‹©æ¶æ„ç”¨æˆ·åä¸ºç©ºï¼Œä½†åŸå§‹ç”¨æˆ·åˆ—è¡¨ä¸ä¸ºç©ºã€‚å›é€€åˆ°éšæœºé‡‡æ ·ã€‚")
                     final_users_df = users_df.sample(n=min(max_users, len(users_df)), random_state=self.config.seed) # ä½¿ç”¨ self.config.seed

            else: # æ²¡æœ‰æ¶æ„åœºæ™¯ä¿¡æ¯ï¼Œéšæœºé‡‡æ ·
                print("    âš ï¸ 'malscene' åˆ—ä¸å­˜åœ¨äºusers_dfï¼Œæ‰§è¡Œéšæœºç”¨æˆ·é‡‡æ ·ã€‚")
                final_users_df = users_df.sample(n=min(max_users, len(users_df)), random_state=self.config.seed) # ä½¿ç”¨ self.config.seed
            
            print(f"   æœ€ç»ˆç­›é€‰ç”¨æˆ·æ•°: {len(final_users_df)}")
            return final_users_df
        return users_df

    def prepare_training_data(self, 
                            start_week: int = 0,
                            end_week: int = None,
                            max_users: int = None, # è¿™ä¸ªmax_userså°†ç”¨äºç»Ÿä¸€ç­›é€‰
                            sequence_length: int = 128) -> Dict[str, Any]:
        """
        å‡†å¤‡å¤šæ¨¡æ€è®­ç»ƒæ•°æ®
        
        Args:
            start_week: å¼€å§‹å‘¨æ•°
            end_week: ç»“æŸå‘¨æ•°
            max_users: æœ€å¤§ç”¨æˆ·æ•°é™åˆ¶ (å°†åœ¨è¿™é‡Œç»Ÿä¸€åº”ç”¨)
            sequence_length: åºåˆ—é•¿åº¦
            
        Returns:
            è®­ç»ƒæ•°æ®å­—å…¸
        """
        logger = logging.getLogger(__name__) # è·å– logger å®ä¾‹
        logger.info(f"[MultiModalDataPipeline.prepare_training_data] æ¥æ”¶åˆ°çš„å‚æ•°: start_week={start_week}, end_week={end_week}, max_users={max_users}, sequence_length={sequence_length}")

        if end_week is None:
            end_week = self.base_pipeline.max_weeks
            
        print(f"\n{'='*60}")
        print(f"Step 3: å‡†å¤‡å¤šæ¨¡æ€è®­ç»ƒæ•°æ® (ç»Ÿä¸€ç”¨æˆ·ç­›é€‰)")
        print(f"{'='*60}")
        
        # --- å…³é”®æ”¹åŠ¨ï¼šåœ¨è¿™é‡Œç»Ÿä¸€åŠ è½½å’Œç­›é€‰ç”¨æˆ· ---
        print("   åŠ è½½åˆå§‹ç”¨æˆ·æ•°æ®...")
        initial_users_df = self.base_pipeline.step2_load_user_data() # åŠ è½½æ‰€æœ‰å¯èƒ½çš„ç”¨æˆ·
        print(f"   åˆå§‹ç”¨æˆ·æ•°: {len(initial_users_df)}")
        
        logger.info(f"[MultiModalDataPipeline.prepare_training_data] è°ƒç”¨ _apply_max_users_filter ä¹‹å‰, ä¼ å…¥çš„ max_users={max_users}")
        final_selected_users_df = self._apply_max_users_filter(initial_users_df, max_users)
        logger.info(f"[MultiModalDataPipeline.prepare_training_data] è°ƒç”¨ _apply_max_users_filter ä¹‹å, final_selected_users_df é•¿åº¦: {len(final_selected_users_df)}")
        
        final_user_list = final_selected_users_df.index.tolist() # å‡è®¾ç´¢å¼•æ˜¯user_id
        logger.info(f"[MultiModalDataPipeline.prepare_training_data] final_user_list é•¿åº¦: {len(final_user_list)}")
        
        if not final_user_list:
            print("âš ï¸ ç»è¿‡ç­›é€‰åæ²¡æœ‰ç”¨æˆ·å¯ç”¨äºè®­ç»ƒï¼Œå°†è¿”å›ç©ºæ•°æ®ã€‚")
            return { # è¿”å›ä¸€ä¸ªç¬¦åˆåç»­æœŸæœ›ç»“æ„ä½†ä¸ºç©ºçš„å­—å…¸
                'behavior_sequences': np.array([]),
                'node_features': np.array([]),
                'adjacency_matrix': np.array([]),
                'text_content': [],
                'structured_features': np.array([]),
                'labels': np.array([]),
                'users': [],
                'user_to_index': {}
            }

        training_data_file = os.path.join(self.multimodal_dir, "TrainingData",
                                        f"training_data_w{start_week}_{end_week}_u{len(final_user_list)}.pickle")
        logger.info(f"[MultiModalDataPipeline.prepare_training_data] æ„é€ çš„ training_data_file è·¯å¾„: {training_data_file}")
        
        if os.path.exists(training_data_file) and not self.config.data.force_regenerate_training_data:
            print(f"   è®­ç»ƒæ•°æ®å·²å­˜åœ¨ä¸”æœªå¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œç›´æ¥åŠ è½½: {training_data_file}")
            try:
                with open(training_data_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"   åŠ è½½ç¼“å­˜è®­ç»ƒæ•°æ®å¤±è´¥: {e}ã€‚å°†é‡æ–°ç”Ÿæˆã€‚")

        # åŠ è½½å„æ¨¡æ€æ•°æ®ï¼Œç¡®ä¿å®ƒä»¬åŸºäºæˆ–å…¼å®¹ final_selected_users_df
        # _load_user_graph_data ç­‰æ–¹æ³•ç°åœ¨éœ€è¦çŸ¥é“æœ€ç»ˆç”¨æˆ·åˆ—è¡¨æˆ–DataFrame
        user_graph_data = self._load_user_graph_data(start_week, end_week, final_selected_users_df)
        text_data = self._load_text_data(start_week, end_week) # æ–‡æœ¬æ•°æ®é€šå¸¸è¾ƒéš¾æŒ‰ç”¨æˆ·ç²¾ç¡®é¢„ç­›é€‰
        behavior_sequences = self._load_behavior_sequences(start_week, end_week, final_user_list) # ä¼ é€’ç”¨æˆ·åˆ—è¡¨
        structured_features = self._load_structured_features(start_week, end_week, final_user_list) # ä¼ é€’ç”¨æˆ·åˆ—è¡¨
        
        # labels_data ç°åœ¨åº”è¯¥åŸºäº final_selected_users_df ç”Ÿæˆ
        labels_data = self._load_labels_data_from_df(final_selected_users_df) 
        
        # æ•´åˆè®­ç»ƒæ•°æ®
        training_data = self._integrate_multimodal_data(
            user_graph_data, text_data, behavior_sequences, 
            structured_features, labels_data, sequence_length,
            final_user_list # ä¼ é€’æƒå¨çš„ç”¨æˆ·åˆ—è¡¨ç»™æ•´åˆå‡½æ•°
        )

        # --- START: Add Normalization ---
        # Normalize behavior_sequences
        # Reshape to 2D for scaler: (num_samples * sequence_length, feature_dim)
        # Then reshape back to 3D
        if 'behavior_sequences' in training_data and training_data['behavior_sequences'].size > 0: # Check if key exists and not empty
            bs_shape = training_data['behavior_sequences'].shape
            # Only scale if there's more than one sample to avoid issues with single sample std dev
            if bs_shape[0] > 1: 
                behavior_sequences_2d = training_data['behavior_sequences'].reshape(-1, bs_shape[-1])
                scaler_bs = StandardScaler()
                normalized_bs_2d = scaler_bs.fit_transform(behavior_sequences_2d)
                training_data['behavior_sequences'] = normalized_bs_2d.reshape(bs_shape)
                print("   è¡Œä¸ºåºåˆ—æ•°æ®å·²è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚")
            else:
                print("   è¡Œä¸ºåºåˆ—æ•°æ®æ ·æœ¬è¿‡å°‘(<2)ï¼Œè·³è¿‡æ ‡å‡†åŒ–ã€‚")
        else:
            print("   æœªæ‰¾åˆ°æˆ–è¡Œä¸ºåºåˆ—æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ ‡å‡†åŒ–ã€‚")

        # Normalize structured_features
        # This is already 2D: (num_samples, feature_dim)
        if 'structured_features' in training_data and training_data['structured_features'].size > 0:
            # Only scale if there's more than one sample
            if training_data['structured_features'].shape[0] > 1:
                scaler_sf = StandardScaler()
                training_data['structured_features'] = scaler_sf.fit_transform(training_data['structured_features'])
                print("   ç»“æ„åŒ–ç‰¹å¾æ•°æ®å·²è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚")
            else:
                print("   ç»“æ„åŒ–ç‰¹å¾æ•°æ®æ ·æœ¬è¿‡å°‘(<2)ï¼Œè·³è¿‡æ ‡å‡†åŒ–ã€‚")
        else:
            print("   æœªæ‰¾åˆ°æˆ–ç»“æ„åŒ–ç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ ‡å‡†åŒ–ã€‚")
        # --- END: Add Normalization ---
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        with open(training_data_file, 'wb') as f:
            pickle.dump(training_data, f)
        
        print(f"   è®­ç»ƒæ•°æ®ä¿å­˜åˆ°: {training_data_file}")
        print(f"   æ•°æ®æ ·æœ¬æ•°: {len(training_data.get('labels', []))}")
        
        return training_data
    
    def _load_user_graph_data(self, start_week: int, end_week: int, users_df_for_graph: pd.DataFrame) -> Dict[str, Any]:
        """
        åŠ è½½æˆ–æ„å»ºç”¨æˆ·å›¾æ•°æ®ï¼ŒåŸºäºæŒ‡å®šçš„users_df_for_graphã€‚
        Args:
            users_df_for_graph: ç»è¿‡ç­›é€‰çš„ã€ç”¨äºæ„å»ºå›¾çš„ç”¨æˆ·DataFrameã€‚
        """
        # æ–‡ä»¶åå¯ä»¥åŒ…å«ç”¨æˆ·æ•°é‡ä¿¡æ¯ä»¥åŒºåˆ†ä¸åŒç­›é€‰ä¸‹çš„å›¾
        graph_file_suffix = f"_u{len(users_df_for_graph)}.pickle" if users_df_for_graph is not None else ".pickle"
        graph_file = os.path.join(self.multimodal_dir, "UserGraphs",
                                f"user_graph_w{start_week}_{end_week}{graph_file_suffix}")

        if os.path.exists(graph_file) and not self.config.data.force_regenerate_graphs:
            print(f"   ç”¨æˆ·å…³ç³»å›¾å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½: {graph_file}")
            with open(graph_file, 'rb') as f:
                return pickle.load(f)
        
        print(f"   æ„å»ºç”¨æˆ·å…³ç³»å›¾ (åŸºäº {len(users_df_for_graph)} ä¸ªå·²ç­›é€‰ç”¨æˆ·)...")
        # _construct_user_relationships éœ€è¦ç¡®ä¿ä½¿ç”¨è¿™ä¸ªä¼ å…¥çš„ users_df_for_graph
        # å¹¶ä¸”ä¸å†è¿›è¡Œå†…éƒ¨çš„ max_users ç­›é€‰
        user_graph_data = self._construct_user_relationships(
            users_df_for_graph, start_week, end_week 
        )
        
        with open(graph_file, 'wb') as f:
            pickle.dump(user_graph_data, f)
        print(f"   ç”¨æˆ·å…³ç³»å›¾ä¿å­˜åˆ°: {graph_file}")
        return user_graph_data
    
    def _load_text_data(self, start_week: int, end_week: int) -> Dict[str, Any]:
        """åŠ è½½æ–‡æœ¬æ•°æ® (æ­¤æ–¹æ³•æš‚ä¸æŒ‰ç”¨æˆ·ç­›é€‰ï¼Œåç»­å¯åœ¨æ•´åˆæ—¶å¤„ç†)"""
        # ... (ä¿ç•™åŸæ ·ï¼Œæ–‡æœ¬çš„ç²¾ç»†ç­›é€‰é€šå¸¸åœ¨æ•´åˆæˆ–æ¨¡å‹å±‚é¢å¤„ç†)
        text_file = os.path.join(self.multimodal_dir, "TextData",
                               f"text_data_w{start_week}_{end_week}.pickle")
        if os.path.exists(text_file) and not self.config.data.force_regenerate_text_data:
            print(f"   æ–‡æœ¬æ•°æ®å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½: {text_file}")
            with open(text_file, 'rb') as f:
                return pickle.load(f)

        # å¦‚æœä¸å­˜åœ¨æˆ–éœ€è¦é‡æ–°ç”Ÿæˆ (è°ƒç”¨åŸæœ‰çš„_extract_text_data)
        print(f"   æå–åŸå§‹æ–‡æœ¬æ•°æ®...") # ä¿®æ”¹æ—¥å¿—æ¶ˆæ¯
        self._extract_text_data(start_week, end_week) # è¿™ä¼šä¿å­˜æ–‡ä»¶
        
        # å†æ¬¡å°è¯•åŠ è½½
        if os.path.exists(text_file):
            with open(text_file, 'rb') as f:
                return pickle.load(f)
        else:
            print(f"   âš ï¸ æœªèƒ½åŠ è½½æˆ–ç”Ÿæˆæ–‡æœ¬æ•°æ®æ–‡ä»¶: {text_file}")
            return {} # è¿”å›ç©ºå­—å…¸

    def _extract_text_data(self, start_week: int, end_week: int):
        """ä»åŸºç¡€æµæ°´çº¿å¤„ç†çš„ Parquet æ•°æ®ä¸­æå–æ–‡æœ¬å†…å®¹å¹¶ä¿å­˜ã€‚"""
        print(f"   æ·±å…¥æå–æ–‡æœ¬æ•°æ® (å‘¨ {start_week}-{end_week-1})...")
        all_user_texts = {}

        # ä½¿ç”¨ self.base_pipeline.current_parquet_dir_name æ¥è·å–æ­£ç¡®çš„ Parquet ç›®å½•
        base_parquet_dir = self.base_pipeline._get_work_file_path(self.base_pipeline.current_parquet_dir_name)

        if not os.path.exists(base_parquet_dir):
            print(f"    âš ï¸ åŸºç¡€ Parquet ç›®å½• {base_parquet_dir} ä¸å­˜åœ¨ï¼Œæ— æ³•æå–æ–‡æœ¬ã€‚")
            return

        for week in range(start_week, end_week):
            print(f"    å¤„ç†å‘¨ {week} çš„æ–‡æœ¬...")
            try:
                week_data_ddf = dd.read_parquet(
                    base_parquet_dir,
                    filters=[('week', '==', week)],
                    columns=['user', 'type', 'activity', 'url', 'filename', 'content'], # æ ¹æ®å®é™…å¯èƒ½åŒ…å«æ–‡æœ¬çš„åˆ—
                    engine='pyarrow'
                )

                if week_data_ddf.npartitions == 0 or week_data_ddf.map_partitions(len).compute().sum() == 0:
                    print(f"     å‘¨ {week}: æ— æ•°æ®ï¼Œè·³è¿‡æ–‡æœ¬æå–ã€‚")
                    continue
                
                week_data_df = week_data_ddf.compute()

                for _, row in week_data_df.iterrows():
                    user = row.get('user')
                    event_type = row.get('type')
                    texts_to_add = []

                    if pd.isna(user):
                        continue

                    if user not in all_user_texts:
                        all_user_texts[user] = {
                            'email_texts': [],
                            'file_texts': [],
                            'http_texts': [],
                            'other_texts': []
                        }
                    
                    content = str(row.get('content', '')) if pd.notna(row.get('content')) else ''
                    activity = str(row.get('activity', '')) if pd.notna(row.get('activity')) else ''
                    url = str(row.get('url', '')) if pd.notna(row.get('url')) else ''
                    filename = str(row.get('filename', '')) if pd.notna(row.get('filename')) else ''

                    if event_type == 'email':
                        if content: texts_to_add.append(content)
                        if activity: texts_to_add.append(activity) # ä¾‹å¦‚é‚®ä»¶ä¸»é¢˜
                        all_user_texts[user]['email_texts'].extend(texts_to_add)
                    elif event_type == 'file':
                        if filename: texts_to_add.append(filename)
                        if content: texts_to_add.append(content) # ä¾‹å¦‚æ–‡ä»¶å†…å®¹ç‰‡æ®µ
                        all_user_texts[user]['file_texts'].extend(texts_to_add)
                    elif event_type == 'http':
                        if url: texts_to_add.append(url)
                        if content: texts_to_add.append(content) # ä¾‹å¦‚ç½‘é¡µæ ‡é¢˜æˆ–ç‰‡æ®µ
                        all_user_texts[user]['http_texts'].extend(texts_to_add)
                    else: # å…¶ä»–äº‹ä»¶ç±»å‹ï¼Œå¦‚logon, device
                        if activity: texts_to_add.append(activity)
                        if content: texts_to_add.append(content)
                        all_user_texts[user]['other_texts'].extend(texts_to_add)
            
            except FileNotFoundError:
                print(f"     å‘¨ {week}: Parquetæ•°æ®åˆ†åŒºæœªæ‰¾åˆ°ï¼Œè·³è¿‡æ–‡æœ¬æå–ã€‚")
            except Exception as e:
                print(f"     å‘¨ {week}: æå–æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        # ç»„ç»‡æˆæœŸæœ›çš„æ ¼å¼ï¼š{week_num: {user_id: {text_type: [texts]}}}
        # å½“å‰ all_user_texts æ˜¯ {user_id: {text_type: [texts]}}
        # ä¸ºäº†ä¸ _load_text_data çš„åŸå§‹æœŸæœ›ï¼ˆæ¯ä¸ª week ä¸€ä¸ª keyï¼‰å¯¹é½ï¼Œè¿™é‡Œå¯ä»¥ç®€å•åœ°å°†æ‰€æœ‰å‘¨çš„æ–‡æœ¬èšåˆ
        # æˆ–è€…ï¼Œå¦‚æœä¸¥æ ¼è¦æ±‚æŒ‰å‘¨åˆ†å¼€ï¼Œåˆ™éœ€è¦åœ¨å¾ªç¯å†…éƒ¨æ„å»ºæ›´å¤æ‚çš„ç»“æ„å¹¶åˆå¹¶ã€‚
        # ç®€åŒ–å¤„ç†ï¼šæ‰€æœ‰å‘¨çš„æ–‡æœ¬èšåˆåˆ°ä¸€ä¸ªè™šæ‹Ÿçš„ "all_weeks" é”®ä¸‹ï¼Œæˆ–è€…ç›´æ¥è¿”å› all_user_texts
        # _load_text_data çš„è¯»å–é€»è¾‘ä¹Ÿéœ€è¦ç›¸åº”è°ƒæ•´ã€‚ 
        # ç°åœ¨çš„ _integrate_multimodal_data ä¼šéå† text_data.values(), æ‰€ä»¥ç›´æ¥è¿”å› all_user_texts ç»“æ„æ˜¯OKçš„ã€‚

        output_text_data = {"all_collected_weeks": all_user_texts} # ä¿æŒ text_data[week_key] çš„ç»“æ„

        text_file_path = os.path.join(self.multimodal_dir, "TextData", f"text_data_w{start_week}_{end_week}.pickle")
        try:
            with open(text_file_path, 'wb') as f:
                pickle.dump(output_text_data, f)
            print(f"   âœ… æå–çš„æ–‡æœ¬æ•°æ®å·²ä¿å­˜åˆ°: {text_file_path}")
        except Exception as e:
            print(f"   âŒ ä¿å­˜æå–çš„æ–‡æœ¬æ•°æ®å¤±è´¥: {e}")

    def _load_behavior_sequences(self, start_week: int, end_week: int, final_user_list: List[str]) -> Dict[str, Any]:
        """
        åŠ è½½æˆ–æ„å»ºè¡Œä¸ºåºåˆ—æ•°æ®, åŸºäº final_user_listã€‚
        Args:
            final_user_list: æƒå¨çš„ç”¨æˆ·IDåˆ—è¡¨ã€‚
        """
        # æ–‡ä»¶åå¯ä»¥åŒ…å«ç”¨æˆ·æ•°é‡ä¿¡æ¯
        sequence_file_suffix = f"_u{len(final_user_list)}.pickle"
        sequence_file = os.path.join(self.multimodal_dir, "BehaviorSequences",
                                   f"behavior_sequences_w{start_week}_{end_week}{sequence_file_suffix}")

        loaded_data = None
        if os.path.exists(sequence_file) and not self.config.data.force_regenerate_sequences:
            print(f"   è¡Œä¸ºåºåˆ—å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½: {sequence_file}")
            try:
                with open(sequence_file, 'rb') as f:
                    loaded_data = pickle.load(f)
            except Exception as e:
                print(f"   âš ï¸ åŠ è½½ç¼“å­˜çš„è¡Œä¸ºåºåˆ—å¤±è´¥: {e}. å°†å°è¯•é‡æ–°æ„å»ºã€‚")
                loaded_data = None # ç¡®ä¿åœ¨åŠ è½½å¤±è´¥æ—¶ä¸ºNoneï¼Œä»¥ä¾¿åç»­é‡æ–°æ„å»º
            
            if loaded_data is not None: # ç¡®ä¿ä¸æ˜¯Noneæ‰è¿”å›
                # å¦‚æœåŠ è½½çš„æ•°æ®ä¸æ˜¯å­—å…¸ (ä¾‹å¦‚ï¼Œæ„å¤–ä¿å­˜ä¸ºNoneæˆ–å…¶ä»–ç±»å‹)ï¼Œåˆ™è¿”å›ç©ºå­—å…¸ä»¥é¿å…ä¸‹æ¸¸é”™è¯¯
                return loaded_data if isinstance(loaded_data, dict) else {}
        
        # å¦‚æœ loaded_data is None (å› ä¸ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œæˆ–åŠ è½½å¤±è´¥/å†…å®¹æ— æ•ˆ)
        # åˆ™ç»§ç»­å‘ä¸‹æ‰§è¡Œä»¥æ„å»ºæ–°çš„

        print(f"   æ„å»ºè¡Œä¸ºåºåˆ— (åŸºäº {len(final_user_list)} ä¸ªå·²ç­›é€‰ç”¨æˆ·)...")
        # _build_behavior_sequences_for_users åº”è¯¥æ€»æ˜¯è¿”å›dict
        new_behavior_sequences = self._build_behavior_sequences_for_users(
            start_week, end_week, final_user_list
        )
        
        # åŒé‡ä¿é™©ï¼šç¡®ä¿ new_behavior_sequences æ˜¯ä¸€ä¸ªå­—å…¸
        if new_behavior_sequences is None:
            print("   âš ï¸ _build_behavior_sequences_for_users è¿”å›äº† Noneï¼Œå°†ä½¿ç”¨ç©ºå­—å…¸ä»£æ›¿ã€‚")
            new_behavior_sequences = {}

        try:
            with open(sequence_file, 'wb') as f:
                pickle.dump(new_behavior_sequences, f)
            print(f"   è¡Œä¸ºåºåˆ—ä¿å­˜åˆ°: {sequence_file}")
        except Exception as e:
            print(f"   âš ï¸ ä¿å­˜è¡Œä¸ºåºåˆ—åˆ°pickleæ–‡ä»¶å¤±è´¥: {e}")
            # å³ä½¿ä¿å­˜å¤±è´¥ï¼Œä¹Ÿè¿”å›æ„å»ºçš„æ•°æ®ï¼Œé¿å…å½±å“å½“å‰è¿è¡Œ

        return new_behavior_sequences

    def _load_structured_features(self, start_week: int, end_week: int, final_user_list: List[str]) -> Dict[str, Any]:
        """
        åŠ è½½æˆ–å‡†å¤‡ç»“æ„åŒ–ç‰¹å¾, åŸºäº final_user_listã€‚
        Args:
            final_user_list: æƒå¨çš„ç”¨æˆ·IDåˆ—è¡¨ã€‚
        """
        # æ–‡ä»¶åå¯ä»¥åŒ…å«ç”¨æˆ·æ•°é‡ä¿¡æ¯
        features_file_suffix = f"_u{len(final_user_list)}.pickle"
        features_file = os.path.join(self.multimodal_dir, "StructuredFeatures",
                                   f"structured_features_w{start_week}_{end_week}{features_file_suffix}")

        if os.path.exists(features_file) and not self.config.data.force_regenerate_structured_features:
            print(f"   ç»“æ„åŒ–ç‰¹å¾å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½: {features_file}")
            with open(features_file, 'rb') as f:
                return pickle.load(f)
        
        print(f"   å‡†å¤‡ç»“æ„åŒ–ç‰¹å¾ (åŸºäº {len(final_user_list)} ä¸ªå·²ç­›é€‰ç”¨æˆ·)...")
        # _prepare_structured_features_for_users éœ€è¦æ¥æ”¶ç”¨æˆ·åˆ—è¡¨
        structured_features = self._prepare_structured_features_for_users(
            start_week, end_week, final_user_list
        )
        
        with open(features_file, 'wb') as f:
            pickle.dump(structured_features, f)
        print(f"   ç»“æ„åŒ–ç‰¹å¾ä¿å­˜åˆ°: {features_file}")
        return structured_features

    def _load_labels_data_from_df(self, final_users_df: pd.DataFrame) -> Dict[str, int]:
        """
        ä»ä¼ å…¥çš„å·²ç­›é€‰ç”¨æˆ·DataFrameåŠ è½½æ ‡ç­¾æ•°æ®ã€‚
        Args:
            final_users_df: åŒ…å«æœ€ç»ˆç”¨æˆ·åŠå…¶ç‰¹å¾ï¼ˆåŒ…æ‹¬æ ‡ç­¾ä¿¡æ¯ï¼‰çš„DataFrameã€‚
                           ç´¢å¼•åº”ä¸º user_idã€‚
        Returns:
            ä¸€ä¸ªå­—å…¸ {user_id: label}
        """
        print(f"   ä»å·²ç­›é€‰çš„DataFrameåŠ è½½ {len(final_users_df)} ä¸ªç”¨æˆ·çš„æ ‡ç­¾...")
        labels = {}
        for user_id, user_row_data in final_users_df.iterrows(): 
            user = user_id 
            if 'malscene' in user_row_data and user_row_data['malscene'] > 0:
                is_malicious = 1
            else:
                is_malicious = int(user_row_data.get('is_malicious', 0) == 1) 
            labels[user] = int(is_malicious)
        return labels
    
    def _integrate_multimodal_data(self, 
                                 user_graph_data: Dict[str, Any],
                                 text_data: Dict[str, Any], # æ–‡æœ¬æ•°æ®ä»æ˜¯å…¨å±€åŠ è½½çš„
                                 behavior_sequences: Dict[str, Any], # å·²æŒ‰final_user_listç­›é€‰
                                 structured_features: Dict[str, Any], # å·²æŒ‰final_user_listç­›é€‰
                                 labels_data: Dict[str, int], # å·²æŒ‰final_user_listç­›é€‰
                                 sequence_length: int,
                                 final_user_list: List[str]) -> Dict[str, Any]: # æƒå¨ç”¨æˆ·åˆ—è¡¨
        """æ•´åˆå¤šæ¨¡æ€æ•°æ®ï¼Œç¡®ä¿åŸºäºfinal_user_list"""
        logger = logging.getLogger(__name__) # <--- æ·»åŠ è¿™ä¸€è¡Œ
        
        print(f"   æ•´åˆå¤šæ¨¡æ€æ•°æ® (åŸºäº {len(final_user_list)} ä¸ªæƒå¨ç”¨æˆ·)...")
        # è·å–ç”¨æˆ·åˆ—è¡¨ - ç°åœ¨ç›´æ¥ä½¿ç”¨ final_user_list
        # users = list(labels_data.keys()) # æ—§æ–¹æ³•ï¼Œç°åœ¨ç”¨final_user_list
        users = final_user_list
        
        # ç¡®ä¿ user_graph_data['user_to_index'] åªåŒ…å« final_user_list ä¸­çš„ç”¨æˆ·
        # å¦‚æœå›¾æ˜¯ä¸ºè¿™äº›ç²¾ç¡®ç”¨æˆ·æ„å»ºçš„ï¼Œé‚£åº”è¯¥æ˜¯åŒ¹é…çš„ã€‚
        # ä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬åŸºäº final_user_list å’Œå›¾ä¸­å®é™…å­˜åœ¨çš„ç”¨æˆ·æ¥è°ƒæ•´
        
        graph_node_features = user_graph_data.get('node_features')
        graph_adj_matrix = user_graph_data.get('adjacency_matrix')
        graph_user_to_index = user_graph_data.get('user_to_index', {})

        # å¦‚æœå›¾æ•°æ®æ˜¯ä¸ºè¶…é›†ç”¨æˆ·æ„å»ºçš„ï¼Œéœ€è¦æ ¹æ®final_user_listè¿›è¡Œå­å›¾æå–
        # ä½†æˆ‘ä»¬ä¿®æ”¹äº†_load_user_graph_dataä½¿å…¶åŸºäºusers_df_for_graphæ„å»ºï¼Œ
        # æ‰€ä»¥è¿™é‡Œçš„ graph_user_to_index åº”è¯¥å·²ç»å¯¹åº” final_user_list
        # å› æ­¤ï¼Œæˆ‘ä»¬ä¿¡ä»»ä¼ é€’è¿‡æ¥çš„ graph_user_to_index
        
        # å‡†å¤‡å„æ¨¡æ€æ•°æ®
        behavior_seq_data = []
        text_content_data = []
        structured_feat_data = []
        labels = []
        user_indices_in_graph_list = [] # æ–°å¢ï¼šç”¨äºæ”¶é›†æ¯ä¸ªç”¨æˆ·çš„å›¾ç´¢å¼•
        
        for user in users:
            # 1. è¡Œä¸ºåºåˆ—æ•°æ®
            if user in behavior_sequences:
                user_sequences = behavior_sequences[user]
                if len(user_sequences) > 0:
                    # å–æœ€åä¸€ä¸ªåºåˆ—æˆ–åˆå¹¶å¤šä¸ªåºåˆ—
                    if len(user_sequences) == 1:
                        seq = user_sequences[0]
                    else:
                        # åˆå¹¶å¤šä¸ªåºåˆ—
                        seq = np.concatenate(user_sequences, axis=0)
                    
                    # æˆªæ–­æˆ–å¡«å……åˆ°æŒ‡å®šé•¿åº¦
                    if len(seq) > sequence_length:
                        seq = seq[-sequence_length:]
                    elif len(seq) < sequence_length:
                        padding = np.zeros((sequence_length - len(seq), seq.shape[1]))
                        seq = np.concatenate([padding, seq], axis=0)
                    
                    behavior_seq_data.append(seq)
                else:
                    behavior_seq_data.append(np.zeros((sequence_length, self.feature_dim)))
            else:
                behavior_seq_data.append(np.zeros((sequence_length, self.feature_dim)))
            
            # 2. æ–‡æœ¬å†…å®¹æ•°æ®
            user_texts = []
            for week_data in text_data.values():
                if user in week_data.get('email_texts', {}):
                    user_texts.extend(week_data['email_texts'][user])
                if user in week_data.get('file_texts', {}):
                    user_texts.extend(week_data['file_texts'][user])
                if user in week_data.get('http_texts', {}):
                    user_texts.extend(week_data['http_texts'][user])
            
            # åˆå¹¶æ–‡æœ¬å†…å®¹
            combined_text = " ".join(user_texts[:10])  # é™åˆ¶æ–‡æœ¬é•¿åº¦
            if not combined_text.strip():
                combined_text = "No text content"
            text_content_data.append(combined_text)
            
            # 3. ç»“æ„åŒ–ç‰¹å¾æ•°æ®
            if user in structured_features:
                user_features = structured_features[user]
                if len(user_features) > 0:
                    # å–å¹³å‡å€¼æˆ–æœ€åä¸€ä¸ªç‰¹å¾å‘é‡
                    avg_features = np.mean(user_features, axis=0)
                    structured_feat_data.append(avg_features)
                else:
                    structured_feat_data.append(np.zeros(50))  # é»˜è®¤ç‰¹å¾ç»´åº¦
            else:
                structured_feat_data.append(np.zeros(50))
            
            # 4. æ ‡ç­¾
            labels.append(labels_data[user])

            # 5. ç”¨æˆ·åœ¨å›¾ä¸­çš„ç´¢å¼• (æ–°å¢)
            if user in graph_user_to_index:
                user_indices_in_graph_list.append(graph_user_to_index[user])
            else:
                # å¦‚æœç”¨æˆ·ä¸åœ¨å›¾çš„æ˜ å°„ä¸­ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå¦‚æœfinal_user_listæ˜¯æƒå¨çš„ä¸”å›¾æ˜¯ä¸ºè¿™äº›ç”¨æˆ·æ„å»ºçš„ï¼‰
                # æ·»åŠ ä¸€ä¸ªæ— æ•ˆç´¢å¼•ï¼Œä¾‹å¦‚ -1
                user_indices_in_graph_list.append(-1) 
                logger.warning(f"ç”¨æˆ· {user} æœªåœ¨ graph_user_to_index ä¸­æ‰¾åˆ°ï¼Œå›¾ç´¢å¼•è®¾ä¸º-1")
        
        return {
            'behavior_sequences': np.array(behavior_seq_data),
            'node_features': graph_node_features,
            'adjacency_matrix': graph_adj_matrix,
            'text_content': text_content_data,
            'structured_features': np.array(structured_feat_data),
            'labels': np.array(labels),
            'users': users,
            'user_to_index': graph_user_to_index,
            'user_indices_in_graph': np.array(user_indices_in_graph_list) # æ–°å¢ï¼šæ·»åŠ å›¾ç´¢å¼•åˆ—è¡¨
        }
    
    def _construct_user_relationships(self, users_df: pd.DataFrame, # ç°åœ¨è¿™ä¸ªusers_dfæ˜¯å·²ç»ç­›é€‰è¿‡çš„
                                    start_week: int, end_week: int) -> Dict[str, Any]:
        """æ„å»ºç”¨æˆ·å…³ç³»å›¾æ•°æ®ï¼Œä¸å†è¿›è¡Œå†…éƒ¨max_usersç­›é€‰"""
        
        # users_df å·²ç»æ˜¯ç­›é€‰è¿‡çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦è¿™é‡Œçš„ max_users é€»è¾‘äº†
        # if max_users:
        #     # ... (æ—§çš„max_usersç­›é€‰é€»è¾‘) ...

        # users = users_df['user'].tolist() # æ—§çš„ï¼Œä¸”å¯èƒ½å‡ºé”™
        users = users_df.index.tolist() # æ­£ç¡®çš„æ–¹å¼ï¼Œå‡è®¾ç´¢å¼•æ˜¯ user_id
        
        num_users = len(users)
        user_to_index_map = {user: i for i, user in enumerate(users)}
        
        # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µ
        adjacency_matrix = np.zeros((num_users, num_users))
        
        # æ„å»ºç”¨æˆ·ç‰¹å¾çŸ©é˜µ
        user_features = []
        for _, user_row in users_df.iterrows():
            # åŸºç¡€ç‰¹å¾ï¼šè§’è‰²ã€éƒ¨é—¨ç­‰
            features = []
            
            # è§’è‰²ç‰¹å¾ (one-hotç¼–ç )
            role = user_row.get('role', 'Unknown')
            role_features = self._encode_categorical_feature(role, ['ITAdmin', 'Manager', 'Employee', 'Unknown'])
            features.extend(role_features)
            
            # éƒ¨é—¨ç‰¹å¾
            dept = user_row.get('functional_unit', 'Unknown')
            dept_features = self._encode_categorical_feature(dept, ['IT', 'Finance', 'HR', 'Sales', 'Unknown'])
            features.extend(dept_features)
            
            # OCEANå¿ƒç†ç‰¹å¾
            ocean_features = [
                user_row.get('O', 0.5), user_row.get('C', 0.5), 
                user_row.get('E', 0.5), user_row.get('A', 0.5), user_row.get('N', 0.5)
            ]
            features.extend(ocean_features)
            
            user_features.append(features)
        
        # æ„å»ºç”¨æˆ·å…³ç³»ï¼ˆåŸºäºå…±åŒæ´»åŠ¨ã€éƒ¨é—¨ç­‰ï¼‰
        for i, user1 in enumerate(users):
            for j, user2 in enumerate(users):
                if i != j:
                    # è®¡ç®—ç”¨æˆ·å…³ç³»å¼ºåº¦
                    relationship_strength = self._calculate_user_relationship(
                        users_df.iloc[i], users_df.iloc[j], start_week, end_week
                    )
                    adjacency_matrix[i, j] = relationship_strength
        
        return {
            'users': users,
            'node_features': np.array(user_features, dtype=np.float32),
            'adjacency_matrix': adjacency_matrix.astype(np.float32),
            'user_to_index': user_to_index_map
        }
    
    def _encode_categorical_feature(self, value: str, categories: List[str]) -> List[float]:
        """å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œone-hotç¼–ç """
        encoding = [0.0] * len(categories)
        if value in categories:
            encoding[categories.index(value)] = 1.0
        else:
            encoding[-1] = 1.0  # Unknownç±»åˆ«
        return encoding
    
    def _calculate_user_relationship(self, user1: pd.Series, user2: pd.Series,
                                   start_week: int, end_week: int) -> float:
        """è®¡ç®—ä¸¤ä¸ªç”¨æˆ·ä¹‹é—´çš„å…³ç³»å¼ºåº¦"""
        relationship_score = 0.0
        
        # 1. éƒ¨é—¨å…³ç³»
        if user1.get('functional_unit') == user2.get('functional_unit'):
            relationship_score += 0.3
        
        # 2. è§’è‰²å…³ç³»
        if user1.get('role') == user2.get('role'):
            relationship_score += 0.2
        
        # 3. OCEANç›¸ä¼¼æ€§
        ocean_features = ['O', 'C', 'E', 'A', 'N']
        ocean_similarity = 0.0
        for feature in ocean_features:
            val1 = user1.get(feature, 0.5)
            val2 = user2.get(feature, 0.5)
            ocean_similarity += 1.0 - abs(val1 - val2)
        ocean_similarity /= len(ocean_features)
        relationship_score += 0.3 * ocean_similarity
        
        # 4. æ´»åŠ¨äº¤äº’ï¼ˆåŸºäºå…±åŒçš„æ–‡ä»¶è®¿é—®ã€é‚®ä»¶é€šä¿¡ç­‰ï¼‰
        # TODO: åŸºäºå®é™…æ´»åŠ¨æ•°æ®è®¡ç®—äº¤äº’å¼ºåº¦
        interaction_score = self._calculate_activity_interaction(
            user1.name, user2.name, start_week, end_week  # ä½¿ç”¨ .name è·å–Seriesçš„ç´¢å¼•åï¼Œå³user_id
        )
        relationship_score += 0.2 * interaction_score
        
        return min(relationship_score, 1.0)
    
    def _calculate_activity_interaction(self, user1: str, user2: str,
                                      start_week: int, end_week: int) -> float:
        """è®¡ç®—ç”¨æˆ·é—´çš„æ´»åŠ¨äº¤äº’å¼ºåº¦"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºç”¨æˆ·åç›¸ä¼¼æ€§
        # å®é™…åº”ç”¨ä¸­åº”è¯¥åŸºäºçœŸå®çš„æ´»åŠ¨äº¤äº’æ•°æ®
        if user1 == user2:
            return 0.0
        
        # åŸºäºç”¨æˆ·åçš„ç®€å•ç›¸ä¼¼æ€§è®¡ç®—
        common_chars = set(user1.lower()) & set(user2.lower())
        similarity = len(common_chars) / max(len(set(user1.lower())), len(set(user2.lower())))
        
        return min(similarity, 0.5)  # é™åˆ¶æœ€å¤§äº¤äº’å¼ºåº¦

    def _build_behavior_sequences_for_users(self, start_week: int, end_week: int, user_list: List[str]) -> Dict[str, list]:
        """ä¸ºæŒ‡å®šç”¨æˆ·åˆ—è¡¨æ„å»ºè¡Œä¸ºåºåˆ—æ•°æ® (ä¼˜åŒ–ç‰ˆï¼Œå‡å°‘I/O)"""
        
        # 1. ç¼–ç å™¨æ‹Ÿåˆ (å¦‚æœéœ€è¦) - è¿™éƒ¨åˆ†é€»è¾‘ä¿æŒï¼Œä½†è¯»å–æ–¹å¼éœ€è¦æ³¨æ„
        if not self.base_pipeline.encoder.is_fitted:
            print("   âš ï¸ è¡Œä¸ºåºåˆ—ç¼–ç å™¨æœªæ‹Ÿåˆï¼Œå°è¯•ä»åŸºç¡€æµæ°´çº¿è·å–è®­ç»ƒæ•°æ®è¿›è¡Œæ‹Ÿåˆ...")
            temp_users_df_for_encoder = self.base_pipeline.step2_load_user_data()
            if not temp_users_df_for_encoder.empty:
                sample_events_for_encoder = []
                # ä» CERTDatasetPipeline çš„ step1_combine_raw_data è¾“å‡ºçš„ Parquet ç›®å½•ä¸­è¯»å–åŸå§‹äº‹ä»¶æ•°æ®
                # è¯¥ç›®å½•æŒ‰å‘¨åˆ†åŒºï¼Œä¾‹å¦‚ DataByWeek_parquet/week=0/, DataByWeek_parquet/week=1/
                base_event_parquet_dir = self.base_pipeline._get_work_file_path(self.base_pipeline.current_parquet_dir_name)
                
                if not os.path.exists(base_event_parquet_dir):
                    print(f"    âŒ åŸºç¡€äº‹ä»¶ Parquet ç›®å½• {base_event_parquet_dir} ä¸å­˜åœ¨ï¼Œæ— æ³•æ‹Ÿåˆç¼–ç å™¨ã€‚")
                else:
                    for week_idx in range(start_week, min(end_week, start_week + 2)): # ç”¨å‰å‡ å‘¨æ•°æ®æ‹Ÿåˆ
                        print(f"      ä¸ºç¼–ç å™¨æ‹Ÿåˆè¯»å–å‘¨ {week_idx} ä» {base_event_parquet_dir}")
                        try:
                            week_event_data_ddf = dd.read_parquet(
                                base_event_parquet_dir,
                                filters=[('week', '==', week_idx)], # å‡è®¾ 'week' åˆ—å­˜åœ¨äºåŸå§‹åˆå¹¶æ•°æ®ä¸­
                                engine='pyarrow'
                            )
                            if not week_event_data_ddf.map_partitions(len).compute().sum() == 0:
                                week_event_data = week_event_data_ddf.compute()
                                if not week_event_data.empty:
                                    sample_events_for_encoder.append(week_event_data.sample(min(10000, len(week_event_data)), random_state=42)) # å¢åŠ é‡‡æ ·æ•°é‡
                                    print(f"        å‘¨ {week_idx}: é‡‡æ · {len(sample_events_for_encoder[-1])} æ¡äº‹ä»¶ç”¨äºç¼–ç å™¨ã€‚")
                            else:
                                print(f"        å‘¨ {week_idx}: æ— äº‹ä»¶æ•°æ®ã€‚")
                        except Exception as e_parquet_fit:
                            print(f"    è¯»å–å‘¨ {week_idx} Parquet (ç”¨äºç¼–ç å™¨æ‹Ÿåˆ) å¤±è´¥: {e_parquet_fit}")
                
                if sample_events_for_encoder:
                    all_sample_events = pd.concat(sample_events_for_encoder, ignore_index=True)
                    print(f"      æ€»å…±é‡‡æ · {len(all_sample_events)} æ¡äº‹ä»¶ç”¨äºç¼–ç å™¨æ‹Ÿåˆã€‚")
                    self.base_pipeline.encoder.fit(all_sample_events, temp_users_df_for_encoder)
                    print("   âœ… è¡Œä¸ºåºåˆ—ç¼–ç å™¨å·²æ‹Ÿåˆã€‚")
                else:
                    print("   âŒ æ— æ³•ä¸ºè¡Œä¸ºåºåˆ—ç¼–ç å™¨æ‰¾åˆ°è®­ç»ƒæ•°æ®ã€‚åºåˆ—å¯èƒ½ä¸ºç©ºæˆ–ä½¿ç”¨é»˜è®¤ç¼–ç ã€‚")
            else:
                 print("   âŒ æ— æ³•åŠ è½½ç”¨æˆ·æ•°æ®ä»¥æ‹Ÿåˆè¡Œä¸ºåºåˆ—ç¼–ç å™¨ã€‚åºåˆ—å¯èƒ½ä¸ºç©ºæˆ–ä½¿ç”¨é»˜è®¤ç¼–ç ã€‚")
        
        # 2. æ”¶é›†æ‰€æœ‰ç”¨æˆ·çš„å‘¨äº‹ä»¶æ•°æ®
        user_all_events_map = {user: [] for user in user_list}
        print(f"   ğŸ”„ å¼€å§‹ä¸º {len(user_list)} ç”¨æˆ·æ”¶é›†æ‰€æœ‰å‘¨ ({start_week}-{end_week-1}) çš„äº‹ä»¶ç‰¹å¾æ•°æ®...")
        num_data_by_week_dir = self.base_pipeline._get_work_file_path("NumDataByWeek")

        for week in range(start_week, end_week):
            # print(f"     å¤„ç†å‘¨ {week} çš„ç‰¹å¾æ–‡ä»¶...") # å¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥è·å–æ›´è¯¦ç»†çš„æ—¥å¿—
            feature_file_parquet = os.path.join(num_data_by_week_dir, f"{week}_features.parquet")
            feature_file_pickle = os.path.join(num_data_by_week_dir, f"{week}_features.pickle")
            
            week_data_df = None
            if os.path.exists(feature_file_parquet):
                try:
                    week_data_df = pd.read_parquet(feature_file_parquet, engine='pyarrow')
                except Exception as e_read_p:
                    # print(f"      å‘¨ {week}: è¯»å– Parquet ç‰¹å¾æ–‡ä»¶ {feature_file_parquet} å¤±è´¥ ({e_read_p}). å°è¯• Pickle...")
                    if os.path.exists(feature_file_pickle):
                        try: week_data_df = pd.read_pickle(feature_file_pickle)
                        except Exception as e_read_pk: 
                            # print(f"      å‘¨ {week}: è¯»å– Pickle æ–‡ä»¶ {feature_file_pickle} ä¹Ÿå¤±è´¥ ({e_read_pk}).")
                            pass # å¿½ç•¥å•ä¸ªæ–‡ä»¶è¯»å–é”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€å‘¨
            elif os.path.exists(feature_file_pickle):
                # print(f"      å‘¨ {week}: Parquet ä¸å­˜åœ¨ï¼Œå°è¯•è¯»å– Pickle {feature_file_pickle}...")
                try: week_data_df = pd.read_pickle(feature_file_pickle)
                except Exception as e_read_pk2: 
                    # print(f"      å‘¨ {week}: è¯»å– Pickle æ–‡ä»¶ {feature_file_pickle} å¤±è´¥ ({e_read_pk2}).")
                    pass
            
            if week_data_df is not None and not week_data_df.empty and 'user' in week_data_df.columns:
                relevant_week_data = week_data_df[week_data_df['user'].isin(user_list)]
                if not relevant_week_data.empty:
                    for user_id_in_file, user_events_this_week in relevant_week_data.groupby('user'):
                        if user_id_in_file in user_all_events_map: # ç¡®ä¿keyå­˜åœ¨
                             user_all_events_map[user_id_in_file].append(user_events_this_week)
            # else:
                # if week_data_df is None: print(f"     å‘¨ {week}: æ— æ•°æ®æ–‡ä»¶ã€‚")
                # elif week_data_df.empty: print(f"     å‘¨ {week}: æ•°æ®æ–‡ä»¶ä¸ºç©ºã€‚")
                # elif 'user' not in week_data_df.columns: print(f"     å‘¨ {week}: æ•°æ®æ–‡ä»¶ç¼ºå°‘ 'user' åˆ—ã€‚")

        # 3. ä¸ºæ¯ä¸ªç”¨æˆ·æ„å»ºæœ€ç»ˆåºåˆ—
        behavior_sequences = {}
        print(f"   ğŸ”„ å¼€å§‹ä¸º {len(user_list)} ä¸ªç”¨æˆ·æ•´ç†å’Œæ„å»ºæœ€ç»ˆçš„è¡Œä¸ºåºåˆ—...")
        
        for user_idx, user in enumerate(user_list):
            if (user_idx + 1) % 50 == 0:
                 print(f"     æ„å»ºåºåˆ—: ç”¨æˆ· {user_idx+1}/{len(user_list)} ({user})")

            user_final_sequences = [] # ä¸€ä¸ªç”¨æˆ·å¯èƒ½åªæœ‰ä¸€ä¸ªä¸»åºåˆ—ï¼Œæˆ–æŒ‰æŸç§é€»è¾‘åˆ‡åˆ†
            user_collected_event_dfs = user_all_events_map.get(user, []) # ä½¿ç”¨ .get é˜²æ­¢KeyError
            
            if user_collected_event_dfs:
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç©ºçš„DataFrameåœ¨åˆ—è¡¨ä¸­ï¼Œè¿™å¯èƒ½ç”±è¯»å–é”™è¯¯æˆ–ç©ºæ–‡ä»¶å¯¼è‡´
                non_empty_dfs = [df for df in user_collected_event_dfs if not df.empty and 'features' in df.columns]
                if not non_empty_dfs:
                    # print(f"      ç”¨æˆ· {user}: æ²¡æœ‰æœ‰æ•ˆçš„äº‹ä»¶DataFrameå¯åˆå¹¶ã€‚")
                    user_final_sequences.append(np.zeros((1, self.feature_dim))) # æ·»åŠ é»˜è®¤åºåˆ—
                    behavior_sequences[user] = user_final_sequences
                    continue

                try:
                    user_combined_events_df = pd.concat(non_empty_dfs, ignore_index=True)
                except ValueError as e_concat: # ä¾‹å¦‚ï¼Œæ²¡æœ‰DataFrameå¯ä»¥åˆå¹¶
                    # print(f"      ç”¨æˆ· {user}: åˆå¹¶äº‹ä»¶DataFrameæ—¶å‡ºé”™: {e_concat}")
                    user_final_sequences.append(np.zeros((1, self.feature_dim)))
                    behavior_sequences[user] = user_final_sequences
                    continue

                if 'date' in user_combined_events_df.columns:
                    user_combined_events_df = user_combined_events_df.sort_values('date')
                
                # æå–ç‰¹å¾åˆ—å¹¶æ„å»ºåºåˆ—
                feature_vectors_list = []
                # ç›´æ¥è¿­ä»£Seriesçš„valuesï¼Œè¿™æ¯”iterrowsæ›´å¿«ï¼Œè€Œä¸”æˆ‘ä»¬åªå…³å¿ƒ'features'åˆ—
                for features_item in user_combined_events_df['features'].values:
                    if isinstance(features_item, list):
                        feature_vectors_list.append(np.array(features_item, dtype=np.float32))
                    elif isinstance(features_item, np.ndarray):
                        feature_vectors_list.append(features_item.astype(np.float32))
                    else: # Fallback for unexpected types or missing data for an event
                        feature_vectors_list.append(np.zeros(self.feature_dim, dtype=np.float32))
                
                if feature_vectors_list:
                    try:
                        # ç¡®ä¿æ‰€æœ‰å†…éƒ¨æ•°ç»„çš„ç»´åº¦ä¸€è‡´ï¼Œç‰¹åˆ«æ˜¯ç‰¹å¾ç»´åº¦ self.feature_dim
                        # np.stack è¦æ±‚æ‰€æœ‰æ•°ç»„å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼ˆé™¤äº†è¦å †å çš„è½´ï¼‰
                        # å¦‚æœç‰¹å¾æ˜¯ (dim,)ï¼Œå †å åæ˜¯ (num_events, dim)
                        valid_feature_vectors = []
                        for f_vec in feature_vectors_list:
                            if f_vec.ndim == 1 and f_vec.shape[0] == self.feature_dim:
                                valid_feature_vectors.append(f_vec)
                            elif f_vec.ndim == 2 and f_vec.shape[1] == self.feature_dim and f_vec.shape[0] > 0: # å¦‚æœå·²ç»æ˜¯ (N, dim)
                                valid_feature_vectors.extend(list(f_vec)) # å±•å¹³å¹¶æ·»åŠ 
                            # else:
                                # print(f"    ç”¨æˆ· {user}: å‘ç°ç»´åº¦ä¸åŒ¹é…çš„ç‰¹å¾å‘é‡ï¼Œå½¢çŠ¶ {f_vec.shape}, é¢„æœŸç»´åº¦ {self.feature_dim}ã€‚å°†è·³è¿‡æ­¤å‘é‡ã€‚")
                        
                        if valid_feature_vectors:
                            sequence_array = np.stack(valid_feature_vectors)
                            user_final_sequences.append(sequence_array)
                        # else:
                            # print(f"    ç”¨æˆ· {user}: æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾å‘é‡å¯å †å ã€‚")

                    except ValueError as e_stack:
                        # print(f"      âš ï¸ ç”¨æˆ· {user}: å †å ç‰¹å¾å‘é‡æ—¶å‡ºé”™: {e_stack}. (æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ä¸€è‡´)")
                        pass # ä¿æŒ user_final_sequences ä¸ºç©ºï¼Œåç»­ä¼šè¡¥é›¶

                # å¦‚æœåœ¨å¤„ç†å user_final_sequences ä»ç„¶ä¸ºç©º (ä¾‹å¦‚ï¼Œæ²¡æœ‰äº‹ä»¶ï¼Œæˆ–ç‰¹å¾æå–/å †å å¤±è´¥)
                if not user_final_sequences or (len(user_final_sequences) > 0 and user_final_sequences[0].shape[0] == 0):
                    user_final_sequences = [np.zeros((1, self.feature_dim), dtype=np.float32)]
            else: # å¦‚æœç”¨æˆ·æ²¡æœ‰ä»»ä½•æ”¶é›†åˆ°çš„äº‹ä»¶DataFrame
                user_final_sequences.append(np.zeros((1, self.feature_dim), dtype=np.float32))
            
            behavior_sequences[user] = user_final_sequences
            
        print(f"   âœ… æ‰€æœ‰ç”¨æˆ·çš„è¡Œä¸ºåºåˆ—æ„å»ºå®Œæˆã€‚")
        return behavior_sequences

    def _prepare_structured_features_for_users(self, start_week: int, end_week: int, user_list: List[str]) -> Dict[str, list]:
        """ä¸ºæŒ‡å®šç”¨æˆ·åˆ—è¡¨å‡†å¤‡ç»“æ„åŒ–ç‰¹å¾ (æ›¿æ¢æ—§çš„ _prepare_structured_features)"""
        # print("ğŸ“‹ å‡†å¤‡ç»“æ„åŒ–ç‰¹å¾...") # æ—¥å¿—ç§»åˆ°è°ƒç”¨å¤„
        
        week_features_dir = os.path.join(self.work_dir, "WeekLevelFeatures")
        structured_features_all_users = {} # å…ˆåŠ è½½æ‰€æœ‰ç”¨æˆ·çš„å‘¨ç‰¹å¾

        for week in range(start_week, end_week):
            # æ³¨æ„ï¼šCERTDatasetPipeline ç”Ÿæˆçš„å‘¨çº§åˆ«ç‰¹å¾æ–‡ä»¶åæ˜¯ weeks_START_END.csv
            # è€Œä¸æ˜¯ week_{week}_features.csvã€‚è¿™é‡Œéœ€è¦åŒ¹é…ã€‚
            # å‡è®¾æ–‡ä»¶åæ˜¯ pipeline.run_full_pipeline ä¸­ _week_level_analysis ç”Ÿæˆçš„
            # WeekLevelFeatures/weeks_{start_week}_{end_week-1}.csv
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾è¿™é‡Œçš„æ–‡ä»¶åæ˜¯æŒ‰å‘¨çš„ï¼Œæˆ–è€…éœ€è¦è°ƒæ•´æ–‡ä»¶åé€»è¾‘
            # æˆ–è€…ï¼Œæ›´ç¨³å¦¥çš„æ˜¯ï¼Œæˆ‘ä»¬ä» NumDataByWeek/{week}_features.pickle é‡æ–°èšåˆæˆ‘ä»¬éœ€è¦çš„ç”¨æˆ·
            
            # æ–¹æ¡ˆ1: å°è¯•è¯»å–å·²èšåˆçš„å‘¨çº§åˆ«CSV (å¯èƒ½éœ€è¦ä¿®æ”¹æ–‡ä»¶åé€»è¾‘)
            # week_file_path = os.path.join(week_features_dir, f"weeks_{week}_{week}.csv") # å‡è®¾æœ‰è¿™æ ·çš„æ–‡ä»¶
            # if not os.path.exists(week_file_path):
            #     week_file_path = os.path.join(week_features_dir, f"weeks_{start_week}_{end_week-1}.csv")

            # æ–¹æ¡ˆ2: ä» NumDataByWeek é‡æ–°èšåˆ (æ›´å¯é ï¼Œä½†å¯èƒ½æ…¢ä¸€ç‚¹)
            # --- ä¿®æ”¹å¼€å§‹: ä¼˜å…ˆè¯»å– Parquet, å›é€€åˆ° Pickle ---
            feature_file_parquet = self.base_pipeline._get_work_file_path(f"NumDataByWeek/{week}_features.parquet")
            feature_file_pickle = self.base_pipeline._get_work_file_path(f"NumDataByWeek/{week}_features.pickle")
            
            week_user_event_features_df = None
            if os.path.exists(feature_file_parquet):
                try:
                    week_user_event_features_df = pd.read_parquet(feature_file_parquet, engine='pyarrow')
                except Exception as e_parquet_read:
                    print(f"    âš ï¸ å‘¨ {week}: è¯»å– Parquet ç‰¹å¾æ–‡ä»¶ {feature_file_parquet} å¤±è´¥ ({e_parquet_read}). å°è¯• Pickle å›é€€...")
                    if os.path.exists(feature_file_pickle):
                        try:
                            week_user_event_features_df = pd.read_pickle(feature_file_pickle)
                            print(f"      âœ… å‘¨ {week}: æˆåŠŸä» Pickle æ–‡ä»¶ {feature_file_pickle} å›é€€è¯»å–ã€‚")
                        except Exception as e_pickle_read:
                            print(f"      âŒ å‘¨ {week}: è¯»å– Pickle å›é€€æ–‡ä»¶ {feature_file_pickle} ä¹Ÿå¤±è´¥ ({e_pickle_read}).")
                    else:
                         print(f"      å‘¨ {week}: Pickle å›é€€æ–‡ä»¶ {feature_file_pickle} ä¸å­˜åœ¨ã€‚")
            elif os.path.exists(feature_file_pickle):
                print(f"    å‘¨ {week}: Parquet ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è¯»å– Pickle æ–‡ä»¶ {feature_file_pickle}...")
                try:
                    week_user_event_features_df = pd.read_pickle(feature_file_pickle)
                except Exception as e_pickle_read:
                    print(f"      âŒ å‘¨ {week}: è¯»å– Pickle æ–‡ä»¶ {feature_file_pickle} å¤±è´¥ ({e_pickle_read}).")
            # --- ä¿®æ”¹ç»“æŸ ---

            if week_user_event_features_df is not None and not week_user_event_features_df.empty:
                # week_user_event_features_df = pd.read_pickle(feature_pickle_file)
                # if week_user_event_features_df.empty:
                #     continue
                for user_id_in_file in week_user_event_features_df['user'].unique():
                    if user_id_in_file not in structured_features_all_users:
                        structured_features_all_users[user_id_in_file] = []
                    
                    user_events_this_week = week_user_event_features_df[week_user_event_features_df['user'] == user_id_in_file]
                    
                    # è°ƒç”¨ CERTDatasetPipeline ä¸­çš„èšåˆé€»è¾‘ (ç®€åŒ–ç‰ˆ)
                    # æ³¨æ„ï¼š_aggregate_user_features è¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å¤šä¸ªå¹³å±•çš„ç‰¹å¾
                    aggregated_feats_dict = self.base_pipeline._aggregate_user_features(user_events_this_week, 'week')
                    
                    # å°†å­—å…¸è½¬æ¢ä¸ºç‰¹å¾å‘é‡ (éœ€è¦ç¡®å®šé¡ºåºå’Œå“ªäº›ç‰¹å¾)
                    # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆå‡è®¾å®ƒè¿”å›ä¸€ä¸ªå›ºå®šé¡ºåºå’Œæ•°é‡çš„æ•°å€¼ç‰¹å¾åˆ—è¡¨
                    # æˆ–è€…ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ CERTDatasetPipeline ä¸­ä¿å­˜åˆ°CSVçš„é‚£äº›åˆ—
                    # è¿™é‡Œç”¨ä¸€ä¸ªç®€åŒ–æ–¹å¼ï¼šå–å­—å…¸ä¸­æ‰€æœ‰æ•°å€¼ç±»å‹çš„å€¼
                    feature_vector = [v for k, v in aggregated_feats_dict.items() if isinstance(v, (int, float, np.number))]
                    
                    # ç¡®ä¿ç‰¹å¾å‘é‡é•¿åº¦ä¸€è‡´ (å¦‚æœéœ€è¦ï¼Œå¦åˆ™åç»­æ¨¡å‹è¾“å…¥ä¼šå‡ºé—®é¢˜)
                    # è¿™é‡Œçš„é»˜è®¤50ç»´æ˜¯åœ¨ train_pipeline.py ä¸­ MultiModalAnomalyDetector å‡è®¾çš„
                    # éœ€è¦ä¸æ¨¡å‹å®šä¹‰åŒ¹é…ï¼Œæˆ–è€…æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¯å˜é•¿åº¦çš„ç»“æ„åŒ–ç‰¹å¾
                    # æš‚æ—¶æˆ‘ä»¬ä¸å¼ºåˆ¶é•¿åº¦ï¼Œç”±æ¨¡å‹éƒ¨åˆ†å¤„ç†æˆ–æŠ¥é”™
                    if feature_vector: # åªæœ‰æå–åˆ°ç‰¹å¾æ‰æ·»åŠ 
                         structured_features_all_users[user_id_in_file].append(feature_vector)
            else:
                # print(f"   âš ï¸ æœªæ‰¾åˆ°å‘¨ {week} çš„æ•°å€¼ç‰¹å¾æ–‡ä»¶ (Parquet/Pickle): {feature_file_parquet} / {feature_file_pickle}")
                # å³ä½¿æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œä¹Ÿç»§ç»­å¤„ç†ä¸‹ä¸€å‘¨ï¼Œè€Œä¸æ˜¯æ‰“å°æ¯æ¡æ¶ˆæ¯
                pass # å·²ç»åœ¨ä¸Šé¢å¤„ç†äº†æ‰“å°


        # ä» structured_features_all_users ä¸­ç­›é€‰å‡º final_user_list æ‰€éœ€çš„
        final_structured_features = {}
        for user in user_list:
            if user in structured_features_all_users and structured_features_all_users[user]:
                # å¯¹äºæ¯ä¸ªç”¨æˆ·ï¼Œå¯èƒ½æœ‰å¤šä¸ªå‘¨çš„ç‰¹å¾å‘é‡ï¼Œå¯ä»¥å–å¹³å‡æˆ–æœ€åä¸€ä¸ª
                # MultiModalDataPipeline._integrate_multimodal_data ä¸­æ˜¯å–çš„å¹³å‡
                # æˆ‘ä»¬è¿™é‡Œä¿æŒåˆ—è¡¨ï¼Œç”± _integrate_multimodal_data å¤„ç†
                final_structured_features[user] = structured_features_all_users[user]
            else:
                final_structured_features[user] = [] # å¦‚æœæŸç”¨æˆ·æ²¡æœ‰ç»“æ„åŒ–ç‰¹å¾
        
        return final_structured_features

    def run_full_multimodal_pipeline(self,
                                   start_week: int = 0,
                                   end_week: int = None,
                                   max_users: int = None, # è¿™ä¸ªmax_userså°†ç”¨äºç»Ÿä¸€ç­›é€‰
                                   sequence_length: int = 128) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„å¤šæ¨¡æ€æ•°æ®å¤„ç†æµæ°´çº¿
        """
        print(f"\n{'='*80}")
        print(f"è¿è¡Œå®Œæ•´å¤šæ¨¡æ€æ•°æ®å¤„ç†æµæ°´çº¿ (max_users={max_users} å°†åœ¨prepare_training_dataä¸­ç»Ÿä¸€åº”ç”¨)")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        # Step 1: è¿è¡ŒåŸºç¡€ç‰¹å¾æå– (è¿™ä¸€æ­¥ç†è®ºä¸Šåº”è¯¥æä¾›æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®ç»™åç»­ç­›é€‰)
        # max_users åœ¨è¿™é‡Œè®¾ç½®ä¸ºNoneï¼Œè®©å®ƒå¤„ç†æ‰€æœ‰ç”¨æˆ·ï¼Œç„¶ååœ¨prepare_training_dataä¸­ç­›é€‰
        self.run_base_feature_extraction(start_week, end_week, max_users=None, 
                                         sample_ratio=getattr(self.config.data, 'sample_ratio', 1.0)) # ä½¿ç”¨å±æ€§è®¿é—®ï¼Œè‹¥æ— åˆ™é»˜è®¤ä¸º1.0
        
        # Step 2: æå–å¤šæ¨¡æ€æ•°æ® - æ­¤æ­¥éª¤å·²è¢«æ•´åˆåˆ° prepare_training_data ä¸­
        # initial_users_df_for_step2 = self.base_pipeline.step2_load_user_data()
        # final_selected_users_df_for_step2 = self._apply_max_users_filter(initial_users_df_for_step2, max_users)
        # # self.extract_multimodal_data(start_week, end_week, final_selected_users_df_for_step2) # å·²ç§»é™¤
        
        # Step 3: å‡†å¤‡è®­ç»ƒæ•°æ® - è¿™ä¸€æ­¥ç°åœ¨æ˜¯æ ¸å¿ƒï¼Œå®ƒä¼šé©±åŠ¨å„æ¨¡æ€æ•°æ®çš„åŠ è½½/ç”Ÿæˆ
        # å¹¶ç¡®ä¿ç”¨æˆ·ä¸€è‡´æ€§
        training_data = self.prepare_training_data(start_week, end_week, max_users, sequence_length)
        
        # è°ƒè¯•æ‰“å°ï¼šæ£€æŸ¥ users åˆ—è¡¨å’Œ user_to_index å­—å…¸çš„ä¸€è‡´æ€§
        # è¿™éƒ¨åˆ†è°ƒè¯•é€»è¾‘ç°åœ¨åº”è¯¥åœ¨ prepare_training_data è¿”å›ä¹‹å‰ï¼Œæˆ–è€…åœ¨ _integrate_multimodal_data å†…éƒ¨è¿›è¡Œ
        # ä½†ç”±äº _integrate_multimodal_data ç°åœ¨æ¥æ”¶ final_user_list, ä¸€è‡´æ€§åº”å¾—åˆ°ä¿è¯
        print("\nDEBUG: Checking user consistency in run_full_multimodal_pipeline (after prepare_training_data)")
        users_in_list = set(training_data.get('users', []))
        users_in_map = set(training_data.get('user_to_index', {}).keys())
        
        if users_in_list == users_in_map:
            print("DEBUG: Users in list and user_to_index map are CONSISTENT.")
        else:
            print("DEBUG: Users in list and user_to_index map are INCONSISTENT.")
            print(f"  Users ONLY in list: {users_in_list - users_in_map}")
            print(f"  Users ONLY in map: {users_in_map - users_in_list}")
            print(f"  Number of users in list: {len(users_in_list)}")
            print(f"  Number of users in map: {len(users_in_map)}")
            # è¿™å¯èƒ½æŒ‡ç¤ºäº†ä¸Šæ¸¸æ•°æ®å¤„ç†ä¸­ç”¨æˆ·é›†åˆä¸åŒ¹é…çš„é—®é¢˜

        total_time = time.time() - start_time
        
        print(f"\n{'='*80}")
        print(f"å¤šæ¨¡æ€æ•°æ®å¤„ç†æµæ°´çº¿å®Œæˆï¼")
        print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"æ•°æ®æ ·æœ¬æ•°: {len(training_data['labels'])}")
        print(f"æ­£å¸¸æ ·æœ¬: {np.sum(training_data['labels'] == 0)}")
        print(f"å¼‚å¸¸æ ·æœ¬: {np.sum(training_data['labels'] == 1)}")
        print(f"{'='*80}")
        
        return training_data

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆ›å»ºå¤šæ¨¡æ€æ•°æ®æµæ°´çº¿
    pipeline = MultiModalDataPipeline(
        data_version='r4.2',
        feature_dim=256,
        num_cores=8
    )
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    training_data = pipeline.run_full_multimodal_pipeline(
        start_week=0,
        end_week=5,  # æµ‹è¯•å‰5å‘¨
        max_users=100,  # æµ‹è¯•100ä¸ªç”¨æˆ·
        sequence_length=128
    )
    
    print("å¤šæ¨¡æ€æ•°æ®å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main() 