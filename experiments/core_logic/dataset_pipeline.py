#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„CERTæ•°æ®é›†ç‰¹å¾æå–æµæ°´çº¿
åŸºäºfeature_extraction_scenarioæ¨¡å—åŒ–ç³»ç»Ÿå®ç°å‘¨çº§åˆ«ã€æ—¥çº§åˆ«å’Œä¼šè¯çº§åˆ«çš„åˆ†æ
"""

import os
import sys
import time
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import multiprocessing as mp
from typing import Dict, List, Tuple, Optional
import pickle
import warnings
from joblib import Parallel, delayed
import dask.dataframe as dd
import shutil # Added for rmtree
from dask.distributed import Client, LocalCluster
warnings.filterwarnings('ignore')

# å¯¼å…¥æ–°çš„æ¨¡å—åŒ–ç³»ç»Ÿ
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from .encoder import EventEncoder
    from .utils import FeatureEncoder
    from .temporal import encode_session_temporal_features
    from .user_context import encode_behavioral_risk_profile
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    
    from encoder import EventEncoder
    from utils import FeatureEncoder
    from temporal import encode_session_temporal_features
    from user_context import encode_behavioral_risk_profile

class CERTDatasetPipeline:
    """
    CERTæ•°æ®é›†å®Œæ•´ç‰¹å¾æå–æµæ°´çº¿
    
    å®ç°åŠŸèƒ½ï¼š
    1. åŸå§‹æ—¥å¿—æ•°æ®çš„æŒ‰å‘¨åˆå¹¶
    2. ç”¨æˆ·ä¿¡æ¯å’Œæ¶æ„ç”¨æˆ·æ ‡è®°çš„æå–
    3. æ´»åŠ¨æ•°æ®çš„æ•°å€¼åŒ–ç‰¹å¾æå–
    4. å¤šç²’åº¦ç‰¹å¾çš„ç»Ÿè®¡è®¡ç®—å’ŒCSVå¯¼å‡º
    """
    
    def __init__(self, data_version: str = 'r4.2', feature_dim: int = 256, num_cores: int = 8,
                 source_dir_override: Optional[str] = None, 
                 work_dir_override: Optional[str] = None,
                 seed: int = 42): # æ·»åŠ  seed å‚æ•°
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            data_version: æ•°æ®é›†ç‰ˆæœ¬
            feature_dim: ç‰¹å¾å‘é‡ç»´åº¦
            num_cores: CPUæ ¸å¿ƒæ•°
            source_dir_override: (å¯é€‰) è¦†ç›–æºæ•°æ®ç›®å½•è·¯å¾„ï¼Œç”¨äºæµ‹è¯•
            work_dir_override: (å¯é€‰) è¦†ç›–å·¥ä½œç›®å½•è·¯å¾„ï¼Œç”¨äºæµ‹è¯•
            seed: éšæœºç§å­
        """
        self.data_version = data_version
        self.feature_dim = feature_dim
        self.num_cores = num_cores
        self.seed = seed # å­˜å‚¨ç§å­
        
        # è·¯å¾„é…ç½®
        if source_dir_override:
            self.source_data_dir = os.path.abspath(source_dir_override) # ç¡®ä¿ç»å¯¹è·¯å¾„
            print(f"âš ï¸  ä½¿ç”¨è¦†ç›–çš„æºæ•°æ®ç›®å½•: {self.source_data_dir}")
        else:
            # åŠ¨æ€è®¡ç®—æ­£ç¡®çš„æ•°æ®ç›®å½•è·¯å¾„
            current_script_dir = os.path.dirname(os.path.abspath(__file__)) # core_logic
            core_logic_parent_dir = os.path.dirname(current_script_dir) # experiments
            project_root_dir = os.path.dirname(core_logic_parent_dir) # InProject
            self.source_data_dir = os.path.abspath(os.path.join(project_root_dir, 'data', data_version)) # ç¡®ä¿ç»å¯¹è·¯å¾„
        
        if work_dir_override:
            self.work_dir = os.path.abspath(work_dir_override) # ç¡®ä¿ç»å¯¹è·¯å¾„
            print(f"âš ï¸  ä½¿ç”¨è¦†ç›–çš„å·¥ä½œç›®å½•: {os.path.abspath(self.work_dir)}")
        else:
            # å·¥ä½œç›®å½•è®¾ç½®ä¸º InProject ç›®å½•
            current_script_dir = os.path.dirname(os.path.abspath(__file__)) # core_logic
            core_logic_parent_dir = os.path.dirname(current_script_dir) # experiments
            project_root_dir = os.path.dirname(core_logic_parent_dir) # InProject
            self.work_dir = os.path.abspath(project_root_dir) # ç¡®ä¿ç»å¯¹è·¯å¾„
        
        # ç¡®å®šæ•°æ®é›†æ€»å‘¨æ•°
        self.max_weeks = 73 if data_version in ['r4.1', 'r4.2'] else 75
        
        # åˆå§‹åŒ–ç¼–ç å™¨
        self.encoder = EventEncoder(feature_dim=feature_dim, data_version=data_version)
        
        # å½“å‰çš„é‡‡æ ·ç‡å’Œå¯¹åº”çš„ Parquet ç›®å½•å
        self.current_sample_ratio = None # ä¼šåœ¨ run_full_pipeline ä¸­è®¾ç½®
        self.current_parquet_dir_name = "DataByWeek_parquet" # é»˜è®¤å€¼

        # åˆ›å»ºå¿…è¦ç›®å½•
        self._create_directories()
        
        print(f"åˆå§‹åŒ–CERTæ•°æ®é›†æµæ°´çº¿")
        print(f"  æ•°æ®ç‰ˆæœ¬: {data_version}")
        print(f"  æºæ•°æ®ç›®å½•: {os.path.abspath(self.source_data_dir)}")
        print(f"  å·¥ä½œç›®å½•: {os.path.abspath(self.work_dir)}")
        print(f"  æœ€å¤§å‘¨æ•°: {self.max_weeks}")
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            "tmp", "ExtractedData", "DataByWeek", "NumDataByWeek",
            "WeekLevelFeatures", "DayLevelFeatures", "SessionLevelFeatures"
        ]
        for directory in directories:
            work_path = os.path.join(self.work_dir, directory)
            os.makedirs(work_path, exist_ok=True)
    
    def _get_source_file_path(self, filename: str) -> str:
        """è·å–æºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
        return os.path.join(self.source_data_dir, filename)
    
    def _get_work_file_path(self, filename: str) -> str:
        """è·å–å·¥ä½œæ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
        return os.path.join(self.work_dir, filename)
    
    def _read_csv_file(self, event_type: str, filename: str, file_path: str, file_type: str, sample_ratio: float = None):
        """
        ä¼˜åŒ–çš„CSVæ–‡ä»¶è¯»å–æ–¹æ³•
        
        Args:
            event_type: äº‹ä»¶ç±»å‹
            filename: æ–‡ä»¶å
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹æè¿°
            sample_ratio: æ•°æ®é‡‡æ ·æ¯”ä¾‹ (0-1)
            
        Returns:
            tuple: (event_type, dataframe) æˆ– None
        """
        import time
        start_time = time.time()
        
        print(f"   è¯»å– {filename} ({file_type})...")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ•°æ®æ–‡ä»¶
        if file_type == "æµ‹è¯•æ•°æ®":
            try:
                with open(file_path, 'r') as f:
                    first_line = f.readline()
                    if 'TEST_DATA_CREATED_BY_PIPELINE' in first_line:
                        print(f"     âš ï¸  ä½¿ç”¨æµ‹è¯•æ•°æ®: {filename}")
            except:
                pass
        
        try:
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
            print(f"     æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
            
            # ä¼˜åŒ–çš„CSVè¯»å–å‚æ•°
            csv_params = {
                'low_memory': False,  # é¿å…æ•°æ®ç±»å‹æ¨æ–­é—®é¢˜
                'engine': 'c',       # ä½¿ç”¨Cå¼•æ“ï¼Œæ›´å¿«
            }
            
            # æ ¹æ®æ–‡ä»¶å¤§å°å’Œé‡‡æ ·æ¯”ä¾‹é€‰æ‹©è¯»å–ç­–ç•¥
            if sample_ratio and sample_ratio < 1.0:
                print(f"     é‡‡æ ·æ¨¡å¼ (åˆ†å—): è¯»å– {sample_ratio*100:.1f}% çš„æ•°æ®")
                chunk_size = 50000  # æ¯æ¬¡è¯»å–5ä¸‡è¡Œï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                sampled_chunks = []
                
                try:
                    # è·å–åˆ—åï¼Œç”¨äºç©ºDataFrameçš„åˆ›å»º
                    header_df = pd.read_csv(file_path, nrows=0, **csv_params)
                    column_names = header_df.columns
                    
                    for chunk in pd.read_csv(file_path, chunksize=chunk_size, **csv_params):
                        # å¯¹æ¯ä¸ªå—è¿›è¡Œé‡‡æ ·
                        # frac=sample_ratio æ›´ç¬¦åˆæ¯”ä¾‹é‡‡æ ·æ„å›¾
                        # random_state ä¿è¯å¯å¤ç°æ€§
                        sampled_chunk = chunk.sample(frac=sample_ratio, random_state=42, replace=False)
                        if not sampled_chunk.empty:
                            sampled_chunks.append(sampled_chunk)
                    
                    if sampled_chunks:
                        df = pd.concat(sampled_chunks, ignore_index=True)
                    else:
                        # å¦‚æœæ²¡æœ‰é‡‡æ ·åˆ°ä»»ä½•æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªå…·æœ‰æ­£ç¡®åˆ—åçš„ç©ºDataFrame
                        df = pd.DataFrame(columns=column_names)
                    print(f"     åˆ†å—é‡‡æ ·è¯»å–å®Œæˆï¼Œæ€»è¡Œæ•°: {len(df)}")
                
                except pd.errors.EmptyDataError:
                    print(f"     âš ï¸ æ–‡ä»¶ {filename} ä¸ºç©ºæˆ–åœ¨åˆ†å—é‡‡æ ·è¿‡ç¨‹ä¸­æœªè¯»å–åˆ°æ•°æ®ã€‚")
                    # å°è¯•è·å–åˆ—åï¼Œå¦‚æœæ–‡ä»¶å°±æ˜¯ç©ºçš„ï¼Œè¿™ä¹Ÿå¯èƒ½å¤±è´¥
                    try:
                        header_df = pd.read_csv(file_path, nrows=0, **csv_params)
                        column_names = header_df.columns
                    except pd.errors.EmptyDataError:
                        column_names = [] # æ— æ³•è·å–åˆ—å
                    df = pd.DataFrame(columns=column_names)
                except Exception as e:
                    print(f"     âŒ åˆ†å—é‡‡æ ·è¯»å– {filename} å¤±è´¥: {e}")
                    return None # å‡ºç°å…¶ä»–é”™è¯¯åˆ™è¿”å›None

            elif file_size > 500:  # å¤§äº500MBçš„æ–‡ä»¶
                print(f"     å¤§æ–‡ä»¶æ£€æµ‹ï¼Œä½¿ç”¨åˆ†å—è¯»å–...")
                chunk_size = 50000  # æ¯æ¬¡è¯»å–5ä¸‡è¡Œ
                chunks = []
                
                for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, **csv_params)):
                    chunks.append(chunk)
                    if (i + 1) % 10 == 0:  # æ¯10ä¸ªchunkæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        print(f"     å·²è¯»å– {(i + 1) * chunk_size} è¡Œ...")
                
                df = pd.concat(chunks, ignore_index=True)
                print(f"     åˆ†å—è¯»å–å®Œæˆï¼Œæ€»è¡Œæ•°: {len(df)}")
            else:
                # å°æ–‡ä»¶ç›´æ¥è¯»å–
                df = pd.read_csv(file_path, **csv_params)
            
            # æ·»åŠ äº‹ä»¶ç±»å‹æ ‡è¯†
            df['type'] = event_type
            
            # ä¼˜åŒ–æ—¥æœŸè½¬æ¢
            if 'date' in df.columns:
                print(f"     è½¬æ¢æ—¥æœŸåˆ—...")
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            elapsed_time = time.time() - start_time
            print(f"     âœ… {filename} è¯»å–å®Œæˆ: {len(df)} è¡Œ, è€—æ—¶ {elapsed_time:.1f} ç§’")
            
            return (event_type, df)
            
        except Exception as e:
            print(f"     âŒ è¯»å– {filename} å¤±è´¥: {e}")
            return None
    
    def step1_combine_raw_data(self, start_week: int = 0, end_week: int = None, sample_ratio: float = None, force_regenerate: bool = False):
        """
        Step 1: æŒ‰å‘¨åˆå¹¶åŸå§‹æ•°æ®
        
        Args:
            start_week: å¼€å§‹å‘¨æ•°
            end_week: ç»“æŸå‘¨æ•°
            sample_ratio: æ•°æ®é‡‡æ ·æ¯”ä¾‹ (0-1)ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
            force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆå‘¨æ•°æ® Parquet æ–‡ä»¶é›†
        """
        try:
            # Try to connect to an existing Dask client/cluster if available
            # This is useful if the script is run within a larger Dask setup.
            # If no existing client, LocalCluster will be created.
            client = Client(timeout="2s", processes=False) # processes=False for LocalCluster to use threads for quicker startup
            print(f"ğŸ›ï¸  Connected to existing Dask client: {client}")
            # dashboard_link = client.dashboard_link
            # if dashboard_link:
            # print(f"Dask Dashboard: {dashboard_link}")
        except (OSError, TimeoutError):
            print(" baÅŸlatÄ±lÄ±yor Dask LocalCluster... (No existing Dask client found or connection timed out)")
            # Fallback to creating a new LocalCluster
            # Using threads for workers can be faster for I/O bound tasks like CSV reading
            # and avoids some of the overhead of multiprocessing on a single machine.
            # Adjust n_workers and threads_per_worker based on your machine's cores.
            # e.g., if you have 8 cores, you might use n_workers=4, threads_per_worker=2
            # or n_workers=self.num_cores (if defined and appropriate)
            cluster = LocalCluster(n_workers=self.num_cores, threads_per_worker=1, memory_limit='auto') #memory_limit can be adjusted
            client = Client(cluster)
            print(f"ğŸ›ï¸  New Dask LocalCluster started: {cluster}")
        
        # Always print the dashboard link
        if client and hasattr(client, 'dashboard_link') and client.dashboard_link:
            print(f"ğŸ”— Dask Dashboard: {client.dashboard_link}")
        else:
            print("âš ï¸  Dask Dashboard link not available.")

        if end_week is None:
            end_week = self.max_weeks
            
        print(f"\n{'='*60}")
        print(f"Step 1: æŒ‰å‘¨åˆå¹¶åŸå§‹æ•°æ® (Daskæ¨¡å¼, è¾“å‡º Parquet) (å‘¨ {start_week} åˆ° {end_week-1})")
        print(f"{'='*60}")

        # ä½¿ç”¨ self.current_parquet_dir_name ä½œä¸ºç›®æ ‡ç›®å½•
        parquet_output_dir = self._get_work_file_path(self.current_parquet_dir_name)
        os.makedirs(parquet_output_dir, exist_ok=True) # ç¡®ä¿ç›®å½•å­˜åœ¨
        print(f"   ç›®æ ‡Parquetç›®å½•: {parquet_output_dir}")

        # é‡æ–°ç”Ÿæˆé€»è¾‘è°ƒæ•´ï¼š
        # 1. å¦‚æœæ˜¯é‡‡æ ·æ•°æ® (self.current_sample_ratio < 1.0)ï¼Œå¹¶ä¸”ç›®å½•å·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡ (ä¸çœ‹ force_regenerate)ã€‚
        #    é™¤é force_regenerate ä¹Ÿä¸ºTrueï¼Œæ­¤æ—¶é‡‡æ ·æ•°æ®ä¹Ÿä¼šè¢«å¼ºåˆ¶é‡æ–°ç”Ÿæˆã€‚
        # 2. å¦‚æœæ˜¯å…¨é‡æ•°æ® (self.current_sample_ratio is None or == 1.0)ï¼Œåˆ™éµå¾ª force_regenerate æ ‡å¿—ã€‚
        should_skip_generation = False
        is_sampled_data = self.current_sample_ratio is not None and 0 < self.current_sample_ratio < 1.0

        is_parquet_dir_valid = False
        if os.path.exists(parquet_output_dir) and os.path.isdir(parquet_output_dir):
            if (os.path.exists(os.path.join(parquet_output_dir, "_metadata")) or
                os.path.exists(os.path.join(parquet_output_dir, "_common_metadata")) or
                any(fname.startswith("week=") for fname in os.listdir(parquet_output_dir))):
                is_parquet_dir_valid = True

        if is_parquet_dir_valid: # ç›®å½•å­˜åœ¨ä¸”æ˜¯æœ‰æ•ˆçš„Parquetç›®å½•
            if is_sampled_data and not force_regenerate: # æƒ…å†µ1: é‡‡æ ·æ•°æ®å·²å­˜åœ¨ï¼Œä¸”æœªå¼ºåˆ¶é‡æ–°ç”Ÿæˆ
                should_skip_generation = True
                print(f"   âœ… é‡‡æ ·æ•°æ®Parquetç›®å½• {parquet_output_dir} å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡ç”Ÿæˆã€‚")
            elif not is_sampled_data and not force_regenerate: # æƒ…å†µ2: å…¨é‡æ•°æ®å·²å­˜åœ¨ï¼Œä¸”æœªå¼ºåˆ¶é‡æ–°ç”Ÿæˆ
                should_skip_generation = True
                print(f"   âœ… å…¨é‡æ•°æ®Parquetç›®å½• {parquet_output_dir} å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡ç”Ÿæˆã€‚")
            elif force_regenerate: # ä»»ä½•æƒ…å†µä¸‹ï¼Œåªè¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œå°±éœ€è¦åˆ é™¤ç°æœ‰ç›®å½•
                print(f"   âš ï¸  é…ç½®äº†å¼ºåˆ¶é‡æ–°åˆå¹¶æ•°æ® (force_regenerate=True)ï¼Œå°†åˆ é™¤ç°æœ‰Parquetç›®å½•: {parquet_output_dir}...")
                try:
                    shutil.rmtree(parquet_output_dir)
                    print(f"     å·²åˆ é™¤: {parquet_output_dir}")
                    is_parquet_dir_valid = False # åˆ é™¤åä¸å†æœ‰æ•ˆ
                except OSError as e:
                    print(f"     âŒ åˆ é™¤ {parquet_output_dir} å¤±è´¥: {e}")
                    # å¦‚æœåˆ é™¤å¤±è´¥ï¼Œå¯èƒ½ä¸åº”è¯¥ç»§ç»­ï¼Œæˆ–è€…æ ¹æ®æƒ…å†µå†³å®šæ˜¯å¦è·³è¿‡
                    # ä¸ºå®‰å…¨èµ·è§ï¼Œå¦‚æœåˆ é™¤å¤±è´¥ä¸”ç›®å½•ä»ç„¶æ˜¯æœ‰æ•ˆçš„ï¼Œåˆ™è·³è¿‡ä»¥é¿å…å†™å…¥ä¸å®Œæ•´æ•°æ®
                    if is_parquet_dir_valid:
                        print(f"     ç”±äºåˆ é™¤å¤±è´¥ä¸”ç›®å½•ä»æœ‰æ•ˆï¼Œå°†è·³è¿‡ç”Ÿæˆä»¥é¿å…æ½œåœ¨é—®é¢˜ã€‚")
                        should_skip_generation = True 
            # å¦‚æœæ˜¯é‡‡æ ·æ•°æ®ï¼Œä½† force_regenerate ä¸ºTrueï¼Œåˆ™ä¸ä¼šè·³è¿‡ï¼Œä¼šç»§ç»­æ‰§è¡Œåˆ é™¤å’Œé‡æ–°ç”Ÿæˆ
            # å¦‚æœæ˜¯å…¨é‡æ•°æ®ï¼Œforce_regenerate ä¸ºTrueï¼Œä¹Ÿä¼šæ‰§è¡Œåˆ é™¤å’Œé‡æ–°ç”Ÿæˆ
        
        if should_skip_generation:
            return # ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œåç»­çš„æ•°æ®è¯»å†™
        
        # å¦‚æœ is_parquet_dir_valid ä¸º Falseï¼ˆç›®å½•ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤ï¼‰ï¼Œåˆ™éœ€è¦åˆ›å»º
        if not is_parquet_dir_valid:
            os.makedirs(parquet_output_dir, exist_ok=True) # å†æ¬¡ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä»¥é˜²è¢«åˆ é™¤
            print(f"   åˆ›å»ºæ–°çš„Parquetç›®å½•: {parquet_output_dir}")

        raw_files = {
            'http': 'http.csv',
            'email': 'email.csv', 
            'file': 'file.csv',
            'logon': 'logon.csv',
            'device': 'device.csv'
        }
        
        missing_files = []
        for filename in raw_files.values():
            source_path = self._get_source_file_path(filename)
            if not os.path.exists(source_path):
                missing_files.append(filename)
        
        if missing_files:
            print(f"âš ï¸  ç¼ºå¤±æºæ•°æ®æ–‡ä»¶: {missing_files}")
            print(f"   æŸ¥æ‰¾ä½ç½®: {os.path.abspath(self.source_data_dir)}")
            local_missing = []
            for filename in missing_files:
                local_path = self._get_work_file_path(filename)
                if not os.path.exists(local_path):
                    local_missing.append(filename)
            
            if local_missing:
                raise FileNotFoundError(f"åœ¨æºç›®å½•å’Œå·¥ä½œç›®å½•éƒ½æœªæ‰¾åˆ°: {local_missing}")
            else:
                print(f"   åœ¨å·¥ä½œç›®å½•æ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æµ‹è¯•æ•°æ®")

        print("ğŸ“ è¯»å–åŸå§‹æ•°æ®æ–‡ä»¶ (ä½¿ç”¨ Dask)...")
        dask_dfs = []

        for event_type, filename in raw_files.items():
            source_path = self._get_source_file_path(filename)
            work_path = self._get_work_file_path(filename)
            actual_file_path = None
            file_kind = ""

            if os.path.exists(source_path):
                actual_file_path = source_path
                file_kind = "æºæ•°æ®"
            elif os.path.exists(work_path):
                actual_file_path = work_path
                file_kind = "æµ‹è¯•æ•°æ®"
            
            if actual_file_path:
                print(f"   è®¡åˆ’è¯»å– {filename} ({file_kind}) ä½¿ç”¨Dask...")
                # Dask read_csv, blocksizeå¯ä»¥è°ƒæ•´ä»¥å¹³è¡¡å†…å­˜å’Œå¹¶è¡Œåº¦
                # å¯¹äºéå¸¸å¤§çš„æ–‡ä»¶ï¼Œå¯ä»¥å‡å° blocksizeï¼Œä¾‹å¦‚ "32MB"
                # low_memory=False ç±»ä¼¼ pandas, engine='c' é€šå¸¸ä¸éœ€è¦æ˜¾å¼æŒ‡å®šç»™ dask
                try:
                    ddf = dd.read_csv(actual_file_path, low_memory=False, blocksize="64MB")
                    # å¦‚æœæŒ‡å®šäº† sample_ratio < 1.0, Daskä¹Ÿæ”¯æŒé‡‡æ ·
                    if sample_ratio and sample_ratio < 1.0:
                        print(f"     Dask é‡‡æ ·æ¨¡å¼: è¯»å– {sample_ratio*100:.1f}% çš„æ•°æ®")
                        ddf = ddf.sample(frac=sample_ratio, random_state=42)

                    ddf = ddf.assign(type=event_type)
                    if 'date' in ddf.columns:
                         # Daskçš„to_datetimeå¯èƒ½éœ€è¦metaä¿¡æ¯æˆ–æ˜¾å¼è®¡ç®—æ¥ç¡®å®šç±»å‹
                         # ä¸€ä¸ªç®€å•çš„æ–¹å¼æ˜¯å…ˆè®¡ç®—è¿™åˆ—ï¼Œæˆ–è€…æä¾›meta
                        ddf['date'] = dd.to_datetime(ddf['date'], errors='coerce')
                    dask_dfs.append(ddf)
                    print(f"     âœ… {filename} å·²åŠ å…¥Daskå¤„ç†é˜Ÿåˆ—ã€‚")
                except Exception as e:
                    print(f"     âŒ Dask è¯»å– {filename} å¤±è´¥: {e}")
            else:
                print(f"   âš ï¸  æ–‡ä»¶ {filename} æœªæ‰¾åˆ°ï¼Œè·³è¿‡ã€‚")

        if not dask_dfs:
            print("âŒ æ²¡æœ‰å¯å¤„ç†çš„Dask DataFramesï¼Œä¸­æ­¢åˆå¹¶æ­¥éª¤ã€‚")
            return

        print("ğŸ”— ä½¿ç”¨ Dask åˆå¹¶æ‰€æœ‰äº‹ä»¶æ•°æ®...")
        combined_ddf = dd.concat(dask_dfs, ignore_index=True, interleave_partitions=True)
        
        # Repartition to consolidate potentially many small partitions after sampling and concat
        # This can make subsequent sort and set_index more efficient.
        # Aim for partitions of a reasonable size, e.g., 64MB-128MB.
        # The actual number of partitions will be data_size / partition_size.
        print(f"âš™ï¸ Repartitioning Dask DataFrame to optimal partition size (e.g., 128MB)...")
        combined_ddf = combined_ddf.repartition(partition_size="128MB")

        print("âš™ï¸ ä½¿ç”¨ Dask è®¡ç®—æ—¥æœŸèŒƒå›´å’Œå‘¨æ•°...")
        # Daskéœ€è¦ .compute() æ¥è·å–æ ‡é‡ç»“æœ
        min_date_series = combined_ddf['date'].min()
        if isinstance(min_date_series, dd.Series) or isinstance(min_date_series, dd.core.Scalar):
             base_date = min_date_series.compute()
        else: # å¦‚æœå·²ç»æ˜¯è®¡ç®—å¥½çš„å€¼ (ä¸å¤ªå¯èƒ½åœ¨æ­¤æµç¨‹ä¸­ï¼Œä½†ä½œä¸ºä¿é™©)
             base_date = min_date_series
        
        combined_ddf['week'] = ((combined_ddf['date'] - base_date).dt.days // 7).astype(int)
        
        # ç§»é™¤æ˜‚è´µçš„å…¨å±€æ’åºæ“ä½œï¼Œå› ä¸ºåŸå§‹CSVæ–‡ä»¶å·²ç»æŒ‰æ—¶é—´æ’åº
        # print("âš™ï¸ ä½¿ç”¨ Dask æ˜¾å¼æŒ‰ 'date' åˆ—æ’åºæ•°æ® (è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´)...")
        # combined_ddf = combined_ddf.sort_values('date')

        # ç§»é™¤äº† set_index('date') æ“ä½œï¼Œå› ä¸ºå®ƒä¹Ÿæ˜¯æ€§èƒ½ç“¶é¢ˆ
        # print("âš™ï¸ å°† 'date' åˆ—è®¾ç½®ä¸º Dask DataFrame çš„ç´¢å¼• (Dask å°†è®¡ç®—æˆ–è°ƒæ•´åˆ†åŒº)...") # Removing set_index for now
        # combined_ddf = combined_ddf.set_index('date') 

        total_events_computed = combined_ddf.shape[0].compute() 
        min_week_computed = combined_ddf['week'].min().compute()
        max_week_computed = combined_ddf['week'].max().compute()
        print(f"ğŸ“Š Dask å¤„ç†åæ€»äº‹ä»¶æ•°: {total_events_computed}, å‘¨æ•°èŒƒå›´: {min_week_computed} - {max_week_computed}")
        
        print(f"ğŸ’¾ å°†Dask DataFrameç›´æ¥ä¿å­˜ä¸ºæŒ‰å‘¨åˆ†åŒºçš„Parquetæ–‡ä»¶é›†åˆ°: {parquet_output_dir}")
        print("   âš¡ è·³è¿‡å…¨å±€æ’åºï¼Œåˆ©ç”¨åŸå§‹CSVæ–‡ä»¶å·²æŒ‰æ—¶é—´æ’åºçš„ç‰¹æ€§")
        try:
            # Ensure 'week' column exists for partitioning
            if 'week' not in combined_ddf.columns:
                raise ValueError("The 'week' column is missing from combined_ddf and is required for partitioning.")

            combined_ddf.to_parquet(
                parquet_output_dir,
                partition_on=['week'],
                engine='pyarrow', # or 'fastparquet' if preferred and installed
                # schema='infer', # schema can be inferred, or explicitly provided for large datasets
                write_index=False # We removed set_index('date'), so no index to write
            )
            print(f"   âœ… Dask DataFrameæˆåŠŸä¿å­˜åˆ°Parquetç›®å½•: {parquet_output_dir}")
        except Exception as e:
            print(f"   âŒ ä¿å­˜Dask DataFrameåˆ°Parquetå¤±è´¥: {e}")
            # Potentially re-raise or handle more gracefully if this is critical
            raise

        # The old loop for saving individual pickle files is now replaced by to_parquet
        
        print("âœ… Step 1 (Daskæ¨¡å¼, Parquetè¾“å‡º, æ— å…¨å±€æ’åº) å®Œæˆ")
    
    def step2_load_user_data(self):
        """
        Step 2: åŠ è½½ç”¨æˆ·ä¿¡æ¯å’Œæ¶æ„ç”¨æˆ·æ ‡è®°
        
        Returns:
            users_df: ç”¨æˆ·ä¿¡æ¯DataFrame
        """
        print(f"\n{'='*60}")
        print("Step 2: åŠ è½½ç”¨æˆ·ä¿¡æ¯å’Œæ¶æ„ç”¨æˆ·æ ‡è®°")
        print(f"{'='*60}")
        
        # æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®ï¼ˆå®é™…åº”ä»LDAP/psychometric.csvç­‰æ–‡ä»¶è¯»å–ï¼‰
        users_data = self._load_or_create_user_data()
        
        # åŠ è½½æ¶æ„ç”¨æˆ·æ ‡è®°ï¼ˆä»answersç›®å½•ï¼‰
        malicious_users = self._load_malicious_user_labels()
        
        # åˆå¹¶ç”¨æˆ·æ•°æ®
        users_df = pd.DataFrame(users_data).set_index('user_id')
        
        # æ ‡è®°æ¶æ„ç”¨æˆ·
        for user_id, mal_info in malicious_users.items():
            if user_id in users_df.index:
                users_df.loc[user_id, 'malscene'] = mal_info['scenario']
                users_df.loc[user_id, 'mstart'] = mal_info['start_week']
                users_df.loc[user_id, 'mend'] = mal_info['end_week']
        
        print(f"ğŸ“Š æ€»ç”¨æˆ·æ•°: {len(users_df)}")
        print(f"ğŸ“Š æ¶æ„ç”¨æˆ·æ•°: {len(users_df[users_df['malscene'] > 0])}")
        
        return users_df
    
    def _load_or_create_user_data(self):
        """åŠ è½½æˆ–åˆ›å»ºç”¨æˆ·æ•°æ®"""
        # å°è¯•ä»æºç›®å½•LDAPå­ç›®å½•è¯»å–
        source_ldap_dir = os.path.join(self.source_data_dir, 'LDAP')
        ldap_files = []
        if os.path.exists(source_ldap_dir):
            ldap_files = [f for f in os.listdir(source_ldap_dir) if f.endswith('.csv')]
            print(f"ğŸ“ å‘ç°æºLDAPæ–‡ä»¶: {len(ldap_files)} ä¸ªæ–‡ä»¶")
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„LDAPæ–‡ä»¶è¯»å–é€»è¾‘
            # ç°åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œä½†ä¿ç•™å°†æ¥æ‰©å±•çš„å¯èƒ½æ€§
        
        # å°è¯•ä»æºç›®å½•è¯»å–å¿ƒç†æµ‹é‡æ•°æ®
        source_psychometric = self._get_source_file_path('psychometric.csv')
        work_psychometric = self._get_work_file_path('psychometric.csv')
        
        ocean_scores = {}
        psychometric_file = None
        
        if os.path.exists(source_psychometric):
            psychometric_file = source_psychometric
            print(f"ğŸ“ è¯»å–æºå¿ƒç†æµ‹é‡æ•°æ®: psychometric.csv")
        elif os.path.exists(work_psychometric):
            psychometric_file = work_psychometric
            print(f"ğŸ“ è¯»å–å·¥ä½œç›®å½•å¿ƒç†æµ‹é‡æ•°æ®: psychometric.csv (æµ‹è¯•æ•°æ®)")
        
        if psychometric_file:
            try:
                psycho_df = pd.read_csv(psychometric_file)
                print(f"   å¿ƒç†æµ‹é‡æ–‡ä»¶åˆ—å: {list(psycho_df.columns)}")
                
                # ç¡®å®šç”¨æˆ·IDåˆ—å
                user_id_col = None
                for col in ['user_id', 'user', 'employee_name']:
                    if col in psycho_df.columns:
                        user_id_col = col
                        break
                
                if user_id_col is None:
                    print("âš ï¸  å¿ƒç†æµ‹é‡æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç”¨æˆ·IDåˆ—")
                else:
                    ocean_cols = ['O', 'C', 'E', 'A', 'N']
                    available_cols = [col for col in ocean_cols if col in psycho_df.columns]
                    if available_cols:
                        ocean_scores = psycho_df.set_index(user_id_col)[available_cols].to_dict('index')
                        print(f"   åŠ è½½ {len(ocean_scores)} ä¸ªç”¨æˆ·çš„OCEANç‰¹å¾ (ä½¿ç”¨åˆ—: {user_id_col})")
                    else:
                        print("âš ï¸  å¿ƒç†æµ‹é‡æ–‡ä»¶ä¸­æœªæ‰¾åˆ°OCEANç‰¹å¾åˆ—")
            except Exception as e:
                print(f"âš ï¸  è¯»å–å¿ƒç†æµ‹é‡æ•°æ®å¤±è´¥: {e}")
        
        # è·å–æ‰€æœ‰ç”¨æˆ·ï¼ˆä»æ´»åŠ¨æ•°æ®ä¸­æå–ï¼‰
        all_users = set()
        parquet_dir = self._get_work_file_path(self.current_parquet_dir_name)

        if not (os.path.exists(parquet_dir) and os.path.isdir(parquet_dir)):
            print(f"âš ï¸  Parquetæ•°æ®ç›®å½• {parquet_dir} ä¸å­˜åœ¨ï¼Œæ— æ³•æå–ç”¨æˆ·åˆ—è¡¨ã€‚å°†å°è¯•ä»æ—§pickleæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–ç”Ÿæˆç©ºç”¨æˆ·åˆ—è¡¨ã€‚")
            # Fallback or error, for now, let's try to see if old pickle files exist as a super fallback
            # This fallback should ideally be removed once Parquet is stable.
            for week in range(self.max_weeks):
                 week_file_pickle = self._get_work_file_path(f"DataByWeek/{week}.pickle")
                 if os.path.exists(week_file_pickle):
                     try:
                         week_data_pickle = pd.read_pickle(week_file_pickle)
                         if 'user' in week_data_pickle.columns:
                             all_users.update(week_data_pickle['user'].unique())
                     except Exception as e_pickle:
                         print(f"  å‘¨ {week}: è¯»å–æ—§pickleæ–‡ä»¶ {week_file_pickle} å¤±è´¥: {e_pickle}")
            if not all_users:
                 print("âš ï¸  æ— æ³•ä»Parquetæˆ–æ—§pickleæ–‡ä»¶åŠ è½½ç”¨æˆ·ï¼Œå°†ç”Ÿæˆç©ºç”¨æˆ·åˆ—è¡¨æˆ–åŸºäºpsychometricï¼ˆå¦‚æœå­˜åœ¨ï¼‰")


        else: # Parquet directory exists
            print(f"   ä»Parquetç›®å½• {parquet_dir} æå–ç”¨æˆ·åˆ—è¡¨...")
            # Potentially list all "week=*" subdirectories to know which weeks have data
            # For simplicity, iterating up to self.max_weeks
            for week in range(self.max_weeks):
                try:
                    # Only read the 'user' column for efficiency
                    user_data_for_week_ddf = dd.read_parquet(
                        parquet_dir,
                        columns=['user'],
                        filters=[('week', '==', week)],
                        engine='pyarrow'
                    )
                    # Check if the ddf is empty before computing
                    if user_data_for_week_ddf.npartitions > 0:
                        # A more robust check for emptiness with Dask
                        is_empty = (user_data_for_week_ddf.map_partitions(len).compute().sum() == 0)
                        if not is_empty:
                            user_data_for_week_pd = user_data_for_week_ddf.compute()
                            if 'user' in user_data_for_week_pd.columns:
                                all_users.update(user_data_for_week_pd['user'].unique())
                        # else:
                            # print(f"  å‘¨ {week}: Parquetåˆ†åŒºä¸ºç©º.")
                except FileNotFoundError:
                     # This might happen if a specific week partition doesn't exist (e.g. week=X dir is missing)
                     # print(f"  å‘¨ {week}: Parquetæ•°æ®åˆ†åŒºæœªæ‰¾åˆ°, è·³è¿‡.")
                     pass 
                except Exception as e:
                    print(f"  å‘¨ {week}: ä»Parquetæå–ç”¨æˆ·æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    pass
        
        # åˆ›å»ºç”¨æˆ·æ•°æ®
        users_data = []
        roles = ['Employee', 'Manager', 'Director', 'Executive']
        departments = ['IT', 'Finance', 'HR', 'Marketing', 'Engineering', 'Operations']
        
        for user_id in sorted(all_users):
            if pd.isna(user_id):
                continue
                
            user_data = {
                'user_id': user_id,
                'role': np.random.choice(roles),
                'dept': np.random.choice(departments),
                'ITAdmin': np.random.choice([0, 1], p=[0.9, 0.1]),
                'pc_type': np.random.choice([0, 1, 2, 3], p=[0.7, 0.2, 0.05, 0.05]),
                'npc': np.random.randint(1, 4),
                'malscene': 0,  # é»˜è®¤éæ¶æ„
                'mstart': 0,
                'mend': 0
            }
            
            # æ·»åŠ OCEANç‰¹å¾
            if user_id in ocean_scores:
                user_data.update(ocean_scores[user_id])
            else:
                # éšæœºç”ŸæˆOCEANåˆ†æ•°
                user_data.update({
                    'O': np.random.uniform(0.2, 0.8),
                    'C': np.random.uniform(0.2, 0.8),
                    'E': np.random.uniform(0.2, 0.8),
                    'A': np.random.uniform(0.2, 0.8),
                    'N': np.random.uniform(0.2, 0.8)
                })
            
            users_data.append(user_data)
        
        return users_data
    
    def _load_malicious_user_labels(self):
        """åŠ è½½æ¶æ„ç”¨æˆ·æ ‡ç­¾"""
        ans_file = self._get_source_file_path("answers/insiders.csv")
        
        # æ›´è¯¦ç»†çš„è°ƒè¯•æ‰“å°
        print(f"[DEBUG CERTDatasetPipeline] Raw ans_file path: {ans_file}")
        abs_ans_file = os.path.abspath(ans_file)
        print(f"[DEBUG CERTDatasetPipeline] Absolute ans_file path: {abs_ans_file}")
        print(f"[DEBUG CERTDatasetPipeline] Is ans_file a file? {os.path.isfile(abs_ans_file)}")
        parent_dir_of_ans_file = os.path.dirname(abs_ans_file)
        print(f"[DEBUG CERTDatasetPipeline] Parent directory of ans_file: {parent_dir_of_ans_file}")
        print(f"[DEBUG CERTDatasetPipeline] Is parent directory a dir? {os.path.isdir(parent_dir_of_ans_file)}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(ans_file):
            print(f"âš ï¸  æ¶æ„ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ {ans_file} ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®.")
            return {}
        
        malicious_users = {}
        
        # å°è¯•ä»æºç›®å½•answerså­ç›®å½•è¯»å–
        source_answers_dir = os.path.join(self.source_data_dir, 'answers')
        work_answers_dir = self._get_work_file_path('answers')
        
        insiders_file = None
        
        # ä¼˜å…ˆä»æºç›®å½•è¯»å–
        source_insiders = os.path.join(source_answers_dir, 'insiders.csv')
        work_insiders = os.path.join(work_answers_dir, 'insiders.csv')
        
        if os.path.exists(source_insiders):
            insiders_file = source_insiders
            print(f"ğŸ“ è¯»å–æºå†…éƒ¨å¨èƒè€…åˆ—è¡¨: answers/insiders.csv")
        elif os.path.exists(work_insiders):
            insiders_file = work_insiders
            print(f"ğŸ“ è¯»å–å·¥ä½œç›®å½•å†…éƒ¨å¨èƒè€…åˆ—è¡¨: answers/insiders.csv (æµ‹è¯•æ•°æ®)")
        
        if insiders_file:
            try:
                insiders_df = pd.read_csv(insiders_file)
                print(f"   æ¶æ„ç”¨æˆ·æ–‡ä»¶åˆ—å: {list(insiders_df.columns)}")
                
                for _, row in insiders_df.iterrows():
                    # å¤„ç†æ—¶é—´åˆ—åçš„å˜åŒ–
                    start_week = 0
                    end_week = self.max_weeks
                    
                    # å°è¯•ä¸åŒçš„æ—¶é—´åˆ—å
                    if 'start_week' in row:
                        start_week = row['start_week']
                    elif 'start' in row:
                        # å¦‚æœæ˜¯æ—¥æœŸæ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºå‘¨æ•°
                        start_date = row['start']
                        if isinstance(start_date, str) and start_date != '':
                            try:
                                # è¿™é‡Œå¯ä»¥æ·»åŠ æ—¥æœŸåˆ°å‘¨æ•°çš„è½¬æ¢é€»è¾‘
                                start_week = 0  # ç®€åŒ–å¤„ç†
                            except:
                                start_week = 0
                    
                    if 'end_week' in row:
                        end_week = row['end_week']
                    elif 'end' in row:
                        # å¦‚æœæ˜¯æ—¥æœŸæ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºå‘¨æ•°
                        end_date = row['end']
                        if isinstance(end_date, str) and end_date != '':
                            try:
                                # è¿™é‡Œå¯ä»¥æ·»åŠ æ—¥æœŸåˆ°å‘¨æ•°çš„è½¬æ¢é€»è¾‘
                                end_week = self.max_weeks  # ç®€åŒ–å¤„ç†
                            except:
                                end_week = self.max_weeks
                    
                    malicious_users[row['user']] = {
                        'scenario': row.get('scenario', 1),
                        'start_week': start_week,
                        'end_week': end_week
                    }
                print(f"   åŠ è½½ {len(malicious_users)} ä¸ªæ¶æ„ç”¨æˆ·æ ‡ç­¾")
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ¶æ„ç”¨æˆ·æ ‡ç­¾å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ ‡ç­¾ï¼Œåˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„æ¶æ„ç”¨æˆ·
        if not malicious_users:
            print("âš ï¸  æœªæ‰¾åˆ°æ¶æ„ç”¨æˆ·æ ‡ç­¾æ–‡ä»¶ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®")
            sample_users = ['ACM2278', 'CMP2946', 'BTH8471']  # ç¤ºä¾‹ç”¨æˆ·
            for i, user in enumerate(sample_users):
                malicious_users[user] = {
                    'scenario': i + 1,
                    'start_week': np.random.randint(10, 30),
                    'end_week': np.random.randint(40, 60)
                }
        
        return malicious_users
    
    def step3_extract_features(self, users_df: pd.DataFrame, start_week: int = 0, 
                              end_week: int = None, max_users: int = None):
        """
        Step 3: æå–æ´»åŠ¨ç‰¹å¾
        
        Args:
            users_df: ç”¨æˆ·ä¿¡æ¯
            start_week: å¼€å§‹å‘¨æ•°  
            end_week: ç»“æŸå‘¨æ•°
            max_users: æœ€å¤§ç”¨æˆ·æ•°é™åˆ¶
        """
        if end_week is None:
            end_week = self.max_weeks
            
        print(f"\n{'='*60}")
        print(f"Step 3: æå–æ´»åŠ¨ç‰¹å¾ (å‘¨ {start_week} åˆ° {end_week-1})")
        print(f"{'='*60}")
        
        # ç”¨æˆ·æ•°é‡é™åˆ¶
        if max_users and len(users_df) > max_users:
            print(f"åº”ç”¨ç”¨æˆ·æ•°é‡é™åˆ¶: {len(users_df)} -> {max_users}")
            
            malicious_users_df = users_df[users_df['malscene'] > 0]
            normal_users_df = users_df[users_df['malscene'] == 0]

            selected_malicious_indices = []
            selected_normal_indices = []

            if len(malicious_users_df) >= max_users:
                # å¦‚æœæ¶æ„ç”¨æˆ·å°±è¶³å¤Ÿå¤šæˆ–è¶…è¿‡ max_usersï¼Œåˆ™åªä»æ¶æ„ç”¨æˆ·ä¸­é€‰
                np.random.seed(self.seed) # ä½¿ç”¨ self.seed
                selected_malicious_indices = np.random.choice(malicious_users_df.index, size=max_users, replace=False).tolist()
            else:
                # æ¶æ„ç”¨æˆ·ä¸è¶³ max_usersï¼Œå…¨éƒ¨é€‰ä¸­
                selected_malicious_indices = malicious_users_df.index.tolist()
                remaining_slots = max_users - len(selected_malicious_indices)
                if remaining_slots > 0 and not normal_users_df.empty:
                    np.random.seed(self.seed) # ä½¿ç”¨ self.seed
                    num_to_select_normal = min(remaining_slots, len(normal_users_df))
                    selected_normal_indices = np.random.choice(normal_users_df.index,
                                                             size=num_to_select_normal,
                                                             replace=False).tolist()
            
            final_selected_user_indices = selected_malicious_indices + selected_normal_indices
            # å¦‚æœå› ä¸ºæŸç§åŸå›  final_selected_user_indices ä¸ºç©ºä½† max_users > 0ï¼Œéœ€è¦å¤„ç†
            if not final_selected_user_indices and max_users > 0 and not users_df.empty:
                 print(f"âš ï¸ ä¼˜å…ˆç­›é€‰åç”¨æˆ·åˆ—è¡¨ä¸ºç©ºï¼Œä½† max_users={max_users} > 0ã€‚å›é€€åˆ°ä»åŸå§‹ users_df éšæœºé‡‡æ · {max_users} ä¸ªç”¨æˆ·ã€‚")
                 np.random.seed(self.seed) # ä½¿ç”¨ self.seed
                 final_selected_user_indices = np.random.choice(users_df.index, size=min(max_users, len(users_df)), replace=False).tolist()
            
            users_df = users_df.loc[final_selected_user_indices]
            
            # é‡æ–°è®¡ç®—é€‰å‡ºç”¨æˆ·çš„æ¶æ„å’Œæ­£å¸¸æ•°é‡
            num_malicious_selected = sum(users_df['malscene'] > 0) if not users_df.empty else 0
            num_normal_selected = sum(users_df['malscene'] == 0) if not users_df.empty else 0
            print(f"æœ€ç»ˆç”¨æˆ·æ•°: {len(users_df)} (æ¶æ„: {num_malicious_selected}, æ­£å¸¸: {num_normal_selected})")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæ”¶é›†æ‰€æœ‰å‘¨çš„æ•°æ®æ ·æœ¬ï¼‰
        print("ğŸ“š å‡†å¤‡ç¼–ç å™¨è®­ç»ƒæ•°æ®...")
        all_events = []
        parquet_dir = self._get_work_file_path(self.current_parquet_dir_name)
        
        # æ£€æŸ¥ Parquet ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(parquet_dir):
            print(f"âš ï¸  Parquetæ•°æ®ç›®å½•ä¸å­˜åœ¨: {parquet_dir}")
            print("âš ï¸  æ— æ³•æ”¶é›†è®­ç»ƒæ•°æ®ï¼Œç¼–ç å™¨å°†ä½¿ç”¨é»˜è®¤é…ç½®")
        else:
            # ä» Parquet æ–‡ä»¶æ”¶é›†è®­ç»ƒæ•°æ®
            for week in range(start_week, min(end_week, start_week + 5)):  # åªç”¨å‰å‡ å‘¨è®­ç»ƒ
                try:
                    week_data_ddf = dd.read_parquet(
                        parquet_dir,
                        filters=[('week', '==', week)],
                        engine='pyarrow'
                    )
                    
                    if week_data_ddf.npartitions > 0 and week_data_ddf.map_partitions(len).compute().sum() > 0:
                        week_data = week_data_ddf.compute()
                        if len(week_data) > 0:
                            # åªå–æ ·æœ¬æ•°æ®ç”¨äºè®­ç»ƒ
                            sample_size = min(1000, len(week_data))
                            sample_data = week_data.sample(n=sample_size, random_state=42)
                            all_events.append(sample_data)
                            print(f"   ä»å‘¨ {week} æ”¶é›† {len(sample_data)} æ¡è®­ç»ƒæ ·æœ¬")
                except Exception as e:
                    print(f"   âš ï¸  ä»å‘¨ {week} æ”¶é›†è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
                    continue
        
        if all_events:
            training_data = pd.concat(all_events, ignore_index=True)
            print(f"ğŸ“Š è®­ç»ƒæ•°æ®æ€»è®¡: {len(training_data)} æ¡äº‹ä»¶")
            
            # æ‹Ÿåˆç¼–ç å™¨
            print("ğŸ”§ æ‹Ÿåˆç¼–ç å™¨...")
            try:
                self.encoder.fit(training_data, users_df)
                print("âœ… ç¼–ç å™¨æ‹Ÿåˆå®Œæˆ")
            except Exception as e:
                print(f"âŒ ç¼–ç å™¨æ‹Ÿåˆå¤±è´¥: {e}")
                print("âš ï¸  å°†ä½¿ç”¨é»˜è®¤ç¼–ç å™¨é…ç½®")
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œç¼–ç å™¨å°†ä½¿ç”¨é»˜è®¤é…ç½®")
            # å³ä½¿æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä¹Ÿè¦ç¡®ä¿ç¼–ç å™¨æœ‰åŸºæœ¬çš„é…ç½®
            try:
                # åˆ›å»ºä¸€ä¸ªæœ€å°çš„è™šæ‹Ÿæ•°æ®é›†æ¥åˆå§‹åŒ–ç¼–ç å™¨
                dummy_data = pd.DataFrame({
                    'user': ['dummy_user'],
                    'type': ['http'],
                    'date': [pd.Timestamp.now()],
                    'content': ['dummy_content']
                })
                dummy_users = pd.DataFrame({
                    'role': ['Employee'],
                    'dept': ['IT']
                }, index=['dummy_user'])
                self.encoder.fit(dummy_data, dummy_users)
                print("âœ… ç¼–ç å™¨ä½¿ç”¨è™šæ‹Ÿæ•°æ®åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # å¤„ç†æ¯ä¸€å‘¨
        print(f"ğŸ”„ å¼€å§‹å¤„ç† {end_week - start_week} å‘¨çš„æ•°æ®...")
        
        # ä½¿ç”¨ joblib.Parallel å¹¶è¡Œå¤„ç†æ¯å‘¨æ•°æ®
        Parallel(n_jobs=self.num_cores)(
            delayed(self._process_week_features)(week, users_df)
            for week in range(start_week, end_week)
        )
        
        print("âœ… Step 3 å®Œæˆ")
    
    def _process_week_features(self, week: int, users_df: pd.DataFrame):
        """å¤„ç†å•å‘¨çš„ç‰¹å¾æå–"""
        # week_file = self._get_work_file_path(f"DataByWeek/{week}.pickle")
        
        # if not os.path.exists(week_file):
        #     print(f"âš ï¸  å‘¨ {week}: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        #     return
        
        # # è¯»å–å‘¨æ•°æ®
        # week_data = pd.read_pickle(week_file)

        # ä½¿ç”¨ self.current_parquet_dir_name åŠ¨æ€ç¡®å®šè¦è¯»å–çš„ Parquet ç›®å½•
        parquet_dir = self._get_work_file_path(self.current_parquet_dir_name)
        week_data = None # Initialize to None

        try:
            week_data_ddf = dd.read_parquet(
                parquet_dir,
                filters=[('week', '==', week)],
                engine='pyarrow' # or 'fastparquet'
            )
            if week_data_ddf.npartitions == 0 or week_data_ddf.map_partitions(len).compute().sum() == 0:
                 print(f"ğŸ“­ å‘¨ {week}: æ— æ´»åŠ¨æ•°æ® (Parquetåˆ†åŒºä¸ºç©ºæˆ–æœªæ‰¾åˆ°).")
                 empty_features = pd.DataFrame()
                 # ä¿å­˜ç©ºçš„ Parquet æ–‡ä»¶ä»¥ä¿æŒä¸€è‡´æ€§
                 empty_features.to_parquet(self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet"), engine='pyarrow', index=False)
                 return
            week_data = week_data_ddf.compute() # Convert Dask DataFrame to Pandas DataFrame
        except FileNotFoundError: # More specific exception for missing Parquet directory/files
            print(f"âš ï¸  å‘¨ {week}: Parquetæ•°æ®ç›®å½•æˆ–ç‰¹å®šå‘¨åˆ†åŒºæœªæ‰¾åˆ° ({parquet_dir}, week={week}).")
        except Exception as e:
            print(f"âš ï¸  å‘¨ {week}: è¯»å–Parquetæ•°æ®å¤±è´¥ ({e}).")
        
        if week_data is None or week_data.empty: # Check if week_data is None (due to error) or empty
            # Save empty features to avoid breaking downstream if one week is missing/fails
            print(f"âš ï¸  å‘¨ {week}: æœ€ç»ˆæ— æ•°æ®å¯å¤„ç†ï¼Œä¿å­˜ç©ºç‰¹å¾æ–‡ä»¶ã€‚")
            empty_features = pd.DataFrame()
            empty_features.to_parquet(self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet"), engine='pyarrow', index=False)
            return

        if len(week_data) == 0: # Should be caught by the above, but as a safeguard
            print(f"ğŸ“­ å‘¨ {week}: æ— æ´»åŠ¨æ•°æ®")
            # Save empty features to avoid breaking downstream if one week is missing/fails
            empty_features = pd.DataFrame()
            empty_features.to_parquet(self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet"), engine='pyarrow', index=False)
            return
        
        # åœ¨å‘¨çº§åˆ«è¿›è¡Œæ’åºï¼Œæ¨¡ä»¿ feature_extraction.py ä¸­ process_week_num() çš„åšæ³•
        # è¿™æ¯”å…¨å±€æ’åºè¦é«˜æ•ˆå¾—å¤šï¼Œå› ä¸ºåªå¯¹å•å‘¨æ•°æ®æ’åº
        if 'date' in week_data.columns:
            print(f"   ğŸ“… å¯¹å‘¨ {week} çš„ {len(week_data)} æ¡äº‹ä»¶æŒ‰æ—¥æœŸæ’åº...")
            week_data = week_data.sort_values('date').reset_index(drop=True)
        
        # è¿‡æ»¤ç”¨æˆ·
        valid_users = set(week_data['user'].unique()) & set(users_df.index)
        week_data = week_data[week_data['user'].isin(valid_users)]
        
        print(f"ğŸ“Š å‘¨ {week}: {len(week_data)} æ¡äº‹ä»¶, {len(valid_users)} ä¸ªç”¨æˆ·")
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·æå–ç‰¹å¾
        user_features = []
        
        for user_id in valid_users:
            user_events = week_data[week_data['user'] == user_id]
            user_context = users_df.loc[user_id].to_dict()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ¶æ„æ—¶æœŸ
            is_malicious_period = False
            if user_context['malscene'] > 0:
                if user_context['mstart'] <= week <= user_context['mend']:
                    is_malicious_period = True
            
            # ç¼–ç ç”¨æˆ·çš„æ‰€æœ‰äº‹ä»¶
            event_features = []
            for _, event in user_events.iterrows():
                event_dict = event.to_dict()
                try:
                    features, mask = self.encoder.encode_event(event_dict, user_context)
                    event_features.append({
                        'user': user_id,
                        'week': week,
                        'event_type': event.get('type', 'unknown'),
                        'date': event.get('date'),
                        'features': features,
                        'mask': mask,
                        'is_malicious': is_malicious_period
                    })
                except Exception as e:
                    print(f"âš ï¸  ç¼–ç äº‹ä»¶å¤±è´¥ - ç”¨æˆ· {user_id}, å‘¨ {week}: {e}")
                    continue
            
            if event_features:
                user_features.extend(event_features)
        
        # ä¿å­˜å‘¨ç‰¹å¾
        if user_features:
            features_df = pd.DataFrame(user_features)
            # å°† features å’Œ mask åˆ—è½¬æ¢ä¸ºé€‚åˆ Parquet çš„æ ¼å¼ (ä¾‹å¦‚ list of floats)
            # Parquet ä¸ç›´æ¥æ”¯æŒå­˜å‚¨å¤æ‚çš„ NumPy æ•°ç»„å¯¹è±¡ï¼Œé™¤éå®ƒä»¬è¢«è½¬æ¢ä¸ºæ›´ç®€å•çš„ç±»å‹ã€‚
            # å¦‚æœ encoder.encode_event è¿”å›çš„æ˜¯ NumPy æ•°ç»„ï¼Œè¿™é‡Œéœ€è¦å¤„ç†ã€‚
            # å‡è®¾ features å’Œ mask æ˜¯æ•°å€¼å‹æˆ–è€…å¯ä»¥è¢«Parquetæ¥å—çš„åˆ—è¡¨
            # å¦‚æœ features å’Œ mask æ˜¯numpy arrays, to_parquet å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æˆ–è½¬æ¢ä¸ºlist
            try:
                # å°è¯•ç›´æ¥ä¿å­˜ã€‚å¦‚æœfeatures/maskæ˜¯å¤æ‚å¯¹è±¡ï¼Œå¯èƒ½éœ€è¦è½¬æ¢
                # features_df['features'] = features_df['features'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)
                # features_df['mask'] = features_df['mask'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)
                features_df.to_parquet(self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet"), engine='pyarrow', index=False)
            except Exception as e_parquet:
                print(f"   âš ï¸  å‘¨ {week}: ä¿å­˜åˆ° Parquet å¤±è´¥ ({e_parquet})ã€‚å°è¯•å°† features/mask è½¬ä¸º list åä¿å­˜...")
                try:
                    features_df_copy = features_df.copy()
                    if 'features' in features_df_copy.columns:
                        features_df_copy['features'] = features_df_copy['features'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)
                    if 'mask' in features_df_copy.columns:
                         features_df_copy['mask'] = features_df_copy['mask'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)
                    features_df_copy.to_parquet(self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet"), engine='pyarrow', index=False)
                except Exception as e_list_save:
                    print(f"   âŒ å‘¨ {week}: è½¬æ¢ä¸ºliståä¿å­˜åˆ° Parquet ä»ç„¶å¤±è´¥ ({e_list_save})ã€‚å°†å›é€€åˆ° Pickleã€‚")
                    features_df.to_pickle(self._get_work_file_path(f"NumDataByWeek/{week}_features.pickle")) # Fallback
            print(f"ğŸ’¾ å‘¨ {week}: ä¿å­˜ {len(user_features)} ä¸ªç‰¹å¾å‘é‡åˆ° Parquet (æˆ–Pickleå›é€€)")
        else:
            print(f"âš ï¸  å‘¨ {week}: æ²¡æœ‰æå–åˆ°æœ‰æ•ˆç‰¹å¾")
    
    def step4_multi_level_analysis(self, start_week: int = 0, end_week: int = None, 
                                   modes: List[str] = ['week', 'day', 'session'],
                                   force_regenerate_analysis_levels: bool = False):
        """
        Step 4: å¤šçº§åˆ«åˆ†æ
        
        Args:
            start_week: å¼€å§‹å‘¨æ•°
            end_week: ç»“æŸå‘¨æ•°  
            modes: åˆ†ææ¨¡å¼åˆ—è¡¨
            force_regenerate_analysis_levels: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆå¤šçº§åˆ«åˆ†æçš„CSVæ–‡ä»¶
        """
        if end_week is None:
            end_week = self.max_weeks
            
        print(f"\n{'='*60}")
        print(f"Step 4: å¤šçº§åˆ«åˆ†æ - æ¨¡å¼: {modes}, å¼ºåˆ¶é‡æ–°ç”Ÿæˆ: {force_regenerate_analysis_levels}")
        print(f"{'='*60}")
        
        for mode in modes:
            if mode == 'week':
                self._week_level_analysis(start_week, end_week, force_regenerate_analysis_levels)
            elif mode == 'day':
                self._day_level_analysis(start_week, end_week, force_regenerate_analysis_levels)
            elif mode == 'session':
                self._session_level_analysis(start_week, end_week, force_regenerate_analysis_levels)
            else:
                print(f"âš ï¸  æœªçŸ¥åˆ†ææ¨¡å¼: {mode}")
        
        print("âœ… Step 4 å®Œæˆ")
    
    def _week_level_analysis(self, start_week: int, end_week: int, force_regenerate_analysis_levels: bool):
        """å‘¨çº§åˆ«åˆ†æ"""
        print("\nğŸ“… æ‰§è¡Œå‘¨çº§åˆ«åˆ†æ...")
        
        output_file = self._get_work_file_path(f"WeekLevelFeatures/weeks_{start_week}_{end_week-1}.csv")
        if os.path.exists(output_file) and not force_regenerate_analysis_levels:
            print(f"   âœ… å‘¨çº§åˆ«åˆ†æç»“æœ {output_file} å·²å­˜åœ¨ä¸”æœªå¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œè·³è¿‡ã€‚")
            return
            
        week_features = []
        
        for week in range(start_week, end_week):
            features_file_parquet = self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet")
            features_file_pickle = self._get_work_file_path(f"NumDataByWeek/{week}_features.pickle") # Fallback
            
            week_data = None
            if os.path.exists(features_file_parquet):
                try:
                    week_data = pd.read_parquet(features_file_parquet, engine='pyarrow')
                except Exception as e_parquet_read:
                    print(f"   âš ï¸ å‘¨ {week}: è¯»å– Parquet ç‰¹å¾æ–‡ä»¶ {features_file_parquet} å¤±è´¥ ({e_parquet_read}). å°è¯• Pickle å›é€€...")
                    if os.path.exists(features_file_pickle):
                        try:
                            week_data = pd.read_pickle(features_file_pickle)
                            print(f"     âœ… å‘¨ {week}: æˆåŠŸä» Pickle æ–‡ä»¶ {features_file_pickle} å›é€€è¯»å–ã€‚")
                        except Exception as e_pickle_read:
                            print(f"     âŒ å‘¨ {week}: è¯»å– Pickle å›é€€æ–‡ä»¶ {features_file_pickle} ä¹Ÿå¤±è´¥ ({e_pickle_read}).")
                    else:
                        print(f"     å‘¨ {week}: Pickle å›é€€æ–‡ä»¶ {features_file_pickle} ä¸å­˜åœ¨ã€‚")
            elif os.path.exists(features_file_pickle):
                print(f"   å‘¨ {week}: Parquet ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è¯»å– Pickle æ–‡ä»¶ {features_file_pickle}...")
                try:
                    week_data = pd.read_pickle(features_file_pickle)
                except Exception as e_pickle_read:
                    print(f"     âŒ å‘¨ {week}: è¯»å– Pickle æ–‡ä»¶ {features_file_pickle} å¤±è´¥ ({e_pickle_read}).")
            
            if week_data is None or week_data.empty:
                continue
            
            # æŒ‰ç”¨æˆ·èšåˆå‘¨ç‰¹å¾
            for user in week_data['user'].unique():
                user_week_data = week_data[week_data['user'] == user]
                
                # è®¡ç®—ç”¨æˆ·è¯¥å‘¨çš„èšåˆç‰¹å¾
                user_features = self._aggregate_user_features(user_week_data, 'week')
                user_features.update({
                    'user': user,
                    'week': week,
                    'mode': 'week',
                    'n_events': len(user_week_data),
                    'malicious_ratio': user_week_data['is_malicious'].mean()
                })
                
                week_features.append(user_features)
        
        # ä¿å­˜å‘¨çº§åˆ«ç‰¹å¾
        if week_features:
            week_df = pd.DataFrame(week_features)
            week_df.to_csv(output_file, index=False)
            print(f"ğŸ’¾ å‘¨çº§åˆ«åˆ†æå®Œæˆ: {output_file} ({len(week_features)} æ¡è®°å½•)")
        else:
            print("âš ï¸  å‘¨çº§åˆ«åˆ†æ: æ²¡æœ‰æ•°æ®")
    
    def _day_level_analysis(self, start_week: int, end_week: int, force_regenerate_analysis_levels: bool):
        """æ—¥çº§åˆ«åˆ†æ"""
        print("\nğŸ“† æ‰§è¡Œæ—¥çº§åˆ«åˆ†æ...")
        
        output_file = self._get_work_file_path(f"DayLevelFeatures/days_{start_week}_{end_week-1}.csv")
        if os.path.exists(output_file) and not force_regenerate_analysis_levels:
            print(f"   âœ… æ—¥çº§åˆ«åˆ†æç»“æœ {output_file} å·²å­˜åœ¨ä¸”æœªå¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œè·³è¿‡ã€‚")
            return
            
        day_features = []
        
        for week in range(start_week, end_week):
            features_file_parquet = self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet")
            features_file_pickle = self._get_work_file_path(f"NumDataByWeek/{week}_features.pickle") # Fallback

            week_data = None
            if os.path.exists(features_file_parquet):
                try:
                    week_data = pd.read_parquet(features_file_parquet, engine='pyarrow')
                except Exception as e_parquet_read:
                    print(f"   âš ï¸ å‘¨ {week}: è¯»å– Parquet ç‰¹å¾æ–‡ä»¶ {features_file_parquet} å¤±è´¥ ({e_parquet_read}). å°è¯• Pickle å›é€€...")
                    if os.path.exists(features_file_pickle):
                        try:
                            week_data = pd.read_pickle(features_file_pickle)
                            print(f"     âœ… å‘¨ {week}: æˆåŠŸä» Pickle æ–‡ä»¶ {features_file_pickle} å›é€€è¯»å–ã€‚")
                        except Exception as e_pickle_read:
                            print(f"     âŒ å‘¨ {week}: è¯»å– Pickle å›é€€æ–‡ä»¶ {features_file_pickle} ä¹Ÿå¤±è´¥ ({e_pickle_read}).")
                    else:
                        print(f"     å‘¨ {week}: Pickle å›é€€æ–‡ä»¶ {features_file_pickle} ä¸å­˜åœ¨ã€‚")
            elif os.path.exists(features_file_pickle):
                print(f"   å‘¨ {week}: Parquet ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è¯»å– Pickle æ–‡ä»¶ {features_file_pickle}...")
                try:
                    week_data = pd.read_pickle(features_file_pickle)
                except Exception as e_pickle_read:
                    print(f"     âŒ å‘¨ {week}: è¯»å– Pickle æ–‡ä»¶ {features_file_pickle} å¤±è´¥ ({e_pickle_read}).")

            if week_data is None or week_data.empty:
                continue
            
            # æ·»åŠ æ—¥æœŸåˆ—
            week_data['day'] = pd.to_datetime(week_data['date']).dt.date
            
            # æŒ‰ç”¨æˆ·å’Œæ—¥æœŸèšåˆ
            for user in week_data['user'].unique():
                user_data = week_data[week_data['user'] == user]
                
                for day in user_data['day'].unique():
                    day_data = user_data[user_data['day'] == day]
                    
                    # è®¡ç®—ç”¨æˆ·è¯¥æ—¥çš„èšåˆç‰¹å¾
                    day_feature = self._aggregate_user_features(day_data, 'day')
                    day_feature.update({
                        'user': user,
                        'week': week,
                        'day': day,
                        'mode': 'day',
                        'n_events': len(day_data),
                        'malicious_ratio': day_data['is_malicious'].mean()
                    })
                    
                    day_features.append(day_feature)
        
        # ä¿å­˜æ—¥çº§åˆ«ç‰¹å¾
        if day_features:
            day_df = pd.DataFrame(day_features)
            day_df.to_csv(output_file, index=False)
            print(f"ğŸ’¾ æ—¥çº§åˆ«åˆ†æå®Œæˆ: {output_file} ({len(day_features)} æ¡è®°å½•)")
        else:
            print("âš ï¸  æ—¥çº§åˆ«åˆ†æ: æ²¡æœ‰æ•°æ®")
    
    def _session_level_analysis(self, start_week: int, end_week: int, force_regenerate_analysis_levels: bool):
        """ä¼šè¯çº§åˆ«åˆ†æ"""
        print("\nğŸ–¥ï¸  æ‰§è¡Œä¼šè¯çº§åˆ«åˆ†æ...")
        
        output_file = self._get_work_file_path(f"SessionLevelFeatures/sessions_{start_week}_{end_week-1}.csv")
        if os.path.exists(output_file) and not force_regenerate_analysis_levels:
            print(f"   âœ… ä¼šè¯çº§åˆ«åˆ†æç»“æœ {output_file} å·²å­˜åœ¨ä¸”æœªå¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œè·³è¿‡ã€‚")
            return
            
        session_features = []
        
        for week in range(start_week, end_week):
            features_file_parquet = self._get_work_file_path(f"NumDataByWeek/{week}_features.parquet")
            features_file_pickle = self._get_work_file_path(f"NumDataByWeek/{week}_features.pickle") # Fallback
            
            week_data = None
            if os.path.exists(features_file_parquet):
                try:
                    week_data = pd.read_parquet(features_file_parquet, engine='pyarrow')
                except Exception as e_parquet_read:
                    print(f"   âš ï¸ å‘¨ {week}: è¯»å– Parquet ç‰¹å¾æ–‡ä»¶ {features_file_parquet} å¤±è´¥ ({e_parquet_read}). å°è¯• Pickle å›é€€...")
                    if os.path.exists(features_file_pickle):
                        try:
                            week_data = pd.read_pickle(features_file_pickle)
                            print(f"     âœ… å‘¨ {week}: æˆåŠŸä» Pickle æ–‡ä»¶ {features_file_pickle} å›é€€è¯»å–ã€‚")
                        except Exception as e_pickle_read:
                            print(f"     âŒ å‘¨ {week}: è¯»å– Pickle å›é€€æ–‡ä»¶ {features_file_pickle} ä¹Ÿå¤±è´¥ ({e_pickle_read}).")
                    else:
                        print(f"     å‘¨ {week}: Pickle å›é€€æ–‡ä»¶ {features_file_pickle} ä¸å­˜åœ¨ã€‚")
            elif os.path.exists(features_file_pickle):
                print(f"   å‘¨ {week}: Parquet ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è¯»å– Pickle æ–‡ä»¶ {features_file_pickle}...")
                try:
                    week_data = pd.read_pickle(features_file_pickle)
                except Exception as e_pickle_read:
                    print(f"     âŒ å‘¨ {week}: è¯»å– Pickle æ–‡ä»¶ {features_file_pickle} å¤±è´¥ ({e_pickle_read}).")
            
            if week_data is None or week_data.empty:
                continue
            
            # æŒ‰ç”¨æˆ·åˆ†ç»„æå–ä¼šè¯
            for user in week_data['user'].unique():
                user_data = week_data[week_data['user'] == user]
                user_data = user_data.sort_values('date')
                
                # è¯†åˆ«ä¼šè¯ï¼ˆåŸºäºæ—¶é—´é—´éš”ï¼‰
                sessions = self._identify_sessions(user_data)
                
                for session_id, session_data in sessions.items():
                    # è®¡ç®—ä¼šè¯ç‰¹å¾
                    session_feature = self._aggregate_user_features(session_data, 'session')
                    
                    # æ·»åŠ ä¼šè¯ç‰¹å®šç‰¹å¾
                    if len(session_data) > 1:
                        start_time = pd.to_datetime(session_data['date'].min())
                        end_time = pd.to_datetime(session_data['date'].max())
                        duration_minutes = (end_time - start_time).total_seconds() / 60
                    else:
                        duration_minutes = 0
                    
                    session_feature.update({
                        'user': user,
                        'week': week,
                        'session_id': session_id,
                        'mode': 'session',
                        'n_events': len(session_data),
                        'duration_minutes': duration_minutes,
                        'malicious_ratio': session_data['is_malicious'].mean()
                    })
                    
                    session_features.append(session_feature)
        
        # ä¿å­˜ä¼šè¯çº§åˆ«ç‰¹å¾
        if session_features:
            session_df = pd.DataFrame(session_features)
            session_df.to_csv(output_file, index=False)
            print(f"ğŸ’¾ ä¼šè¯çº§åˆ«åˆ†æå®Œæˆ: {output_file} ({len(session_features)} æ¡è®°å½•)")
        else:
            print("âš ï¸  ä¼šè¯çº§åˆ«åˆ†æ: æ²¡æœ‰æ•°æ®")
    
    def _identify_sessions(self, user_data: pd.DataFrame, gap_threshold: int = 60):
        """
        è¯†åˆ«ç”¨æˆ·ä¼šè¯
        
        Args:
            user_data: ç”¨æˆ·æ•°æ®
            gap_threshold: ä¼šè¯é—´éš”é˜ˆå€¼ï¼ˆåˆ†é’Ÿï¼‰
            
        Returns:
            sessions: ä¼šè¯å­—å…¸
        """
        sessions = {}
        current_session = 0
        last_time = None
        
        user_data = user_data.sort_values('date').reset_index(drop=True)
        
        for idx, row in user_data.iterrows():
            current_time = pd.to_datetime(row['date'])
            
            # å¦‚æœæ—¶é—´é—´éš”è¶…è¿‡é˜ˆå€¼ï¼Œå¼€å§‹æ–°ä¼šè¯
            if last_time is not None:
                gap_minutes = (current_time - last_time).total_seconds() / 60
                if gap_minutes > gap_threshold:
                    current_session += 1
            
            if current_session not in sessions:
                sessions[current_session] = []
            
            sessions[current_session].append(row)
            last_time = current_time
        
        # è½¬æ¢ä¸ºDataFrame
        session_dfs = {}
        for session_id, session_events in sessions.items():
            session_dfs[session_id] = pd.DataFrame(session_events)
        
        return session_dfs
    
    def _aggregate_user_features(self, user_data: pd.DataFrame, mode: str) -> Dict:
        """
        èšåˆç”¨æˆ·ç‰¹å¾
        
        Args:
            user_data: ç”¨æˆ·äº‹ä»¶æ•°æ®
            mode: èšåˆæ¨¡å¼
            
        Returns:
            aggregated_features: èšåˆç‰¹å¾å­—å…¸
        """
        if len(user_data) == 0:
            return {}
        
        # æå–æ‰€æœ‰ç‰¹å¾å‘é‡
        feature_vectors = np.stack(user_data['features'].values)
        mask_vectors = np.stack(user_data['mask'].values)
        
        # è®¡ç®—æœ‰æ•ˆç‰¹å¾çš„ç»Ÿè®¡é‡
        valid_features = feature_vectors * mask_vectors
        
        aggregated = {
            # åŸºç¡€ç»Ÿè®¡
            'mean_features': np.mean(valid_features, axis=0),
            'std_features': np.std(valid_features, axis=0),
            'max_features': np.max(valid_features, axis=0),
            'min_features': np.min(valid_features, axis=0),
            
            # äº‹ä»¶ç±»å‹åˆ†å¸ƒ
            'email_ratio': (user_data['event_type'] == 'email').mean(),
            'file_ratio': (user_data['event_type'] == 'file').mean(),
            'http_ratio': (user_data['event_type'] == 'http').mean(),
            'device_ratio': (user_data['event_type'] == 'device').mean(),
            'logon_ratio': (user_data['event_type'] == 'logon').mean(),
            
            # ç‰¹å¾æœ‰æ•ˆæ€§
            'feature_completeness': mask_vectors.mean(),
            
            # æ—¶é—´ç‰¹å¾ï¼ˆå¦‚æœæœ‰æ—¥æœŸä¿¡æ¯ï¼‰
            'unique_event_types': user_data['event_type'].nunique()
        }
        
        # å¹³å±•å¤šç»´ç‰¹å¾ä¸ºæ ‡é‡ï¼ˆç”¨äºCSVä¿å­˜ï¼‰
        for i, val in enumerate(aggregated['mean_features']):
            aggregated[f'mean_feature_{i}'] = val
        
        # ç§»é™¤å¤šç»´æ•°ç»„ï¼ˆé¿å…CSVä¿å­˜é”™è¯¯ï¼‰
        del aggregated['mean_features']
        del aggregated['std_features']
        del aggregated['max_features']  
        del aggregated['min_features']
        
        return aggregated
    
    def run_full_pipeline(self, start_week: int = 0, end_week: int = None, 
                         max_users: int = None, modes: List[str] = ['week', 'day', 'session'],
                         sample_ratio: float = None, force_regenerate_combined_weeks: bool = False,
                         force_regenerate_analysis_levels: bool = False):
        """
        è¿è¡Œå®Œæ•´çš„ç‰¹å¾æå–æµæ°´çº¿
        
        Args:
            start_week: å¼€å§‹å‘¨æ•°
            end_week: ç»“æŸå‘¨æ•°
            max_users: æœ€å¤§ç”¨æˆ·æ•°é™åˆ¶
            modes: åˆ†ææ¨¡å¼
            sample_ratio: æ•°æ®é‡‡æ ·æ¯”ä¾‹ (0-1)ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
            force_regenerate_combined_weeks: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆå‘¨æ•°æ® Parquet æ–‡ä»¶é›†
            force_regenerate_analysis_levels: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆå¤šçº§åˆ«åˆ†æçš„CSVæ–‡ä»¶
        """
        start_time = time.time()
        self.current_sample_ratio = sample_ratio # å­˜å‚¨å½“å‰è¿è¡Œçš„é‡‡æ ·ç‡
        
        if sample_ratio is not None and 0 < sample_ratio < 1.0:
            self.current_parquet_dir_name = f"DataByWeek_parquet_r{str(sample_ratio).replace('.', '')}"
        else:
            self.current_parquet_dir_name = "DataByWeek_parquet"
        print(f"Target Parquet directory for this run: {self.current_parquet_dir_name}")

        print(f"ğŸš€ å¯åŠ¨CERTæ•°æ®é›†ç‰¹å¾æå–æµæ°´çº¿")
        print(f"ğŸ“Š å‚æ•°: å‘¨ {start_week}-{end_week or self.max_weeks-1}, ç”¨æˆ·é™åˆ¶: {max_users or 'æ— '}, æ¨¡å¼: {modes}, é‡‡æ ·ç‡: {sample_ratio or 1.0}")
        
        try:
            # Step 1: åˆå¹¶åŸå§‹æ•°æ®
            self.step1_combine_raw_data(start_week, end_week, sample_ratio, force_regenerate=force_regenerate_combined_weeks)
            
            # Step 2: åŠ è½½ç”¨æˆ·æ•°æ®  
            users_df = self.step2_load_user_data()
            
            # Step 3: æå–ç‰¹å¾
            self.step3_extract_features(users_df, start_week, end_week, max_users)
            
            # Step 4: å¤šçº§åˆ«åˆ†æ
            self.step4_multi_level_analysis(start_week, end_week, modes, force_regenerate_analysis_levels)
            
            total_time = (time.time() - start_time) / 60
            print(f"\nğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆ! æ€»è€—æ—¶: {total_time:.1f} åˆ†é’Ÿ")
            
            # è¾“å‡ºç»“æœç»Ÿè®¡
            self._print_results_summary(start_week, end_week, modes)
            
        except Exception as e:
            print(f"\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _print_results_summary(self, start_week: int, end_week: int, modes: List[str]):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        print(f"\nğŸ“‹ ç»“æœæ‘˜è¦:")
        print(f"{'='*40}")
        
        for mode in modes:
            if mode == 'week':
                pattern = self._get_work_file_path(f"WeekLevelFeatures/weeks_{start_week}_{end_week-1}.csv")
            elif mode == 'day':
                pattern = self._get_work_file_path(f"DayLevelFeatures/days_{start_week}_{end_week-1}.csv")
            elif mode == 'session':
                pattern = self._get_work_file_path(f"SessionLevelFeatures/sessions_{start_week}_{end_week-1}.csv")
            else:
                continue
            
            if os.path.exists(pattern):
                df = pd.read_csv(pattern)
                print(f"ğŸ“Š {mode.capitalize()}çº§åˆ«: {len(df)} æ¡è®°å½• -> {pattern}")
            else:
                print(f"âš ï¸  {mode.capitalize()}çº§åˆ«: æ— è¾“å‡ºæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import sys
    
    # é»˜è®¤å‚æ•°
    num_cores = 8
    start_week = 0  
    end_week = None
    max_users = None
    modes = ['week', 'day', 'session']
    data_version = 'r4.2'
    
    # è§£æå‘½ä»¤è¡Œ
    if len(sys.argv) > 1:
        num_cores = int(sys.argv[1])
    if len(sys.argv) > 2:
        start_week = int(sys.argv[2])
    if len(sys.argv) > 3:
        end_week = int(sys.argv[3])
    if len(sys.argv) > 4:
        max_users = int(sys.argv[4])
    if len(sys.argv) > 5:
        modes = sys.argv[5].split(',')
    
    # è·å–æ•°æ®ç‰ˆæœ¬
    current_dir = os.getcwd().split('/')[-1]
    if current_dir in ['r4.1', 'r4.2', 'r5.1', 'r5.2', 'r6.1', 'r6.2']:
        data_version = current_dir
    
    print(f"ğŸ¯ é…ç½®å‚æ•°:")
    print(f"   æ•°æ®ç‰ˆæœ¬: {data_version}")
    print(f"   CPUæ ¸å¿ƒæ•°: {num_cores}")
    print(f"   å¤„ç†å‘¨èŒƒå›´: {start_week} - {end_week or 'æœ€å¤§å‘¨æ•°'}")
    print(f"   ç”¨æˆ·é™åˆ¶: {max_users or 'æ— é™åˆ¶'}")
    print(f"   åˆ†ææ¨¡å¼: {modes}")
    
    # åˆ›å»ºå¹¶è¿è¡Œæµæ°´çº¿
    pipeline = CERTDatasetPipeline(
        data_version=data_version,
        feature_dim=256,
        num_cores=num_cores
    )
    
    pipeline.run_full_pipeline(
        start_week=start_week,
        end_week=end_week,
        max_users=max_users,
        modes=modes
    )

if __name__ == "__main__":
    main() 