#!/usr/bin/env python3
"""Multi-modal Anomaly Detection Model"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import sys
import os
import logging

# æ·»åŠ æ¨¡å‹è·¯å¾„
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..config import ModelConfig
    from ..models.base_model.transformer_encoder import TransformerEncoder
    from ..models.base_model.user_gnn import UserGNN
    from ..models.base_model.base_fusion import BaseFusion
    from ..models.base_model.head import ClassificationHead
    from ..models.text_encoder.bert_module import BERTTextEncoder
    from ..models.structure_encoder.lightgbm_branch import LightGBMBranch
    # from ..models.fusion.attention_fusion import AttentionFusion # Comment out old fusion
    from ..models.fusion.cross_attention_fusion import CrossAttentionFusion # Import new fusion
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œæ·»åŠ è·¯å¾„å¹¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    if parent_dir not in sys.path:
        sys.path.insert(0, parent_dir)
    
    from config import ModelConfig
    from models.base_model.transformer_encoder import TransformerEncoder
    from models.base_model.user_gnn import UserGNN
    from models.base_model.base_fusion import BaseFusion
    from models.base_model.head import ClassificationHead
    from models.text_encoder.bert_module import BERTTextEncoder
    from models.structure_encoder.lightgbm_branch import LightGBMBranch
    # from models.fusion.attention_fusion import AttentionFusion # Comment out old fusion
    from models.fusion.cross_attention_fusion import CrossAttentionFusion # Import new fusion

class MultiModalAnomalyDetector(nn.Module):
    """
    å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹
    
    æ•´åˆæ‰€æœ‰æ¨¡æ€ç¼–ç å™¨å’Œèåˆæœºåˆ¶ï¼Œç”¨äºç”¨æˆ·è¡Œä¸ºå¼‚å¸¸æ£€æµ‹
    """
    
    def __init__(
        self,
        model_config_obj: ModelConfig, # ä¸»é…ç½®å¯¹è±¡
        # å„ç»„ä»¶çš„é…ç½®å­—å…¸ï¼Œä¸»è¦ç”¨äºä¼ é€’åŠ¨æ€ç¡®å®šçš„input_dim
        transformer_config: Dict, 
        gnn_config: Dict,
        bert_config: Dict,
        lgbm_config: Dict,
        fusion_config: Dict,
        head_config: Dict,
        # embed_dim å’Œ dropout å¯ä»¥ä» model_config_obj è·å–ï¼Œè¿™é‡Œä½œä¸ºå‚æ•°æ˜¯å†—ä½™çš„
    ):
        super().__init__()
        
        self.model_config = model_config_obj # å­˜å‚¨ ModelConfig
        self.embed_dim = self.model_config.hidden_dim # ä¸»éšè—ç»´åº¦
        
        # 1. è¡Œä¸ºåºåˆ—ç¼–ç å™¨ï¼ˆTransformerï¼‰
        # ä½¿ç”¨ model_config_obj ä¸­çš„å‚æ•°ï¼Œå¹¶ä»ä¼ å…¥çš„ transformer_config æ›´æ–° input_dim
        _actual_transformer_config = {
            'input_dim': transformer_config['input_dim'], # æ¥è‡ª trainer
            'hidden_dim': self.model_config.hidden_dim,
            'num_heads': self.model_config.num_heads,
            'num_layers': self.model_config.num_layers
            # TransformerEncoder __init__ ä¸æ¥å— dropout, å…¶å†…éƒ¨ dropout å›ºå®š
        }
        self.behavior_encoder = TransformerEncoder(**_actual_transformer_config)
        
        # 2. ç”¨æˆ·å…³ç³»ç¼–ç å™¨ï¼ˆGNNï¼‰
        _actual_gnn_config = {
            'input_dim': gnn_config['input_dim'], # æ¥è‡ª trainer
            'hidden_dim': self.model_config.gnn_hidden_dim,
            'output_dim': self.model_config.hidden_dim, # GNN è¾“å‡ºå¯¹é½åˆ°ä¸» hidden_dim
            'num_layers': self.model_config.gnn_num_layers,
            'dropout': self.model_config.gnn_dropout
        }
        self.user_encoder = UserGNN(**_actual_gnn_config)
        
        # 3. æ–‡æœ¬å†…å®¹ç¼–ç å™¨ï¼ˆBERTï¼‰
        _actual_bert_config = {
            'bert_model_name': self.model_config.bert_model_name,
            'max_length': self.model_config.bert_max_length,
            'output_dim': self.model_config.hidden_dim, # BERT è¾“å‡ºå¯¹é½
            'dropout': self.model_config.dropout
            # input_dim for BERT is handled internally by tokenizer
        }
        self.text_encoder = BERTTextEncoder(**_actual_bert_config)
        
        # 4. ç»“æ„åŒ–ç‰¹å¾ç¼–ç å™¨ï¼ˆLightGBMï¼‰
        _actual_lgbm_config = {
            'input_dim': lgbm_config['input_dim'], # æ¥è‡ª trainer
            'output_dim': self.model_config.hidden_dim, # LGBM è¾“å‡ºå¯¹é½
            'dropout': self.model_config.dropout, # General dropout
            # Pass LightGBM specific params from ModelConfig
            'num_leaves': self.model_config.lgbm_num_leaves,
            'max_depth': self.model_config.lgbm_max_depth,
            'learning_rate': self.model_config.lgbm_learning_rate,
            'n_estimators': self.model_config.lgbm_n_estimators if hasattr(self.model_config, 'lgbm_n_estimators') else 100,
            'feature_fraction': self.model_config.lgbm_feature_fraction,
            'bagging_fraction': self.model_config.lgbm_bagging_fraction if hasattr(self.model_config, 'lgbm_bagging_fraction') else 0.8,
            'bagging_freq': self.model_config.lgbm_bagging_freq if hasattr(self.model_config, 'lgbm_bagging_freq') else 5
        }
        self.structure_encoder = LightGBMBranch(**_actual_lgbm_config)
        
        # 5. å¤šæ¨¡æ€èåˆå™¨
        # åŠ¨æ€æ„å»º input_dims_for_fusion based on enabled_modalities and their actual output dimensions
        # self.fusion_input_dims = [] # Old list way
        self.fusion_input_dims_dict = {} # New dict way for CrossAttentionFusion

        if 'behavior' in self.model_config.enabled_modalities:
            # self.fusion_input_dims.append(_actual_transformer_config['hidden_dim'])
            # self.fusion_input_dims_dict['behavior'] = self.behavior_encoder.output_dim # Use actual output_dim from encoder
            self.fusion_input_dims_dict['behavior'] = _actual_transformer_config['hidden_dim'] # Transformer output_dim is its hidden_dim
        if 'graph' in self.model_config.enabled_modalities:
            # self.fusion_input_dims.append(_actual_gnn_config['output_dim'])
            self.fusion_input_dims_dict['graph'] = _actual_gnn_config['output_dim'] # GNN config already specifies output_dim aligned with hidden_dim
        if 'text' in self.model_config.enabled_modalities:
            # self.fusion_input_dims.append(_actual_bert_config['output_dim'])
            self.fusion_input_dims_dict['text'] = _actual_bert_config['output_dim'] # BERT config specifies output_dim aligned with hidden_dim
        if 'structured' in self.model_config.enabled_modalities:
            # self.fusion_input_dims.append(_actual_lgbm_config['output_dim'])
            self.fusion_input_dims_dict['structured'] = _actual_lgbm_config['output_dim'] # LGBM config specifies output_dim aligned with hidden_dim
        
        # å†³å®šèåˆæ¨¡å—è¾“å‡ºç»´åº¦ (ä¹Ÿæ˜¯åˆ†ç±»å¤´è¾“å…¥ç»´åº¦)
        # For CrossAttentionFusion, the output_dim is self.model_config.hidden_dim (or passed as embed_dim to it)
        effective_fusion_embed_dim = self.model_config.hidden_dim

        # _actual_fusion_config = { # For old AttentionFusion
        #     'input_dims': self.fusion_input_dims,
        #     'embed_dim': effective_fusion_embed_dim,
        #     'dropout': self.model_config.dropout,
        #     'use_gating': True 
        # }
        
        _actual_cross_fusion_config = {
            'input_dims': self.fusion_input_dims_dict,
            'embed_dim': self.model_config.hidden_dim, # This is the output dim of CrossAttentionFusion's MLP
            'num_heads': getattr(self.model_config, 'fusion_num_heads', 4), # Default to 4 if not in config
            'dropout': self.model_config.dropout,
            'query_modality_name': getattr(self.model_config, 'fusion_query_modality', 'behavior') # Default
        }

        # self.fusion_module = AttentionFusion(**_actual_fusion_config) # Old
        self.fusion_module = CrossAttentionFusion(**_actual_cross_fusion_config) # New
        
        # 6. åˆ†ç±»å¤´
        _actual_head_config = {
            'input_dim': effective_fusion_embed_dim, # Input to head is output of fusion
            'num_classes': self.model_config.num_classes,
            'dropout': self.model_config.head_dropout 
        }
        # ä»ModelConfigè·å–å¯é€‰å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self.model_config, 'head_hidden_dims'):
            _actual_head_config['hidden_dims'] = self.model_config.head_hidden_dims
        if hasattr(self.model_config, 'head_activation'):
            _actual_head_config['activation'] = self.model_config.head_activation

        # print(f"[DEBUG MultiModalAnomalyDetector] Fusion module embed_dim: {self.fusion_module.embed_dim}")
        # print(f"[DEBUG MultiModalAnomalyDetector] ClassificationHead _actual_head_config: {_actual_head_config}")

        self.classification_head = ClassificationHead(**_actual_head_config)
        
        self._feature_cache = {}
        
    def encode_behavior_sequences(self, sequences: torch.Tensor) -> torch.Tensor:
        """ç¼–ç è¡Œä¸ºåºåˆ—"""
        return self.behavior_encoder(sequences)
    
    def encode_user_relations(self, node_features: torch.Tensor, 
                            adjacency_matrix: torch.Tensor) -> torch.Tensor:
        """ç¼–ç ç”¨æˆ·å…³ç³»"""
        return self.user_encoder(node_features, adjacency_matrix)
    
    def encode_text_content(self, texts: List[str]) -> torch.Tensor:
        """ç¼–ç æ–‡æœ¬å†…å®¹"""
        return self.text_encoder(texts)
    
    def encode_structured_features(self, features: Union[torch.Tensor, dict]) -> torch.Tensor:
        """ç¼–ç ç»“æ„åŒ–ç‰¹å¾"""
        return self.structure_encoder(features)
    
    def forward(self, inputs: Dict[str, Union[torch.Tensor, List]]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: è¾“å…¥å­—å…¸ï¼ŒåŒ…å«å„æ¨¡æ€æ•°æ®
                - 'behavior_sequences': [batch_size, seq_len, feature_dim] è¡Œä¸ºåºåˆ—
                - 'node_features': [num_nodes, node_feat_dim] èŠ‚ç‚¹ç‰¹å¾
                - 'adjacency_matrix': [num_nodes, num_nodes] é‚»æ¥çŸ©é˜µ
                - 'text_content': List[str] æ–‡æœ¬å†…å®¹
                - 'structured_features': ç»“æ„åŒ–ç‰¹å¾ï¼ˆtensoræˆ–dictï¼‰
                
        Returns:
            è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«é¢„æµ‹ç»“æœå’Œä¸­é—´ç‰¹å¾
        """
        batch_size = inputs['behavior_sequences'].shape[0]
        
        # 1. å„æ¨¡æ€ç‰¹å¾ç¼–ç 
        # modality_features = [] # Old list
        # modality_names = []    # Old list
        encoded_features_dict = {} # New dict
        
        # print(f"[DEBUG MMA Detector] Initial enabled_modalities: {self.model_config.enabled_modalities}")

        # è¡Œä¸ºåºåˆ—ç‰¹å¾
        if 'behavior_sequences' in inputs and 'behavior' in self.model_config.enabled_modalities:
            behavior_features = self.encode_behavior_sequences(inputs['behavior_sequences'])
            # modality_features.append(behavior_features)
            # modality_names.append('behavior')
            encoded_features_dict['behavior'] = behavior_features
            # print(f"[DEBUG MMA Detector] Added 'behavior' features. Shape: {behavior_features.shape}")
            
        # ç”¨æˆ·å…³ç³»ç‰¹å¾ (GNN)
        if (
            'node_features' in inputs and 
            'adjacency_matrix' in inputs and 
            # 'batch_user_indices_in_graph' in inputs and # GNN in this model takes full graph, then selects
            'graph' in self.model_config.enabled_modalities
        ): 
            # GNN processing needs to result in per-batch-item features
            # The current GNN (UserGNN) outputs features for ALL nodes in the graph.
            # We need to select the features for the users in the current batch.
            user_features_global = self.user_encoder(
                inputs['node_features'], 
                inputs['adjacency_matrix']
            ) # Shape: [num_total_graph_users, gnn_output_dim]

            if 'batch_user_indices_in_graph' in inputs: # This key is crucial for selecting batch-specific GNN features
                batch_indices = inputs['batch_user_indices_in_graph'] # [B]
                valid_mask = batch_indices >= 0
                
                gnn_output_dim = self.user_encoder.output_dim if hasattr(self.user_encoder, 'output_dim') else self.model_config.hidden_dim
                user_features_for_batch = torch.zeros(batch_size, gnn_output_dim, device=user_features_global.device)

                if torch.any(valid_mask):
                    valid_batch_indices_in_graph = batch_indices[valid_mask]
                    if valid_batch_indices_in_graph.max() < user_features_global.shape[0]:
                        selected_gnn_features = user_features_global[valid_batch_indices_in_graph]
                        user_features_for_batch[valid_mask] = selected_gnn_features
                    else:
                        # Log warning about out-of-bounds indices
                        pass 
                
                # modality_features.append(user_features_for_batch)
                # modality_names.append('graph')
                encoded_features_dict['graph'] = user_features_for_batch
                # print(f"[DEBUG MMA Detector] Added 'graph' features. Shape: {user_features_for_batch.shape}")
            else:
                # print("[DEBUG MMA Detector] 'batch_user_indices_in_graph' missing, cannot extract GNN features for batch.")
                # Fallback: add zeros or handle error, for now, 'graph' won't be in encoded_features_dict
                pass # Or add zeros: encoded_features_dict['graph'] = torch.zeros(...)

            
        # æ–‡æœ¬å†…å®¹ç‰¹å¾
        if 'text_content' in inputs and 'text' in self.model_config.enabled_modalities:
            text_features = self.text_encoder(inputs['text_content'])
            # modality_features.append(text_features)
            # modality_names.append('text')
            encoded_features_dict['text'] = text_features
            # print(f"[DEBUG MMA Detector] Added 'text' features. Shape: {text_features.shape}")
            
        # ç»“æ„åŒ–ç‰¹å¾
        if 'structured_features' in inputs and 'structured' in self.model_config.enabled_modalities:
            struct_features = self.encode_structured_features(inputs['structured_features'])
            # modality_features.append(struct_features)
            # modality_names.append('structured')
            encoded_features_dict['structured'] = struct_features
            # print(f"[DEBUG MMA Detector] Added 'structured' features. Shape: {struct_features.shape}")
        
        if not encoded_features_dict:
            # This means no modalities were enabled or no input provided for them.
            # Handle this gracefully: return zero logits/probabilities for the batch.
            # print("[DEBUG MMA Detector] Encoded features dictionary is empty!")
            dummy_logits = torch.zeros(batch_size, self.model_config.num_classes, device=inputs['behavior_sequences'].device if 'behavior_sequences' in inputs and inputs['behavior_sequences'].numel() > 0 else 'cpu')
            dummy_probs = F.softmax(dummy_logits, dim=1)
            return {
                'logits': dummy_logits,
                'probabilities': dummy_probs,
                'anomaly_scores': dummy_probs[:, 1] if self.model_config.num_classes > 1 else dummy_probs[:, 0],
                'confidence': torch.max(dummy_probs, dim=1)[0],
                'fused_features': torch.zeros(batch_size, self.model_config.hidden_dim, device=dummy_logits.device),
                'modality_features': {}
            }

        # 2. å¤šæ¨¡æ€èåˆ
        # if len(modality_features) > 1: # Old logic
        #     fusion_outputs = self.fusion_module(modality_features)
        # elif modality_features: # Old logic: only one modality
        #     fused_features = modality_features[0]
        #     fusion_outputs = { ... } # Mocked output
        # else: # No features, should be caught by the check above
            # pass

        # ğŸ”§ æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨æ¶ˆèå®éªŒä¸­ï¼‰
        if len(self.model_config.enabled_modalities) <= 2:  # æ¶ˆèå®éªŒé€šå¸¸æ˜¯å•æ¨¡æ€æˆ–åŒæ¨¡æ€
            logger = logging.getLogger(__name__)
            logger.debug(f"[MMA Forward] Enabled modalities: {self.model_config.enabled_modalities}")
            logger.debug(f"[MMA Forward] Actually encoded modalities: {list(encoded_features_dict.keys())}")
            for name, feat in encoded_features_dict.items():
                logger.debug(f"[MMA Forward] {name} features shape: {feat.shape}, mean: {feat.mean().item():.4f}, std: {feat.std().item():.4f}")

        fusion_outputs = self.fusion_module(encoded_features_dict) # Pass dictionary
        fused_features = fusion_outputs['fused_features']
        
        # ğŸ”§ æ·»åŠ èåˆåç‰¹å¾çš„è°ƒè¯•ä¿¡æ¯
        if len(self.model_config.enabled_modalities) <= 2:
            logger.debug(f"[MMA Forward] Fused features shape: {fused_features.shape}, mean: {fused_features.mean().item():.4f}, std: {fused_features.std().item():.4f}")
        
        # 3. å¼‚å¸¸æ£€æµ‹åˆ†ç±»
        logits = self.classification_head(fused_features)
        # print(f"[DEBUG MMA Detector] Inferred fused_features shape: {fused_features.shape}")
        # print(f"[DEBUG MMA Detector] Logits shape from ClassificationHead: {logits.shape}")
        probabilities = torch.softmax(logits, dim=1)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ probabilities çš„å½¢çŠ¶
        # print(f"[DEBUG MMA Detector] Probabilities shape: {probabilities.shape}")
        
        # å®‰å…¨åœ°è·å–å¼‚å¸¸åˆ†æ•°
        if probabilities.shape[1] > 1:
            anomaly_scores = probabilities[:, 1]
        else:
            # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰æ¦‚ç‡
            anomaly_scores = probabilities[:, 0]
            # print(f"[DEBUG MMA Detector] Warning: probabilities only has 1 class, using [:, 0]")
        
        confidence = torch.max(probabilities, dim=1)[0]
        
        # æ•´åˆè¾“å‡º
        outputs = {
            'logits': logits,
            'probabilities': probabilities,
            'anomaly_scores': anomaly_scores,
            'confidence': confidence,
            'fused_features': fused_features,
            'modality_features': fusion_outputs.get('modality_features_projected', {}) # Get projected features from fusion output
            # outputs['modality_features'] = { # Old way
            #     name: feat for name, feat in zip(modality_names, modality_features)
            # }
        }
        
        # æ·»åŠ èåˆæƒé‡ä¿¡æ¯ (CrossAttentionFusion might not return these in the same way)
        # if fusion_outputs.get('attention_weights') is not None:
        #     outputs['attention_weights'] = fusion_outputs['attention_weights']
        # if fusion_outputs.get('gate_weights') is not None:
        #     outputs['gate_weights'] = fusion_outputs['gate_weights']
            
        return outputs
    
    def predict_anomaly(self, inputs: Dict[str, Union[torch.Tensor, List]], 
                       threshold: float = 0.5) -> Dict[str, Union[torch.Tensor, List]]:
        """
        å¼‚å¸¸æ£€æµ‹é¢„æµ‹
        
        Args:
            inputs: è¾“å…¥æ•°æ®
            threshold: å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        self.eval()
        with torch.no_grad():
            outputs = self.forward(inputs)
            
            # å¼‚å¸¸åˆ¤æ–­
            anomaly_predictions = (outputs['anomaly_scores'] > threshold).long()
            
            # ç½®ä¿¡åº¦è¯„ä¼°
            probs = outputs['probabilities']
            confidence = torch.max(probs, dim=1)[0]
            
            return {
                'is_anomaly': anomaly_predictions,
                'anomaly_scores': outputs['anomaly_scores'],
                'confidence': confidence,
                'probabilities': outputs['probabilities'],
                'class_predictions': torch.argmax(outputs['logits'], dim=1)
            }
    
    def get_feature_importance(self) -> Dict[str, float]:
        """è·å–æ¨¡æ€é‡è¦æ€§"""
        importance = {}
        
        # å¦‚æœæœ‰èåˆæƒé‡ï¼Œä½¿ç”¨èåˆæƒé‡ä½œä¸ºé‡è¦æ€§
        if hasattr(self, '_last_fusion_weights'):
            weights = self._last_fusion_weights.mean(dim=0).cpu().numpy()
            modality_names = ['behavior', 'user_relations', 'text', 'structured']
            for i, name in enumerate(modality_names[:len(weights)]):
                importance[name] = float(weights[i])
        
        return importance

def test_multimodal_model():
    """æµ‹è¯•å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
    print("ğŸ§ª Testing Multi-modal Anomaly Detection Model...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 8
    
    inputs = {
        # è¡Œä¸ºåºåˆ—ï¼š[batch_size, seq_len, feature_dim]
        'behavior_sequences': torch.randn(batch_size, 32, 128),
        
        # ç”¨æˆ·å…³ç³»ï¼šèŠ‚ç‚¹ç‰¹å¾å’Œé‚»æ¥çŸ©é˜µ
        'node_features': torch.randn(50, 10),
        'adjacency_matrix': torch.randint(0, 2, (50, 50)).float(),
        
        # æ–‡æœ¬å†…å®¹
        'text_content': [
            f"This is email content {i} about system alerts and notifications."
            for i in range(batch_size)
        ],
        
        # ç»“æ„åŒ–ç‰¹å¾
        'structured_features': torch.randn(batch_size, 20)
    }
    
    print(f"  è¾“å…¥æ•°æ®å½¢çŠ¶:")
    print(f"    è¡Œä¸ºåºåˆ—: {inputs['behavior_sequences'].shape}")
    print(f"    èŠ‚ç‚¹ç‰¹å¾: {inputs['node_features'].shape}")
    print(f"    é‚»æ¥çŸ©é˜µ: {inputs['adjacency_matrix'].shape}")
    print(f"    æ–‡æœ¬æ•°é‡: {len(inputs['text_content'])}")
    print(f"    ç»“æ„åŒ–ç‰¹å¾: {inputs['structured_features'].shape}")
    
    # åˆ›å»ºæ¨¡å‹
    model = MultiModalAnomalyDetector(
        embed_dim=128,
        transformer_config={'input_dim': 128, 'hidden_dim': 128},
        gnn_config={'input_dim': 10, 'output_dim': 128},
        bert_config={'output_dim': 128},
        lgbm_config={'input_dim': 20, 'output_dim': 128},
        fusion_config={'embed_dim': 128}
    )
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(inputs)
    
    print(f"\n  è¾“å‡ºç»“æœ:")
    print(f"    é¢„æµ‹logits: {outputs['logits'].shape}")
    print(f"    æ¦‚ç‡åˆ†å¸ƒ: {outputs['probabilities'].shape}")
    print(f"    å¼‚å¸¸åˆ†æ•°: {outputs['anomaly_scores'].shape}")
    print(f"    ç½®ä¿¡åº¦: {outputs['confidence'].shape}")
    print(f"    èåˆç‰¹å¾: {outputs['fused_features'].shape}")
    
    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    print(f"    å¹³å‡å¼‚å¸¸åˆ†æ•°: {outputs['anomaly_scores'].mean().item():.4f}")
    print(f"    å¹³å‡ç½®ä¿¡åº¦: {outputs['confidence'].mean().item():.4f}")
    
    # æµ‹è¯•å¼‚å¸¸æ£€æµ‹
    anomaly_results = model.predict_anomaly(inputs, threshold=0.5)
    print(f"    æ£€æµ‹åˆ°å¼‚å¸¸æ•°é‡: {anomaly_results['is_anomaly'].sum().item()}/{batch_size}")
    
    # æµ‹è¯•æ¢¯åº¦
    model.train()
    outputs = model(inputs)
    loss = outputs['logits'].sum()
    loss.backward()
    
    print("  âœ… Multi-modal Model æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    test_multimodal_model() 