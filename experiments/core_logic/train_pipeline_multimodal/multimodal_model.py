#!/usr/bin/env python3
"""Multi-modal Anomaly Detection Model"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import sys
import os

# æ·»åŠ æ¨¡å‹è·¯å¾„
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..config import ModelConfig
    from ..models.base_model.transformer_encoder import TransformerEncoder
    from ..models.base_model.user_gnn import UserGNN
    from ..models.base_model.base_fusion import BaseFusion
    from ..models.base_model.head import ClassificationHead
    from ..models.text_encoder.bert_module import BERTTextEncoder
    from ..models.structure_encoder.lightgbm_branch import LightGBMBranch
    from ..models.fusion.attention_fusion import AttentionFusion
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œæ·»åŠ è·¯å¾„å¹¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    if parent_dir not in sys.path:
        sys.path.insert(0, parent_dir)
    
    from config import ModelConfig
    from models.base_model.transformer_encoder import TransformerEncoder
    from models.base_model.user_gnn import UserGNN
    from models.base_model.base_fusion import BaseFusion
    from models.base_model.head import ClassificationHead
    from models.text_encoder.bert_module import BERTTextEncoder
    from models.structure_encoder.lightgbm_branch import LightGBMBranch
    from models.fusion.attention_fusion import AttentionFusion

class MultiModalAnomalyDetector(nn.Module):
    """
    å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹
    
    æ•´åˆæ‰€æœ‰æ¨¡æ€ç¼–ç å™¨å’Œèåˆæœºåˆ¶ï¼Œç”¨äºç”¨æˆ·è¡Œä¸ºå¼‚å¸¸æ£€æµ‹
    """
    
    def __init__(
        self,
        model_config_obj: ModelConfig, # ä¸»é…ç½®å¯¹è±¡
        # å„ç»„ä»¶çš„é…ç½®å­—å…¸ï¼Œä¸»è¦ç”¨äºä¼ é€’åŠ¨æ€ç¡®å®šçš„input_dim
        transformer_config: Dict, 
        gnn_config: Dict,
        bert_config: Dict,
        lgbm_config: Dict,
        fusion_config: Dict,
        head_config: Dict,
        # embed_dim å’Œ dropout å¯ä»¥ä» model_config_obj è·å–ï¼Œè¿™é‡Œä½œä¸ºå‚æ•°æ˜¯å†—ä½™çš„
    ):
        super().__init__()
        
        self.model_config = model_config_obj # å­˜å‚¨ ModelConfig
        self.embed_dim = self.model_config.hidden_dim # ä¸»éšè—ç»´åº¦
        
        # 1. è¡Œä¸ºåºåˆ—ç¼–ç å™¨ï¼ˆTransformerï¼‰
        # ä½¿ç”¨ model_config_obj ä¸­çš„å‚æ•°ï¼Œå¹¶ä»ä¼ å…¥çš„ transformer_config æ›´æ–° input_dim
        _actual_transformer_config = {
            'input_dim': transformer_config['input_dim'], # æ¥è‡ª trainer
            'hidden_dim': self.model_config.hidden_dim,
            'num_heads': self.model_config.num_heads,
            'num_layers': self.model_config.num_layers
            # TransformerEncoder __init__ ä¸æ¥å— dropout, å…¶å†…éƒ¨ dropout å›ºå®š
        }
        self.behavior_encoder = TransformerEncoder(**_actual_transformer_config)
        
        # 2. ç”¨æˆ·å…³ç³»ç¼–ç å™¨ï¼ˆGNNï¼‰
        _actual_gnn_config = {
            'input_dim': gnn_config['input_dim'], # æ¥è‡ª trainer
            'hidden_dim': self.model_config.gnn_hidden_dim,
            'output_dim': self.model_config.hidden_dim, # GNN è¾“å‡ºå¯¹é½åˆ°ä¸» hidden_dim
            'num_layers': self.model_config.gnn_num_layers,
            'dropout': self.model_config.gnn_dropout
        }
        self.user_encoder = UserGNN(**_actual_gnn_config)
        
        # 3. æ–‡æœ¬å†…å®¹ç¼–ç å™¨ï¼ˆBERTï¼‰
        _actual_bert_config = {
            'bert_model_name': self.model_config.bert_model_name,
            'max_length': self.model_config.bert_max_length,
            'output_dim': self.model_config.hidden_dim, # BERT è¾“å‡ºå¯¹é½
            'dropout': self.model_config.dropout
            # input_dim for BERT is handled internally by tokenizer
        }
        self.text_encoder = BERTTextEncoder(**_actual_bert_config)
        
        # 4. ç»“æ„åŒ–ç‰¹å¾ç¼–ç å™¨ï¼ˆLightGBMï¼‰
        _actual_lgbm_config = {
            'input_dim': lgbm_config['input_dim'], # æ¥è‡ª trainer
            'output_dim': self.model_config.hidden_dim, # LGBM è¾“å‡ºå¯¹é½
            'dropout': self.model_config.dropout, # General dropout
            # Pass LightGBM specific params from ModelConfig
            'num_leaves': self.model_config.lgbm_num_leaves,
            'max_depth': self.model_config.lgbm_max_depth,
            'learning_rate': self.model_config.lgbm_learning_rate,
            'n_estimators': self.model_config.lgbm_n_estimators if hasattr(self.model_config, 'lgbm_n_estimators') else 100,
            'feature_fraction': self.model_config.lgbm_feature_fraction,
            'bagging_fraction': self.model_config.lgbm_bagging_fraction if hasattr(self.model_config, 'lgbm_bagging_fraction') else 0.8,
            'bagging_freq': self.model_config.lgbm_bagging_freq if hasattr(self.model_config, 'lgbm_bagging_freq') else 5
        }
        self.structure_encoder = LightGBMBranch(**_actual_lgbm_config)
        
        # 5. å¤šæ¨¡æ€èåˆå™¨
        # åŠ¨æ€æ„å»º input_dims_for_fusion based on enabled_modalities and their actual output dimensions
        self.fusion_input_dims = []
        if 'behavior' in self.model_config.enabled_modalities:
            self.fusion_input_dims.append(_actual_transformer_config['hidden_dim'])
        if 'graph' in self.model_config.enabled_modalities:
            self.fusion_input_dims.append(_actual_gnn_config['output_dim'])
        if 'text' in self.model_config.enabled_modalities:
            self.fusion_input_dims.append(_actual_bert_config['output_dim'])
        if 'structured' in self.model_config.enabled_modalities:
            self.fusion_input_dims.append(_actual_lgbm_config['output_dim'])
        
        # å†³å®šèåˆæ¨¡å—è¾“å‡ºç»´åº¦ (ä¹Ÿæ˜¯åˆ†ç±»å¤´è¾“å…¥ç»´åº¦)
        if len(self.fusion_input_dims) == 1:
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ¨¡æ€ï¼Œåˆ†ç±»å¤´çš„è¾“å…¥ç»´åº¦å°±æ˜¯è¿™ä¸ªæ¨¡æ€çš„è¾“å‡ºç»´åº¦
            effective_fusion_embed_dim = self.fusion_input_dims[0]
        elif hasattr(self.model_config, 'fusion_hidden_dim'):
            effective_fusion_embed_dim = self.model_config.fusion_hidden_dim
        else:
            effective_fusion_embed_dim = self.model_config.hidden_dim

        _actual_fusion_config = {
            'input_dims': self.fusion_input_dims,
            'embed_dim': effective_fusion_embed_dim, # ä½¿ç”¨ä¿®æ­£åçš„ç»´åº¦
            'dropout': self.model_config.dropout,
            'use_gating': True # Or derive from self.model_config.fusion_type
        }
        
        # --- DEBUG PRINTS ---
        # print(f"[DEBUG MultiModalAnomalyDetector] _actual_fusion_config: {_actual_fusion_config}")
        from ..models.fusion.attention_fusion import AttentionFusion # ç¡®ä¿å¯¼å…¥æœ€æ–°çš„
        # print(f"[DEBUG MultiModalAnomalyDetector] AttentionFusion class: {AttentionFusion}")
        # print(f"[DEBUG MultiModalAnomalyDetector] AttentionFusion __init__ signature: {AttentionFusion.__init__.__text_signature__ if hasattr(AttentionFusion.__init__, '__text_signature__') else 'not available'}")
        # --- END DEBUG PRINTS ---
        
        self.fusion_module = AttentionFusion(**_actual_fusion_config)
        
        # 6. åˆ†ç±»å¤´
        _actual_head_config = {
            'input_dim': effective_fusion_embed_dim, # Input to head is output of fusion / single modality
            'num_classes': self.model_config.num_classes,
            'dropout': self.model_config.head_dropout 
        }
        # ä»ModelConfigè·å–å¯é€‰å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self.model_config, 'head_hidden_dims'):
            _actual_head_config['hidden_dims'] = self.model_config.head_hidden_dims
        if hasattr(self.model_config, 'head_activation'):
            _actual_head_config['activation'] = self.model_config.head_activation

        # print(f"[DEBUG MultiModalAnomalyDetector] Fusion module embed_dim: {self.fusion_module.embed_dim}")
        # print(f"[DEBUG MultiModalAnomalyDetector] ClassificationHead _actual_head_config: {_actual_head_config}")

        self.classification_head = ClassificationHead(**_actual_head_config)
        
        self._feature_cache = {}
        
    def encode_behavior_sequences(self, sequences: torch.Tensor) -> torch.Tensor:
        """ç¼–ç è¡Œä¸ºåºåˆ—"""
        return self.behavior_encoder(sequences)
    
    def encode_user_relations(self, node_features: torch.Tensor, 
                            adjacency_matrix: torch.Tensor) -> torch.Tensor:
        """ç¼–ç ç”¨æˆ·å…³ç³»"""
        return self.user_encoder(node_features, adjacency_matrix)
    
    def encode_text_content(self, texts: List[str]) -> torch.Tensor:
        """ç¼–ç æ–‡æœ¬å†…å®¹"""
        return self.text_encoder(texts)
    
    def encode_structured_features(self, features: Union[torch.Tensor, dict]) -> torch.Tensor:
        """ç¼–ç ç»“æ„åŒ–ç‰¹å¾"""
        return self.structure_encoder(features)
    
    def forward(self, inputs: Dict[str, Union[torch.Tensor, List]]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: è¾“å…¥å­—å…¸ï¼ŒåŒ…å«å„æ¨¡æ€æ•°æ®
                - 'behavior_sequences': [batch_size, seq_len, feature_dim] è¡Œä¸ºåºåˆ—
                - 'node_features': [num_nodes, node_feat_dim] èŠ‚ç‚¹ç‰¹å¾
                - 'adjacency_matrix': [num_nodes, num_nodes] é‚»æ¥çŸ©é˜µ
                - 'text_content': List[str] æ–‡æœ¬å†…å®¹
                - 'structured_features': ç»“æ„åŒ–ç‰¹å¾ï¼ˆtensoræˆ–dictï¼‰
                
        Returns:
            è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«é¢„æµ‹ç»“æœå’Œä¸­é—´ç‰¹å¾
        """
        batch_size = inputs['behavior_sequences'].shape[0]
        
        # 1. å„æ¨¡æ€ç‰¹å¾ç¼–ç 
        modality_features = []
        modality_names = []
        
        # print(f"[DEBUG MMA Detector] Initial enabled_modalities: {self.model_config.enabled_modalities}")

        # è¡Œä¸ºåºåˆ—ç‰¹å¾
        if 'behavior_sequences' in inputs and 'behavior' in self.model_config.enabled_modalities:
            behavior_features = self.encode_behavior_sequences(inputs['behavior_sequences'])
            modality_features.append(behavior_features)
            modality_names.append('behavior')
            # print(f"[DEBUG MMA Detector] Added 'behavior' features. Current modality_features length: {len(modality_features)}")
            
        # ç”¨æˆ·å…³ç³»ç‰¹å¾ (GNN)
        if ('node_features' in inputs and 
            'adjacency_matrix' in inputs and 
            'batch_user_indices_in_graph' in inputs and 
            'graph' in self.model_config.enabled_modalities): # ä½¿ç”¨ self.model_config
            
            user_features_global = self.user_encoder(
                inputs['node_features'], 
                inputs['adjacency_matrix']
            ) # Shape: [num_total_graph_users, gnn_output_dim]

            batch_indices = inputs['batch_user_indices_in_graph']
            valid_mask = batch_indices >= 0
            
            # Initialize batch GNN features with zeros
            # Ensure self.user_encoder has output_dim attribute or get it from config
            gnn_output_dim = self.user_encoder.output_dim if hasattr(self.user_encoder, 'output_dim') else self.config.model.hidden_dim # Fallback
            user_features_for_batch = torch.zeros(batch_size, gnn_output_dim, device=user_features_global.device)

            if torch.any(valid_mask):
                valid_batch_indices_in_graph = batch_indices[valid_mask]
                # Ensure indices are within bounds for user_features_global
                if valid_batch_indices_in_graph.max() < user_features_global.shape[0]:
                    selected_gnn_features = user_features_global[valid_batch_indices_in_graph]
                    # Place selected features into the correct batch positions
                    user_features_for_batch[valid_mask] = selected_gnn_features
                else:
                    # This case should ideally not happen if data pipeline and indexing are correct
                    # Or, log a warning
                    pass # Or log: print("Warning: GNN batch_user_indices_in_graph out of bounds")
            
            modality_features.append(user_features_for_batch)
            modality_names.append('graph')
            # print(f"[DEBUG MMA Detector] Added 'graph' features. Current modality_features length: {len(modality_features)}")
            
        # æ–‡æœ¬å†…å®¹ç‰¹å¾
        if 'text_content' in inputs and 'text' in self.model_config.enabled_modalities: # ä½¿ç”¨ self.model_config
            text_features = self.text_encoder(inputs['text_content'])
            modality_features.append(text_features)
            modality_names.append('text')
            # print(f"[DEBUG MMA Detector] Added 'text' features. Current modality_features length: {len(modality_features)}")
            
        # ç»“æ„åŒ–ç‰¹å¾
        if 'structured_features' in inputs and 'structured' in self.model_config.enabled_modalities: # ä½¿ç”¨ self.model_config
            struct_features = self.encode_structured_features(inputs['structured_features'])
            modality_features.append(struct_features)
            modality_names.append('structured')
            # print(f"[DEBUG MMA Detector] Added 'structured' features. Current modality_features length: {len(modality_features)}")
        
        # print(f"[DEBUG MMA Detector] Final modality_features length: {len(modality_features)}")
        if not modality_features:
            # print("[DEBUG MMA Detector] ERROR: modality_features list is empty!")
            # Potentially raise an error or return dummy data to avoid further crashes
            # This should not happen if config and inputs are correct
            # For now, let it proceed to see where it crashes, but this is a critical check.
            pass # æ·»åŠ ä¸€ä¸ª pass è¯­å¥ä»¥ä¿æŒå—çš„æœ‰æ•ˆæ€§

        # 2. å¤šæ¨¡æ€èåˆ
        if len(modality_features) > 1:
            fusion_outputs = self.fusion_module(modality_features)
            fused_features = fusion_outputs['fused_features']
        else:
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ¨¡æ€ï¼Œç›´æ¥ä½¿ç”¨è¯¥æ¨¡æ€ç‰¹å¾
            fused_features = modality_features[0]
            fusion_outputs = {
                'fused_features': fused_features,
                'attention_weights': None,
                'gate_weights': None
            }
        
        # 3. å¼‚å¸¸æ£€æµ‹åˆ†ç±»
        logits = self.classification_head(fused_features)
        # print(f"[DEBUG MMA Detector] Inferred fused_features shape: {fused_features.shape}")
        # print(f"[DEBUG MMA Detector] Logits shape from ClassificationHead: {logits.shape}")
        probabilities = torch.softmax(logits, dim=1)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ probabilities çš„å½¢çŠ¶
        # print(f"[DEBUG MMA Detector] Probabilities shape: {probabilities.shape}")
        
        # å®‰å…¨åœ°è·å–å¼‚å¸¸åˆ†æ•°
        if probabilities.shape[1] > 1:
            anomaly_scores = probabilities[:, 1]
        else:
            # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰æ¦‚ç‡
            anomaly_scores = probabilities[:, 0]
            # print(f"[DEBUG MMA Detector] Warning: probabilities only has 1 class, using [:, 0]")
        
        confidence = torch.max(probabilities, dim=1)[0]
        
        # æ•´åˆè¾“å‡º
        outputs = {
            'logits': logits,
            'probabilities': probabilities,
            'anomaly_scores': anomaly_scores,
            'confidence': confidence,
            'fused_features': fused_features,
            'modality_features': {
                name: feat for name, feat in zip(modality_names, modality_features)
            }
        }
        
        # æ·»åŠ èåˆæƒé‡ä¿¡æ¯
        if fusion_outputs.get('attention_weights') is not None:
            outputs['attention_weights'] = fusion_outputs['attention_weights']
        if fusion_outputs.get('gate_weights') is not None:
            outputs['gate_weights'] = fusion_outputs['gate_weights']
            
        return outputs
    
    def predict_anomaly(self, inputs: Dict[str, Union[torch.Tensor, List]], 
                       threshold: float = 0.5) -> Dict[str, Union[torch.Tensor, List]]:
        """
        å¼‚å¸¸æ£€æµ‹é¢„æµ‹
        
        Args:
            inputs: è¾“å…¥æ•°æ®
            threshold: å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        self.eval()
        with torch.no_grad():
            outputs = self.forward(inputs)
            
            # å¼‚å¸¸åˆ¤æ–­
            anomaly_predictions = (outputs['anomaly_scores'] > threshold).long()
            
            # ç½®ä¿¡åº¦è¯„ä¼°
            probs = outputs['probabilities']
            confidence = torch.max(probs, dim=1)[0]
            
            return {
                'is_anomaly': anomaly_predictions,
                'anomaly_scores': outputs['anomaly_scores'],
                'confidence': confidence,
                'probabilities': outputs['probabilities'],
                'class_predictions': torch.argmax(outputs['logits'], dim=1)
            }
    
    def get_feature_importance(self) -> Dict[str, float]:
        """è·å–æ¨¡æ€é‡è¦æ€§"""
        importance = {}
        
        # å¦‚æœæœ‰èåˆæƒé‡ï¼Œä½¿ç”¨èåˆæƒé‡ä½œä¸ºé‡è¦æ€§
        if hasattr(self, '_last_fusion_weights'):
            weights = self._last_fusion_weights.mean(dim=0).cpu().numpy()
            modality_names = ['behavior', 'user_relations', 'text', 'structured']
            for i, name in enumerate(modality_names[:len(weights)]):
                importance[name] = float(weights[i])
        
        return importance

def test_multimodal_model():
    """æµ‹è¯•å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
    print("ğŸ§ª Testing Multi-modal Anomaly Detection Model...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 8
    
    inputs = {
        # è¡Œä¸ºåºåˆ—ï¼š[batch_size, seq_len, feature_dim]
        'behavior_sequences': torch.randn(batch_size, 32, 128),
        
        # ç”¨æˆ·å…³ç³»ï¼šèŠ‚ç‚¹ç‰¹å¾å’Œé‚»æ¥çŸ©é˜µ
        'node_features': torch.randn(50, 10),
        'adjacency_matrix': torch.randint(0, 2, (50, 50)).float(),
        
        # æ–‡æœ¬å†…å®¹
        'text_content': [
            f"This is email content {i} about system alerts and notifications."
            for i in range(batch_size)
        ],
        
        # ç»“æ„åŒ–ç‰¹å¾
        'structured_features': torch.randn(batch_size, 20)
    }
    
    print(f"  è¾“å…¥æ•°æ®å½¢çŠ¶:")
    print(f"    è¡Œä¸ºåºåˆ—: {inputs['behavior_sequences'].shape}")
    print(f"    èŠ‚ç‚¹ç‰¹å¾: {inputs['node_features'].shape}")
    print(f"    é‚»æ¥çŸ©é˜µ: {inputs['adjacency_matrix'].shape}")
    print(f"    æ–‡æœ¬æ•°é‡: {len(inputs['text_content'])}")
    print(f"    ç»“æ„åŒ–ç‰¹å¾: {inputs['structured_features'].shape}")
    
    # åˆ›å»ºæ¨¡å‹
    model = MultiModalAnomalyDetector(
        embed_dim=128,
        transformer_config={'input_dim': 128, 'hidden_dim': 128},
        gnn_config={'input_dim': 10, 'output_dim': 128},
        bert_config={'output_dim': 128},
        lgbm_config={'input_dim': 20, 'output_dim': 128},
        fusion_config={'embed_dim': 128}
    )
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(inputs)
    
    print(f"\n  è¾“å‡ºç»“æœ:")
    print(f"    é¢„æµ‹logits: {outputs['logits'].shape}")
    print(f"    æ¦‚ç‡åˆ†å¸ƒ: {outputs['probabilities'].shape}")
    print(f"    å¼‚å¸¸åˆ†æ•°: {outputs['anomaly_scores'].shape}")
    print(f"    ç½®ä¿¡åº¦: {outputs['confidence'].shape}")
    print(f"    èåˆç‰¹å¾: {outputs['fused_features'].shape}")
    
    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    print(f"    å¹³å‡å¼‚å¸¸åˆ†æ•°: {outputs['anomaly_scores'].mean().item():.4f}")
    print(f"    å¹³å‡ç½®ä¿¡åº¦: {outputs['confidence'].mean().item():.4f}")
    
    # æµ‹è¯•å¼‚å¸¸æ£€æµ‹
    anomaly_results = model.predict_anomaly(inputs, threshold=0.5)
    print(f"    æ£€æµ‹åˆ°å¼‚å¸¸æ•°é‡: {anomaly_results['is_anomaly'].sum().item()}/{batch_size}")
    
    # æµ‹è¯•æ¢¯åº¦
    model.train()
    outputs = model(inputs)
    loss = outputs['logits'].sum()
    loss.backward()
    
    print("  âœ… Multi-modal Model æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    test_multimodal_model() 