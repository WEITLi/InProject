#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€å†…éƒ¨å¨èƒæ£€æµ‹ç³»ç»Ÿ - ä¸»å®éªŒæ§åˆ¶è„šæœ¬
Main Experiment Controller for Multimodal Internal Threat Detection System

æ”¯æŒçš„å®éªŒç±»å‹ï¼š
1. baseline - ä¼ ç»ŸMLæ–¹æ³•å¯¹æ¯”å®éªŒ (RandomForest vs XGBoost vs å¤šæ¨¡æ€)
2. tune - è¶…å‚æ•°ä¼˜åŒ–å®éªŒ (ä½¿ç”¨Optuna)
3. ablation - æ¶ˆèå®éªŒ (ä¸åŒæ¨¡æ€ç»„åˆ)
4. imbalance - æ•°æ®ä¸å¹³è¡¡é€‚åº”æ€§å®éªŒ
5. realtime - å®æ—¶æ£€æµ‹å®éªŒ (å¾…å®ç°)
6. generalization - æ¨¡å‹æ³›åŒ–èƒ½åŠ›è¯„ä¼°å®éªŒ
"""

import os
import sys
import copy

# --- Start of sys.path modification for robust imports ---
current_script_dir = os.path.dirname(os.path.abspath(__file__)) # .../experiments
parent_dir = os.path.dirname(current_script_dir) # .../InProject

# Add parent_dir to sys.path so 'experiments' can be treated as a package
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Clean up: remove current_script_dir itself if it was added, to avoid confusion
if current_script_dir in sys.path:
    sys.path.remove(current_script_dir)
# --- End of sys.path modification ---

import time
import argparse
import logging
import json
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import numpy as np
import pandas as pd #ç¡®ä¿å¯¼å…¥pandas

# å¯¼å…¥æ ¸å¿ƒæ¨¡å— (relative to parent_dir)
try:
    try:
        from experiments.core_logic.config import Config, ModelConfig, TrainingConfig, DataConfig
        from experiments.core_logic.multimodal_pipeline import MultiModalDataPipeline
        from experiments.core_logic.train_pipeline_multimodal import MultiModalTrainer
        # from experiments.core_logic.evaluation_utils import ExperimentEvaluator
    except ImportError as e:
        # print(f"âš ï¸ æ ¸å¿ƒæ¨¡å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼: {e}")
        class Config: 
            def __init__(self):
                self.data = type('DataConfig', (), { 'max_users': 20, 'max_weeks': 2, 'sample_ratio': 1.0, 'sequence_length': 10, 'num_cores': 1, 'data_version': 'mock', 'feature_dim': 8 })()
                self.training = type('TrainingConfig', (), { 'epochs': 1, 'batch_size': 4, 'learning_rate': 0.001, 'device': 'cpu' })()
                self.model = type('ModelConfig', (), { 'hidden_dim': 32, 'num_heads': 2, 'num_layers': 1, 'dropout': 0.1, 'enabled_modalities': ['behavior', 'structured'], 'sequence_length': 10 })()
                self.n_trials = 2
                self.seed = 42
        
        class MultiModalDataPipeline:
            def __init__(self, config): self.config = config
            def run_base_feature_extraction(self, **kwargs): print("Mock: Running base feature extraction...")
            def prepare_training_data(self, **kwargs): 
                print("Mock: Preparing training data for baseline...")
                num_samples = self.config.data.max_users
                if num_samples >= 2:
                    labels = [0]*(num_samples // 2) + [1]*(num_samples - num_samples // 2)
                elif num_samples == 1:
                    labels = [0]
                else:
                    labels = []
                np.random.shuffle(labels)
                print(f"Mock labels generated: {labels} (Total: {len(labels)}, Users: {num_samples})")
                return {
                    'labels': np.array(labels),
                    'users': [f'mock_user_{i}' for i in range(num_samples)],
                    'behavior_sequences': np.random.rand(num_samples, self.config.model.sequence_length, 8) if num_samples > 0 else np.array([]),
                    'structured_features': np.random.rand(num_samples, 5) if num_samples > 0 else np.array([]),
                    'node_features': np.random.rand(num_samples, 10) if num_samples > 0 else np.array([]),
                    'text_content': [f"mock text {i}" for i in range(num_samples)] if num_samples > 0 else []
                    }
        
        class MultiModalTrainer:
            def __init__(self, config, output_dir): 
                self.config = config
                self.output_dir = output_dir
                self.train_history = {'val_f1': [0.5+np.random.rand()*0.1], 'val_auc': [0.6+np.random.rand()*0.1], 'train_loss': [0.5], 'val_loss': [0.5]}
            def train(self, data): 
                print("Mock: Training MultiModal model...")
                num_test_samples = 0
                if 'labels' in data and hasattr(data['labels'], '__len__'):
                    num_test_samples = len(data['labels']) // 5 
                    if num_test_samples == 0 and len(data['labels']) > 0:
                        num_test_samples = 1
                
                mock_y_true = np.random.randint(0, 2, num_test_samples) if num_test_samples > 0 else np.array([])
                # Ensure mock_y_pred_proba has shape (num_test_samples, 2) for binary classification
                mock_y_pred_proba = np.random.rand(num_test_samples, 2) if num_test_samples > 0 else np.array([])
                if num_test_samples > 0 and mock_y_pred_proba.shape[0] > 0 : # Normalize probabilities to sum to 1 per sample
                    mock_y_pred_proba = mock_y_pred_proba / np.sum(mock_y_pred_proba, axis=1, keepdims=True)


                mock_test_metrics = {
                    'f1': 0.55 + np.random.rand() * 0.1 if num_test_samples > 0 else 0.0,
                    'auc': 0.65 + np.random.rand() * 0.1 if num_test_samples > 0 else 0.0,
                    'precision': 0.5 + np.random.rand() * 0.1 if num_test_samples > 0 else 0.0,
                    'recall': 0.6 + np.random.rand() * 0.1 if num_test_samples > 0 else 0.0,
                    'accuracy': 0.6 + np.random.rand() * 0.1 if num_test_samples > 0 else 0.0,
                    'fpr': 0.2 + np.random.rand() * 0.1 if num_test_samples > 0 else 0.0, # Mock FPR
                    'y_true': mock_y_true,
                    'y_pred_proba': mock_y_pred_proba
                }
                return self, mock_test_metrics
    
    # å¯¼å…¥å·¥å…·æ¨¡å— (relative to parent_dir)
    from experiments.utils.wandb_utils import init_wandb
    from experiments.utils.baseline_models import BaselineModelTrainer, run_baseline_comparison
    from experiments.utils.improved_baseline_models import run_improved_baseline_comparison  # æ–°å¢æ”¹è¿›ç‰ˆbaseline
    from experiments.utils.imbalance_utils import run_imbalance_experiment as utils_run_imbalance_experiment, ImbalanceHandler
    from experiments.utils.optuna_tuning import run_optuna_tuning, get_multimodal_search_space
    
except ImportError as e:
    print(f"âŒ æœ€ç»ˆå¯¼å…¥æ¨¡å—å¤±è´¥: {e}. è¯·æ£€æŸ¥PYTHONPATHå’Œè„šæœ¬ä½ç½®ã€‚")
    print("   PYTHONPATH: ", os.environ.get('PYTHONPATH'))
    print("   å½“å‰ sys.path:")
    for i, p in enumerate(sys.path):
        print(f"     {i}: {p}")
    print(f"   é¢„æœŸçˆ¶ç›®å½• (InProject): {parent_dir}")
    print(f"   å½“å‰è„šæœ¬ç›®å½• (experiments): {current_script_dir}")
    sys.exit(1)

def setup_logging(output_dir: str, experiment_name: str) -> logging.Logger:
    """
    è®¾ç½®æ—¥å¿—é…ç½®
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        experiment_name: å®éªŒåç§°
        
    Returns:
        é…ç½®å¥½çš„logger
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = os.path.join(output_dir, f"{experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger('MainExperiment')
    logger.info(f"å®éªŒæ—¥å¿—å·²é…ç½®: {log_file}")
    
    return logger

def load_config(config_file: Optional[str] = None, **kwargs) -> Config:
    """
    åŠ è½½å®éªŒé…ç½®
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„ (JSON/YAML)
        **kwargs: å‘½ä»¤è¡Œå‚æ•°è¦†ç›– (é€šå¸¸æ¥è‡ª argparse)
        
    Returns:
        é…ç½®å¯¹è±¡
    """
    # åˆ›å»ºé»˜è®¤é…ç½®
    config = Config()
    
    # argparse çš„é»˜è®¤å€¼ï¼Œç”¨äºåˆ¤æ–­å‘½ä»¤è¡Œå‚æ•°æ˜¯å¦è¢«ç”¨æˆ·æ˜¾å¼è®¾ç½®
    # æ³¨æ„ï¼šå¦‚æœ argparse çš„é»˜è®¤å€¼æ”¹å˜ï¼Œè¿™é‡Œä¹Ÿéœ€è¦æ›´æ–°
    argparse_defaults = {
        'max_users': 100,
        'data_version': 'r4.2',
        'sample_ratio': 1.0,
        'epochs': 3,
        'batch_size': 32,
        'learning_rate': 0.001,
        'device': 'auto',
        'hidden_dim': 256,
        'num_heads': 8,
        'num_layers': 6,
        'n_trials': 20,
        'num_cores': 8,
        'seed': 42,
        'use_improved_baseline': False,
        'baseline_cv_folds': 5
    }

    file_values_loaded = {}

    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½å¹¶åˆå¹¶
    if config_file and os.path.exists(config_file):
        print(f"ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            if config_file.endswith('.json'):
                file_config = json.load(f)
            elif config_file.endswith( ('.yaml', '.yml')):
                file_config = yaml.safe_load(f)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {config_file}")
        
        # åˆå¹¶é…ç½®
        for section_name, section_values in file_config.items():
            if hasattr(config, section_name) and isinstance(section_values, dict):
                section_obj = getattr(config, section_name)
                for key, value in section_values.items():
                    if hasattr(section_obj, key):
                        setattr(section_obj, key, value)
                        file_values_loaded[key] = value # è®°å½•ä»æ–‡ä»¶åŠ è½½çš„å€¼
                # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯trainingé…ç½®ä¸”è®¾ç½®äº†epochsï¼ŒåŒæ­¥num_epochs
                if section_name == 'training' and hasattr(section_obj, '__post_init__'):
                    section_obj.__post_init__()
            elif section_name in ['n_trials', 'seed', 'num_workers']: # å¤„ç†ç›´æ¥åœ¨Configæ ¹ä¸Šçš„å±æ€§
                 if hasattr(config, section_name):
                    setattr(config, section_name, section_values)
                    file_values_loaded[section_name] = section_values # åŒ…æ‹¬ num_workers
            elif key in ['use_improved_baseline', 'baseline_cv_folds']: # æ”¹è¿›ç‰ˆbaselineå‚æ•°
                target_section_obj = config
            else:
                # å¯¹äºé…ç½®æ–‡ä»¶ä¸­å­˜åœ¨ï¼Œä½†Configç±»ä¸­å®Œå…¨æ²¡æœ‰å¯¹åº”å±æ€§æˆ–èŠ‚çš„é”®ï¼Œå¯ä»¥é€‰æ‹©å¿½ç•¥æˆ–è­¦å‘Š
                pass

    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    for key, value_from_cmd in kwargs.items():
        if value_from_cmd is not None:
            is_default_argparse_value = (key in argparse_defaults and value_from_cmd == argparse_defaults[key])
            
            target_section_obj = None
            target_key_in_section = key

            if key in ['max_users', 'data_version', 'feature_dim', 'sample_ratio', 'sequence_length']: # num_cores is already in data section
                target_section_obj = config.data
            elif key in ['epochs', 'batch_size', 'learning_rate', 'device', 'test_split', 'val_split']:
                target_section_obj = config.training
            elif key in ['hidden_dim', 'num_heads', 'num_layers', 'dropout', 'enabled_modalities', 
                         'gnn_hidden_dim', 'gnn_num_layers', 'gnn_dropout', 
                         'bert_model_name', 'bert_max_length', 
                         'lgbm_num_leaves', 'lgbm_max_depth', 'lgbm_learning_rate', 'lgbm_feature_fraction',
                         'fusion_type', 'num_classes', 'head_dropout']:
                target_section_obj = config.model
            elif key in ['n_trials', 'seed', 'num_workers']: # ç›´æ¥åœ¨Configæ ¹ä¸Šçš„å±æ€§ï¼ŒåŒ…æ‹¬ num_workers
                target_section_obj = config
            elif key in ['use_improved_baseline', 'baseline_cv_folds']: # æ”¹è¿›ç‰ˆbaselineå‚æ•°
                target_section_obj = config
            else:
                if hasattr(config, key):
                    if not (is_default_argparse_value and key in file_values_loaded):
                        setattr(config, key, value_from_cmd)
                continue

            if target_section_obj is not None and hasattr(target_section_obj, target_key_in_section):
                # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æ˜¯é»˜è®¤å€¼ ä¸” æ–‡ä»¶ä¸­å·²åŠ è½½è¯¥å€¼ï¼Œåˆ™ä¸è¦†ç›–
                if not (is_default_argparse_value and target_key_in_section in file_values_loaded):
                    setattr(target_section_obj, target_key_in_section, value_from_cmd)
                    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœè®¾ç½®äº†epochsï¼ŒåŒæ­¥num_epochs
                    if target_key_in_section == 'epochs' and hasattr(target_section_obj, '__post_init__'):
                        target_section_obj.__post_init__()
            elif hasattr(config, key): # å†æ¬¡æ£€æŸ¥æ ¹çº§åˆ«ï¼Œä»¥é˜²ä¸Šé¢æœªåŒ¹é…ä½†æ ¹çº§åˆ«å­˜åœ¨
                if not (is_default_argparse_value and key in file_values_loaded):
                     setattr(config, key, value_from_cmd)
    
    # data.num_cores çš„ç‰¹æ®Šå¤„ç†ï¼šç¡®ä¿å‘½ä»¤è¡Œ --num_cores ä¼˜å…ˆäºæ–‡ä»¶ä¸­çš„ data.num_cores
    # å¹¶ç¡®ä¿ config.num_cores (ç”¨äºDataLoader) ä¸ config.data.num_cores (ç”¨äºæ•°æ®æµæ°´çº¿) å¯ä»¥ç‹¬ç«‹è®¾ç½®æˆ–åŒæ­¥
    # å½“å‰çš„å‘½ä»¤è¡Œ --num_cores ä¼šé€šè¿‡ä¸Šé¢çš„é€»è¾‘ç›´æ¥è®¾ç½®åˆ° config.num_workers (å¦‚æœ Config ç±»æœ‰ num_workers å±æ€§)
    # æˆ– config.num_cores (å¦‚æœ Config ç±»æœ‰ num_cores å±æ€§ä¸”å®ƒè¢«æ˜ å°„åˆ°é‚£é‡Œ)ã€‚

    # å½“å‰é€»è¾‘ä¼šå°† --num_cores (å‘½ä»¤è¡Œ) è®¾ç½®åˆ° config.num_cores (æ ¹çº§åˆ«)
    # å¦‚æœYAMLä¸­ data: num_cores: X å­˜åœ¨, å®ƒä¼šè®¾ç½®åˆ° config.data.num_cores
    # å¦‚æœYAMLä¸­ num_workers: Y å­˜åœ¨, å®ƒä¼šè®¾ç½®åˆ° config.num_workers (æ ¹çº§åˆ«)
    
    # ç¡®ä¿ config.data.num_cores å¯ä»¥è¢«å‘½ä»¤è¡Œ --num_cores è¦†ç›–ï¼Œå¦‚æœç”¨æˆ·æœŸæœ›è¿™ç§è¡Œä¸º
    # é‰´äº `--num_cores` çš„å¸®åŠ©æ–‡æœ¬æ˜¯ "CPUæ ¸å¿ƒæ•°"ï¼Œå®ƒå¯èƒ½åŒæ—¶æŒ‡ä»£æ•°æ®å¤„ç†å’Œæ•°æ®åŠ è½½
    # è¿™é‡Œæˆ‘ä»¬å‡è®¾å‘½ä»¤è¡Œçš„ --num_cores ä¸»è¦ç›®æ ‡æ˜¯ config.num_cores (æˆ–æˆ‘ä»¬å°†å…¶è§†ä¸º config.num_workers)
    # è€Œæ–‡ä»¶ä¸­çš„ data.num_cores æ§åˆ¶æ•°æ®é¢„å¤„ç†çš„æ ¸å¿ƒæ•°ã€‚

    # å¦‚æœæ‚¨å¸Œæœ›å‘½ä»¤è¡Œçš„ --num_cores åŒæ—¶è¦†ç›– config.num_workers å’Œ config.data.num_cores:
    # if 'num_cores' in kwargs and kwargs['num_cores'] is not None:
    #     num_cores_cmd_val = kwargs['num_cores']
    #     is_default_nc = ('num_cores' in argparse_defaults and num_cores_cmd_val == argparse_defaults['num_cores'])
    #     if not (is_default_nc and 'num_cores' in file_values_loaded): # æ£€æŸ¥æ ¹ num_cores
    #         config.num_workers = num_cores_cmd_val # å‡è®¾ config.num_workers æ˜¯DataLoaderç”¨çš„
    #     if not (is_default_nc and 'num_cores' in file_values_loaded.get('data', {})):
    #         config.data.num_cores = num_cores_cmd_val # æ•°æ®é¢„å¤„ç†ç”¨çš„

    # ç¡®ä¿ config.num_workers (DataLoaderç”¨çš„) å’Œ config.data.num_cores (Pipelineç”¨çš„) è¢«æ­£ç¡®è®¾ç½®
    # å‘½ä»¤è¡Œ --num_cores åº”è¯¥æ›´æ–° config.num_workers (å‡è®¾Configç±»ä¸­æœ‰æ­¤å±æ€§ for DataLoader)
    # YAML ä¸­æ ¹çº§åˆ«çš„ num_workers æ›´æ–° config.num_workers
    # YAML ä¸­ data.num_cores æ›´æ–° config.data.num_cores

    # æ•´ç†ï¼š
    # 1. YAML æ ¹ num_workers -> config.num_workers (å·²ç”±ä¸Šé¢ä»£ç å¤„ç†)
    # 2. YAML data.num_cores -> config.data.num_cores (å·²ç”±ä¸Šé¢ä»£ç å¤„ç†)
    # 3. å‘½ä»¤è¡Œ --num_cores -> config.num_workers (å¦‚æœ Config é¡¶çº§å±æ€§æ˜¯ num_workers)
    #                         æˆ– config.num_cores (å¦‚æœ Config é¡¶çº§å±æ€§æ˜¯ num_cores)
    # å‡è®¾æˆ‘ä»¬å¸Œæœ› `--num_cores` æ›´æ–°ç”¨äº DataLoader çš„ worker æ•°é‡ï¼Œå³ `config.num_workers`ã€‚
    # è€Œ `config.data.num_cores` ç”¨äºæ•°æ®æµæ°´çº¿ä¸­çš„å¹¶è¡Œå¤„ç†ã€‚

    # å¦‚æœ Config ç±»ç›´æ¥æœ‰ num_workers å±æ€§ (æ¨èç”¨äº DataLoader)
    if 'num_cores' in kwargs and hasattr(config, 'num_workers'):
        cmd_val = kwargs['num_cores']
        if cmd_val is not None:
            is_default_argparse_val_for_nc = ('num_cores' in argparse_defaults and cmd_val == argparse_defaults['num_cores'])
            if not (is_default_argparse_val_for_nc and 'num_workers' in file_values_loaded): # æ¯”è¾ƒæ—¶ç”¨ num_workers å› ä¸ºè¿™æ˜¯ç›®æ ‡
                config.num_workers = cmd_val
    
    # å¦‚æœ Config ç±»ç›´æ¥æœ‰ num_cores å±æ€§ (ä½œä¸º DataLoader worker æ•°çš„æ—§æ–¹å¼æˆ–é€šç”¨CPUæ•°)
    # å¹¶ä¸”ä¹Ÿå¸Œæœ›å‘½ä»¤è¡Œ --num_cores æ›´æ–°å®ƒ (è¦†ç›–æ–‡ä»¶ä¸­çš„æ ¹çº§åˆ« num_cores)
    elif 'num_cores' in kwargs and hasattr(config, 'num_cores'):
        cmd_val = kwargs['num_cores']
        if cmd_val is not None:
            is_default_argparse_val_for_nc = ('num_cores' in argparse_defaults and cmd_val == argparse_defaults['num_cores'])
            if not (is_default_argparse_val_for_nc and 'num_cores' in file_values_loaded): # æ¯”è¾ƒæ—¶ç”¨ num_cores
                config.num_cores = cmd_val

    return config

def run_baseline_experiment(config: Config, output_dir: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    è¿è¡ŒåŸºçº¿æ–¹æ³•å¯¹æ¯”å®éªŒ
    æ¯”è¾ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ä¸å¤šæ¨¡æ€æ¨¡å‹çš„æ£€æµ‹æ€§èƒ½
    
    Args:
        config: å®éªŒé…ç½®
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœ
    """
    logger.info("ğŸ”¬ å¼€å§‹åŸºçº¿æ–¹æ³•å¯¹æ¯”å®éªŒ")
    
    # åˆå§‹åŒ–WandB
    wandb_logger = init_wandb(
        project_name="threat_detection_experiments",
        experiment_type="baseline",
        model_type="comparison",
        config=config.__dict__,
        tags=["baseline", "comparison", "traditional_ml", "multimodal"]
    )
    
    try:
        # åˆ›å»ºæ•°æ®æµæ°´çº¿
        pipeline = MultiModalDataPipeline(config=config)
        
        # è¿è¡ŒåŸºç¡€ç‰¹å¾æå–
        pipeline.run_base_feature_extraction(
            start_week=0,
            end_week=config.data.max_weeks,
            max_users=config.data.max_users,
            sample_ratio=config.data.sample_ratio
        )
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        training_data = pipeline.prepare_training_data(
            start_week=0,
            end_week=config.data.max_weeks,
            max_users=config.data.max_users,
            sequence_length=config.model.sequence_length
        )
        
        results = {'experiment_type': 'baseline', 'models': {}}
        
        # 1. ä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿
        logger.info("ğŸ”§ è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹...")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ”¹è¿›ç‰ˆbaselineæ¨¡å‹
        use_improved_baseline = getattr(config, 'use_improved_baseline', False)
        
        if use_improved_baseline:
            logger.info("ğŸ”§ ä½¿ç”¨æ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹...")
            traditional_results = run_improved_baseline_comparison(
                training_data, 
                output_dir,
                models=["random_forest", "xgboost"],
                cv_folds=getattr(config, 'baseline_cv_folds', 5)
            )
            
            # è®°å½•æ”¹è¿›ç‰ˆä¼ ç»ŸMLç»“æœåˆ°WandB
            for model_type, model_results in traditional_results.items():
                if 'error' not in model_results and 'cv_results' in model_results:
                    cv_results = model_results['cv_results']
                    wandb_logger.log_metrics({
                        f"{model_type}_cv_f1_mean": cv_results.get('f1_test_mean', 0.0),
                        f"{model_type}_cv_f1_std": cv_results.get('f1_test_std', 0.0),
                        f"{model_type}_cv_auc_mean": cv_results.get('roc_auc_test_mean', 0.0),
                        f"{model_type}_cv_auc_std": cv_results.get('roc_auc_test_std', 0.0),
                        f"{model_type}_cv_precision_mean": cv_results.get('precision_test_mean', 0.0),
                        f"{model_type}_cv_recall_mean": cv_results.get('recall_test_mean', 0.0),
                        f"{model_type}_cv_pr_auc_mean": cv_results.get('average_precision_test_mean', 0.0),
                        f"{model_type}_n_features": model_results.get('n_features', 0)
                    })
                    
                    # è®°å½•ç‰¹å¾é‡è¦æ€§
                    if 'feature_importance' in model_results and 'feature_names' in model_results:
                        wandb_logger.log_feature_importance(
                            model_results['feature_names'],
                            model_results['feature_importance'],
                            title=f"{model_type.title()} Feature Importance (Improved)"
                        )
        else:
            logger.info("ğŸ“Š ä½¿ç”¨åŸå§‹åŸºçº¿æ¨¡å‹...")
            traditional_results = run_baseline_comparison(
                training_data, 
                output_dir,
                models=["random_forest", "xgboost"]
            )
            
            # è®°å½•ä¼ ç»ŸMLç»“æœåˆ°WandB
            for model_type, model_results in traditional_results.items():
                if 'error' not in model_results and 'test_metrics' in model_results:
                    tm = model_results['test_metrics']
                    wandb_logger.log_metrics({
                        f"{model_type}_test_f1": tm.get('f1', 0.0),
                        f"{model_type}_test_auc": tm.get('auc', 0.0),
                        f"{model_type}_test_accuracy": tm.get('accuracy', 0.0),
                        f"{model_type}_test_precision": tm.get('precision', 0.0),
                        f"{model_type}_test_recall": tm.get('recall', 0.0),
                        f"{model_type}_test_fpr": tm.get('fpr', 0.0) # å‡è®¾fprå·²è®¡ç®—
                    })
                    
                    # è®°å½•ç‰¹å¾é‡è¦æ€§
                    if 'feature_importance' in model_results and 'feature_names' in model_results:
                        wandb_logger.log_feature_importance(
                            model_results['feature_names'],
                            model_results['feature_importance'],
                            title=f"{model_type.title()} Feature Importance"
                        )
                    
                    # è®°å½•ROCæ›²çº¿ (å¦‚æœæ•°æ®å­˜åœ¨)
                    if 'y_true' in tm and 'y_pred_proba' in tm and \
                       isinstance(tm['y_true'], np.ndarray) and isinstance(tm['y_pred_proba'], np.ndarray) and \
                       len(tm['y_true']) > 0 and len(tm['y_pred_proba']) > 0 and \
                       tm['y_pred_proba'].ndim == 2 and tm['y_pred_proba'].shape[1] >=2: # Ensure proba has at least 2 classes for ROC
                        try:
                            wandb_logger.log_roc_curve(tm['y_true'], tm['y_pred_proba'], title=f"{model_type.title()} ROC Curve")
                        except Exception as e_roc:
                            logger.warning(f"WandB: ç»˜åˆ¶ {model_type} ROCæ›²çº¿å¤±è´¥: {e_roc}")
        
        results['models'].update(traditional_results)
        
        # 2. å¤šæ¨¡æ€æ¨¡å‹
        logger.info("ğŸ§  è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹...")
        multimodal_trainer = MultiModalTrainer(config=config, output_dir=output_dir)
        multimodal_model, multimodal_test_metrics = multimodal_trainer.train(training_data)
        
        # è·å–å¤šæ¨¡æ€æ¨¡å‹ç»“æœ
        multimodal_results = {
            'model_type': 'multimodal',
            'train_history': multimodal_trainer.train_history,
            'model_path': os.path.join(output_dir, 'best_model.pth'),
            'test_metrics': multimodal_test_metrics
        }
        
        # è®°å½•å¤šæ¨¡æ€ç»“æœåˆ°WandB
        if multimodal_trainer.train_history:
            history = multimodal_trainer.train_history
            if 'val_f1' in history and len(history['val_f1']) > 0:
                best_val_f1 = max(history['val_f1'])
                wandb_logger.log_metrics({
                    'multimodal_best_val_f1': best_val_f1,
                    'multimodal_final_train_loss': history.get('train_loss', [0])[-1],
                    'multimodal_final_val_loss': history.get('val_loss', [0])[-1]
                })
            
            # è®°å½•è®­ç»ƒæ›²çº¿
            wandb_logger.log_training_curves(history, "Multimodal Training Curves")

        if multimodal_test_metrics: # è®°å½•æµ‹è¯•é›†æŒ‡æ ‡
            mm_tm = multimodal_test_metrics
            wandb_logger.log_metrics({
                f"multimodal_test_f1": mm_tm.get('f1', 0.0),
                f"multimodal_test_auc": mm_tm.get('auc', 0.0),
                f"multimodal_test_accuracy": mm_tm.get('accuracy', 0.0),
                f"multimodal_test_precision": mm_tm.get('precision', 0.0),
                f"multimodal_test_recall": mm_tm.get('recall', 0.0),
                f"multimodal_test_fpr": mm_tm.get('fpr', 0.0)
            })
            # è®°å½•ROCæ›²çº¿ (å¦‚æœæ•°æ®å­˜åœ¨)
            if 'y_true' in mm_tm and 'y_pred_proba' in mm_tm and \
               isinstance(mm_tm['y_true'], np.ndarray) and isinstance(mm_tm['y_pred_proba'], np.ndarray) and \
               len(mm_tm['y_true']) > 0 and len(mm_tm['y_pred_proba']) > 0 and \
               mm_tm['y_pred_proba'].ndim == 2 and mm_tm['y_pred_proba'].shape[1] >=2:
                try:
                    wandb_logger.log_roc_curve(mm_tm['y_true'], mm_tm['y_pred_proba'], title="Multimodal ROC Curve")
                except Exception as e_roc_mm:
                    logger.warning(f"WandB: ç»˜åˆ¶å¤šæ¨¡æ€ROCæ›²çº¿å¤±è´¥: {e_roc_mm}")

        results['models']['multimodal'] = multimodal_results
        
        # 3. æ€§èƒ½å¯¹æ¯”æ€»ç»“
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š...")
        comparison_summary = {}
        
        for model_name, model_result in results['models'].items():
            if 'error' not in model_result:
                metrics_to_log = {}
                if model_name == 'multimodal':
                    # å¤šæ¨¡æ€æ¨¡å‹ä½¿ç”¨æµ‹è¯•é›†åˆ†æ•° (å¦‚æœå­˜åœ¨)ï¼Œå¦åˆ™å›é€€åˆ°éªŒè¯é›†åˆ†æ•°
                    test_metrics = model_result.get('test_metrics', {})
                    if test_metrics: # ä¼˜å…ˆä½¿ç”¨æµ‹è¯•é›†æŒ‡æ ‡
                        metrics_to_log['f1_score'] = test_metrics.get('f1', 0.0)
                        metrics_to_log['auc_score'] = test_metrics.get('auc', 0.0)
                        metrics_to_log['precision'] = test_metrics.get('precision', 0.0)
                        metrics_to_log['recall'] = test_metrics.get('recall', 0.0)
                        metrics_to_log['accuracy'] = test_metrics.get('accuracy', 0.0)
                        metrics_to_log['fpr'] = test_metrics.get('fpr', 0.0) 
                    else: # å›é€€åˆ°éªŒè¯é›†æŒ‡æ ‡ (ä¸»è¦ç”¨äºF1å’ŒAUC)
                        history = model_result.get('train_history', {})
                        metrics_to_log['f1_score'] = max(history.get('val_f1', [0.0])) if history.get('val_f1') else 0.0
                        metrics_to_log['auc_score'] = max(history.get('val_auc', [0.0])) if history.get('val_auc') else 0.0
                        # å¯¹äºéªŒè¯é›†ï¼Œé€šå¸¸ä¸ç›´æ¥è®¡ç®—precision/recall/fprï¼Œé™¤éæ˜¾å¼æä¾›
                        metrics_to_log['precision'] = 0.0 
                        metrics_to_log['recall'] = 0.0
                        metrics_to_log['accuracy'] = 0.0
                        metrics_to_log['fpr'] = 0.0
                elif 'cv_results' in model_result:
                    # æ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹ä½¿ç”¨äº¤å‰éªŒè¯ç»“æœ
                    cv_results = model_result['cv_results']
                    metrics_to_log['f1_score'] = cv_results.get('f1_test_mean', 0.0)
                    metrics_to_log['auc_score'] = cv_results.get('roc_auc_test_mean', 0.0)
                    metrics_to_log['precision'] = cv_results.get('precision_test_mean', 0.0)
                    metrics_to_log['recall'] = cv_results.get('recall_test_mean', 0.0)
                    metrics_to_log['accuracy'] = cv_results.get('accuracy_test_mean', 0.0)
                    metrics_to_log['fpr'] = 0.0  # æ”¹è¿›ç‰ˆæ¨¡å‹æš‚ä¸è®¡ç®—FPR
                    metrics_to_log['pr_auc'] = cv_results.get('average_precision_test_mean', 0.0)
                    metrics_to_log['n_features'] = model_result.get('n_features', 0)
                    metrics_to_log['model_type'] = 'improved_baseline'
                else:
                    # ä¼ ç»ŸMLæ¨¡å‹ä½¿ç”¨æµ‹è¯•åˆ†æ•°
                    test_metrics = model_result.get('test_metrics', {})
                    metrics_to_log['f1_score'] = test_metrics.get('f1', 0.0)
                    metrics_to_log['auc_score'] = test_metrics.get('auc', 0.0)
                    metrics_to_log['precision'] = test_metrics.get('precision', 0.0)
                    metrics_to_log['recall'] = test_metrics.get('recall', 0.0)
                    metrics_to_log['accuracy'] = test_metrics.get('accuracy', 0.0)
                    metrics_to_log['fpr'] = test_metrics.get('fpr', 0.0)
                    metrics_to_log['model_type'] = 'original_baseline'
                
                comparison_summary[model_name] = metrics_to_log
        
        results['comparison_summary'] = comparison_summary
        
        # è®°å½•å¯¹æ¯”ç»“æœåˆ°WandB (è¿™é‡Œä¿ç•™F1ï¼Œä½†å¯æ‰©å±•)
        model_names_for_chart = list(comparison_summary.keys())
        f1_scores_for_chart = [comparison_summary[name]['f1_score'] for name in model_names_for_chart]
        
        wandb_logger.log_ablation_results(model_names_for_chart, f1_scores_for_chart)
        # å¯ä»¥æ·»åŠ æ›´å¤šå›¾è¡¨ï¼Œä¾‹å¦‚ AUC comparison_summary[name]['auc_score'] ç­‰
        
        logger.info("âœ… åŸºçº¿å¯¹æ¯”å®éªŒå®Œæˆ")
        logger.info("ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        for model_name, metrics_dict in comparison_summary.items():
            model_type_info = metrics_dict.get('model_type', 'unknown')
            log_str = f"   {model_name} ({model_type_info}): "
            log_str += f"F1={metrics_dict.get('f1_score', 0.0):.4f}, "
            log_str += f"AUC={metrics_dict.get('auc_score', 0.0):.4f}, "
            log_str += f"Precision={metrics_dict.get('precision', 0.0):.4f}, "
            log_str += f"Recall={metrics_dict.get('recall', 0.0):.4f}, "
            log_str += f"Accuracy={metrics_dict.get('accuracy', 0.0):.4f}"
            
            # ä¸ºæ”¹è¿›ç‰ˆbaselineæ·»åŠ é¢å¤–ä¿¡æ¯
            if model_type_info == 'improved_baseline':
                log_str += f", PR-AUC={metrics_dict.get('pr_auc', 0.0):.4f}"
                log_str += f", Features={metrics_dict.get('n_features', 0)}"
            elif model_type_info == 'original_baseline':
                log_str += f", FPR={metrics_dict.get('fpr', 0.0):.4f}"
            
            logger.info(log_str)
        
        return results
        
    finally:
        wandb_logger.finish()

def run_tune_experiment(config: Config, output_dir: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–å®éªŒ (ä½¿ç”¨Optuna)
    
    Args:
        config: å®éªŒé…ç½®
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœ
    """
    logger.info("ğŸ¯ å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–å®éªŒ")
    
    # åˆå§‹åŒ–WandB
    wandb_logger = init_wandb(
        project_name="threat_detection_experiments",
        experiment_type="tuning",
        model_type="multimodal",
        config=config.__dict__,
        tags=["tuning", "optuna", "hyperparameter_optimization"]
    )
    
    try:
        # å‡†å¤‡æ•°æ®
        pipeline = MultiModalDataPipeline(config=config)
        pipeline.run_base_feature_extraction(
            start_week=0,
            end_week=config.data.max_weeks,
            max_users=config.data.max_users,
            sample_ratio=config.data.sample_ratio
        )
        
        training_data = pipeline.prepare_training_data(
            start_week=0,
            end_week=config.data.max_weeks,
            max_users=config.data.max_users,
            sequence_length=config.model.sequence_length
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MultiModalTrainer(config=config, output_dir=output_dir)
        
        # å®šä¹‰æœç´¢ç©ºé—´
        search_space = get_multimodal_search_space()
        
        # è¿è¡ŒOptunaä¼˜åŒ–
        logger.info("ğŸ” å¼€å§‹Optunaè¶…å‚æ•°æœç´¢...")
        tuning_results = run_optuna_tuning(
            model_type="multimodal",
            data=training_data,
            trainer=trainer,
            output_dir=output_dir,
            n_trials=getattr(config, 'n_trials', 20),  # é»˜è®¤20æ¬¡è¯•éªŒ
            wandb_logger=wandb_logger
        )
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒ
        logger.info("ğŸ† ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹...")
        best_params = tuning_results['best_params']
        
        # æ›´æ–°é…ç½®
        for param_name, param_value in best_params.items():
            if param_name in ['learning_rate', 'batch_size', 'weight_decay']:
                setattr(config.training, param_name, param_value)
            elif param_name in ['hidden_dim', 'num_heads', 'num_layers', 'dropout']:
                setattr(config.model, param_name, param_value)
        
        # é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
        best_trainer = MultiModalTrainer(config=config, output_dir=os.path.join(output_dir, 'best_model'))
        best_model, best_test_metrics = best_trainer.train(training_data)
        
        results = {
            'experiment_type': 'tuning',
            'tuning_results': tuning_results,
            'best_params': best_params,
            'best_score': tuning_results['best_value'],
            'final_model_path': os.path.join(output_dir, 'best_model', 'best_model.pth'),
            'test_metrics': best_test_metrics
        }
        
        # è®°å½•æœ€ç»ˆç»“æœåˆ°WandB
        wandb_logger.log_metrics({
            'best_hyperopt_score': tuning_results['best_value'],
            'total_trials': tuning_results['n_trials']
        })
        
        logger.info("âœ… è¶…å‚æ•°ä¼˜åŒ–å®éªŒå®Œæˆ")
        logger.info(f"ğŸ† æœ€ä½³å‚æ•°: {best_params}")
        logger.info(f"ğŸ¯ æœ€ä½³åˆ†æ•°: {tuning_results['best_value']:.4f}")
        
        return results
        
    finally:
        wandb_logger.finish()

def run_ablation_experiment(config: Config, output_dir: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    è¿è¡Œæ¶ˆèå®éªŒ
    ç ”ç©¶å¤šæ¨¡æ€æ¨¡å‹ä¸­å„åˆ†æ”¯æ¨¡å—çš„ç‹¬ç«‹è´¡çŒ®
    
    Args:
        config: å®éªŒé…ç½®
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœ
    """
    logger.info("ğŸ”¬ å¼€å§‹æ¶ˆèå®éªŒ")
    
    # åˆå§‹åŒ–WandB
    wandb_logger = init_wandb(
        project_name="threat_detection_experiments",
        experiment_type="ablation",
        model_type="multimodal",
        config=config.__dict__,
        tags=["ablation", "modality_analysis", "feature_contribution"]
    )
    
    try:
        # å®šä¹‰æ¶ˆèå®éªŒçš„æ¨¡æ€ç»„åˆ
        modality_combinations = [
            {'name': 'behavior_only', 'modalities': ['behavior']},
            {'name': 'graph_only', 'modalities': ['graph']},
            {'name': 'text_only', 'modalities': ['text']},
            {'name': 'structured_only', 'modalities': ['structured']},
            {'name': 'behavior_graph', 'modalities': ['behavior', 'graph']},
            {'name': 'behavior_text', 'modalities': ['behavior', 'text']},
            {'name': 'behavior_structured', 'modalities': ['behavior', 'structured']},
            {'name': 'all_modalities', 'modalities': ['behavior', 'graph', 'text', 'structured']}
        ]
        
        results = {'experiment_type': 'ablation', 'combinations': {}}
        combination_f1_scores = []
        combination_names = []
        
        for combination in modality_combinations:
            logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡æ€ç»„åˆ: {combination['name']}")
            
            # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºå­ç›®å½•
            combo_output_dir = os.path.join(output_dir, combination['name'])
            os.makedirs(combo_output_dir, exist_ok=True)
            
            # ä¿®æ”¹é…ç½®ä»¥åªä½¿ç”¨ç‰¹å®šæ¨¡æ€
            combo_config = copy.deepcopy(config)
            combo_config.model.enabled_modalities = combination['modalities']
            
            try:
                # åˆ›å»ºæ•°æ®æµæ°´çº¿
                pipeline = MultiModalDataPipeline(config=combo_config)
                pipeline.run_base_feature_extraction(
                    start_week=0,
                    end_week=combo_config.data.max_weeks,
                    max_users=combo_config.data.max_users,
                    sample_ratio=combo_config.data.sample_ratio
                )
                
                training_data = pipeline.prepare_training_data(
                    start_week=0,
                    end_week=combo_config.data.max_weeks,
                    max_users=combo_config.data.max_users,
                    sequence_length=combo_config.model.sequence_length
                )
                
                # è®­ç»ƒæ¨¡å‹
                trainer = MultiModalTrainer(config=combo_config, output_dir=combo_output_dir)
                model, test_metrics = trainer.train(training_data)
                
                # è·å–æœ€ä½³éªŒè¯F1åˆ†æ•°
                train_history = trainer.train_history
                best_f1 = max(train_history.get('val_f1', [0.0])) if train_history.get('val_f1') else 0.0
                
                combo_result = {
                    'modalities': combination['modalities'],
                    'best_val_f1': best_f1,
                    'train_history': train_history,
                    'model_path': os.path.join(combo_output_dir, 'best_model.pth'),
                    'test_metrics': test_metrics
                }
                
                results['combinations'][combination['name']] = combo_result
                combination_f1_scores.append(best_f1)
                combination_names.append(combination['name'])
                
                # è®°å½•åˆ°WandB
                wandb_logger.log_metrics({
                    f"{combination['name']}_f1": best_f1,
                    f"{combination['name']}_num_modalities": len(combination['modalities'])
                })
                
                logger.info(f"âœ… {combination['name']} å®Œæˆ - F1: {best_f1:.4f}")
                
            except Exception as e:
                logger.error(f"âŒ æ¨¡æ€ç»„åˆ {combination['name']} å®éªŒå¤±è´¥: {e}")
                results['combinations'][combination['name']] = {'error': str(e)}
                combination_f1_scores.append(0.0)
                combination_names.append(combination['name'])
        
        # è®°å½•æ¶ˆèç»“æœåˆ°WandB
        wandb_logger.log_ablation_results(combination_names, combination_f1_scores)
        
        # åˆ†æç»“æœ
        results['analysis'] = {
            'best_combination': combination_names[combination_f1_scores.index(max(combination_f1_scores))],
            'best_f1_score': max(combination_f1_scores),
            'modality_contribution': dict(zip(combination_names, combination_f1_scores))
        }
        
        logger.info("âœ… æ¶ˆèå®éªŒå®Œæˆ")
        logger.info("ğŸ“Š æ¨¡æ€ç»„åˆæ€§èƒ½æ’åº:")
        sorted_combinations = sorted(zip(combination_names, combination_f1_scores), 
                                   key=lambda x: x[1], reverse=True)
        for name, score in sorted_combinations:
            logger.info(f"   {name}: F1={score:.4f}")
        
        return results
        
    finally:
        wandb_logger.finish()

def run_imbalance_experiment(config: Config, output_dir: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    è¿è¡Œæ•°æ®ä¸å¹³è¡¡é€‚åº”æ€§å®éªŒ
    è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒç¨‹åº¦çš„æ•°æ®å¤±è¡¡ä¸‹çš„é²æ£’æ€§è¡¨ç°
    
    Args:
        config: å®éªŒé…ç½®
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœ
    """
    logger.info("âš–ï¸ å¼€å§‹æ•°æ®ä¸å¹³è¡¡é€‚åº”æ€§å®éªŒ")
    
    # åˆå§‹åŒ–WandB
    wandb_logger = init_wandb(
        project_name="threat_detection_experiments",
        experiment_type="imbalance",
        model_type="analysis",
        config=config.__dict__,
        tags=["imbalance", "robustness", "sampling_strategies"]
    )
    
    try:
        # å‡†å¤‡æ•°æ®
        pipeline = MultiModalDataPipeline(config=config)
        pipeline.run_base_feature_extraction(
            start_week=0,
            end_week=config.data.max_weeks,
            max_users=config.data.max_users,
            sample_ratio=config.data.sample_ratio
        )
        
        training_data = pipeline.prepare_training_data(
            start_week=0,
            end_week=config.data.max_weeks,
            max_users=config.data.max_users,
            sequence_length=config.model.sequence_length
        )
        
        # åˆ›å»ºåŸºçº¿æ¨¡å‹è®­ç»ƒå™¨ç”¨äºä¸å¹³è¡¡å®éªŒ
        baseline_trainer = BaselineModelTrainer(model_type="random_forest")
        
        # å®šä¹‰ä¸å¹³è¡¡æ¯”ä¾‹å’Œé‡‡æ ·ç­–ç•¥
        ratios = [1.0, 2.0, 3.0, 4.0, 5.0]  # æ­£å¸¸:æ¶æ„æ¯”ä¾‹
        sampling_strategies = ['none', 'smote', 'adasyn', 'random_undersample']
        
        # è¿è¡Œä¸å¹³è¡¡å®éªŒ
        imbalance_results = utils_run_imbalance_experiment(
            training_data,
            output_dir,
            baseline_trainer,
            ratios=ratios,
            sampling_strategies=sampling_strategies
        )
        
        # è®°å½•ç»“æœåˆ°WandB
        strategy_comparison = imbalance_results.get('strategy_comparison', {})
        
        for strategy, metrics in strategy_comparison.items():
            wandb_logger.log_metrics({
                f"{strategy}_avg_f1": metrics['avg_f1'],
                f"{strategy}_avg_auc": metrics['avg_auc']
            })
        
        # ç”Ÿæˆä¸å¹³è¡¡åˆ†æå›¾è¡¨
        if strategy_comparison:
            # é€‰æ‹©æœ€ä½³ç­–ç•¥çš„ç»“æœè¿›è¡Œå¯è§†åŒ–
            best_strategy = max(strategy_comparison.keys(), 
                              key=lambda x: strategy_comparison[x]['avg_f1'])
            best_metrics = strategy_comparison[best_strategy]
            
            wandb_logger.log_imbalance_analysis(
                ratios,
                best_metrics['f1_scores'],
                best_metrics['auc_scores']
            )
        
        results = {
            'experiment_type': 'imbalance',
            'imbalance_results': imbalance_results,
            'best_strategy': max(strategy_comparison.keys(), 
                               key=lambda x: strategy_comparison[x]['avg_f1']) if strategy_comparison else None
        }
        
        logger.info("âœ… æ•°æ®ä¸å¹³è¡¡å®éªŒå®Œæˆ")
        if strategy_comparison:
            logger.info("ğŸ“ˆ é‡‡æ ·ç­–ç•¥æ€§èƒ½å¯¹æ¯”:")
            for strategy, metrics in strategy_comparison.items():
                logger.info(f"   {strategy}: å¹³å‡F1={metrics['avg_f1']:.4f}, å¹³å‡AUC={metrics['avg_auc']:.4f}")
        
        return results
        
    finally:
        wandb_logger.finish()

def run_realtime_experiment(config: Config, output_dir: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    è¿è¡Œå®æ—¶æ£€æµ‹å®éªŒ
    
    Args:
        config: å®éªŒé…ç½®
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœ
    """
    logger.info("âš¡ å¼€å§‹å®æ—¶æ£€æµ‹å®éªŒ")
    
    # TODO: å®ç°å®æ—¶æ£€æµ‹é€»è¾‘
    # è¿™é‡Œåº”è¯¥åŒ…æ‹¬ï¼š
    # 1. æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµ
    # 2. å¢é‡å­¦ä¹ 
    # 3. åœ¨çº¿æ£€æµ‹
    # 4. æ€§èƒ½ç›‘æ§
    
    results = {
        'experiment_type': 'realtime',
        'status': 'not_implemented',
        'message': 'å®æ—¶æ£€æµ‹å®éªŒå°šæœªå®ç°'
    }
    
    logger.warning("âš ï¸ å®æ—¶æ£€æµ‹å®éªŒå°šæœªå®ç°")
    return results

def run_generalization_eval(config: Config, output_dir: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    è¿è¡Œæ³›åŒ–èƒ½åŠ›è¯„ä¼°å®éªŒ
    åœ¨ä¸åŒç”¨æˆ·å­é›†ä¸‹é‡å¤è®­ç»ƒå’ŒéªŒè¯ï¼Œè§‚æµ‹æ¨¡å‹æ€§èƒ½æ˜¯å¦ç¨³å®šã€‚

    Args:
        config: å®éªŒé…ç½® (gen_config.yaml)
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—å™¨

    Returns:
        å®éªŒç»“æœ
    """
    logger.info("ğŸ§¬ å¼€å§‹æ³›åŒ–èƒ½åŠ›è¯„ä¼°å®éªŒ")

    num_runs = getattr(config.generalization, 'num_gen_runs', 5) # ä»é…ç½®ä¸­è·å–è¿è¡Œæ¬¡æ•°ï¼Œé»˜è®¤ä¸º5
    base_seed = getattr(config.generalization, 'base_seed', 42)  # åŸºç¡€ç§å­ï¼Œé»˜è®¤ä¸º42
    
    fixed_max_users = 200
    fixed_sample_ratio = 1.0

    all_run_metrics = []
    wandb_project_name = "threat_detection_experiments" # ä¸promptä¸€è‡´

    for i in range(num_runs):
        current_seed = base_seed + i
        run_name = f"gen_eval_seed_{current_seed}"
        logger.info(f"\n--- æ³›åŒ–å®éªŒè½®æ¬¡ {i+1}/{num_runs} (ç§å­: {current_seed}) ---")

        # æ›´æ–°é…ç½®ä»¥ç”¨äºå½“å‰è½®æ¬¡
        current_run_config = Config() # åˆ›å»ºæ–°çš„Configå®ä¾‹ä»¥é¿å…ä¿®æ”¹åŸå§‹configå¯¹è±¡
        current_run_config.__dict__.update(config.__dict__) # æ·±æ‹·è´æˆ–é€‰æ‹©æ€§æ‹·è´å¯èƒ½æ›´å¥½
        
        # æ›´æ–°DataConfigéƒ¨åˆ†
        current_run_config.data.max_users = fixed_max_users
        current_run_config.data.sample_ratio = fixed_sample_ratio
        current_run_config.seed = current_seed # è®¾ç½®å…¨å±€ç§å­ï¼Œä¼šè¢«æµæ°´çº¿ä½¿ç”¨
        
        # MultiModalDataPipelineçš„ __init__ ä¼šä» config.data è¯»å– num_cores, data_version, feature_dim
        # ä¹Ÿä¼šä» config è¯»å– seed

        # WandB åˆå§‹åŒ–
        wandb_run_logger = init_wandb(
            project_name=wandb_project_name,
            experiment_type="generalization", # group
            model_type="multimodal", # å¯ä»¥æ˜¯å›ºå®šçš„ï¼Œæˆ–è€…ä»configè¯»å–
            config={**current_run_config.__dict__, "experiment_name": "generalization", "random_seed": current_seed, "max_users": fixed_max_users, "run_iteration": i+1}, # é™„åŠ å­—æ®µ
            tags=["generalization", f"seed_{current_seed}"],
            run_name_override=run_name # ä½¿ç”¨ç‰¹å®šåç§°
        )

        try:
            logger.info(f"  ä½¿ç”¨é…ç½®: max_users={current_run_config.data.max_users}, sample_ratio={current_run_config.data.sample_ratio}, seed={current_run_config.seed}")
            
            # 1. æ•°æ®å‡†å¤‡
            pipeline = MultiModalDataPipeline(config=current_run_config) # MMPä¼šä½¿ç”¨configä¸­çš„seedæ¥åˆå§‹åŒ–CERTDatasetPipeline
            
            # æ³¨æ„: run_base_feature_extraction çš„ max_users å’Œ sample_ratio å‚æ•°ä¼šè¦†ç›– CERTDatasetPipeline ä¸­
            # é€šè¿‡ config è®¾ç½®çš„åŒåå‚æ•°ã€‚ä¸ºäº†è®©æ³›åŒ–å®éªŒçš„ç§å­å’Œç”¨æˆ·/é‡‡æ ·ç‡è®¾ç½®ç”Ÿæ•ˆï¼Œ
            # æˆ‘ä»¬éœ€è¦ç¡®ä¿ CERTDatasetPipeline åœ¨åˆ›å»ºæ—¶å°±æ‹¿åˆ°äº†æ­£ç¡®çš„ç§å­ï¼Œ
            # å¹¶ä¸” run_base_feature_extraction è°ƒç”¨æ—¶ä¸ä¼ é€’ max_users å’Œ sample_ratio (æˆ–ä¼ é€’None)ï¼Œ
            # è¿™æ ·å®ƒå°±ä¼šä½¿ç”¨ CERTDatasetPipeline å®ä¾‹å†…éƒ¨å·²ç»é€šè¿‡configè®¾ç½®å¥½çš„å€¼ã€‚
            # æˆ–è€…ï¼Œç›´æ¥åœ¨è¿™é‡Œä¼ é€’æ­£ç¡®çš„å€¼ã€‚
            # å½“å‰ MultiModalDataPipeline.__init__ ä¼šå°† config.data.max_users ç­‰ä¼ é€’ç»™ CERTDatasetPipeline
            # æ‰€ä»¥ CERTDatasetPipeline åº”è¯¥å·²ç»æœ‰äº†æ­£ç¡®çš„ max_users å’Œ sample_ratio (å¦‚æœå®ƒä»¬åœ¨ config.data ä¸­è®¾ç½®)ã€‚
            # ç„¶è€Œï¼Œæ³›åŒ–å®éªŒè¦æ±‚ max_users=200, sample_ratio=1.0ï¼Œè¿™äº›å€¼å·²åœ¨ current_run_config.data ä¸­è®¾ç½®ã€‚
            
            # MultiModalDataPipeline.run_base_feature_extraction ç°åœ¨ä¼šä» self.config.data.sample_ratio è¯»å–é‡‡æ ·ç‡
            # å®ƒä¼ é€’ç»™ CERTDatasetPipeline.run_full_pipeline çš„ max_users æ˜¯ None (å› ä¸ºåŸå§‹è°ƒç”¨ä¸­æ˜¯None)ï¼Œ
            # sample_ratio æ˜¯ä» config.data.sample_ratio æ¥çš„ã€‚
            # CERTDatasetPipeline.run_full_pipeline ä¸­çš„ max_users å’Œ sample_ratio å¦‚æœä¸ºNoneï¼Œåˆ™ä¸ä¼šè¦†ç›–å®ä¾‹å†…çš„å€¼ã€‚
            # ä¸ºäº†ç¡®ä¿ CERTDatasetPipeline ä½¿ç”¨æˆ‘ä»¬ä¸ºå½“å‰è½®æ¬¡è®¾ç½®çš„ fixed_max_users å’Œ fixed_sample_ratioï¼Œ
            # æœ€å¥½çš„æ–¹å¼æ˜¯ç¡®ä¿ CERTDatasetPipeline åˆå§‹åŒ–æ—¶å°±ä» config.data æ¥æ”¶åˆ°è¿™äº›å€¼ã€‚
            # è€Œ current_run_config.data å·²ç»è¢«æ­£ç¡®è®¾ç½®äº†ã€‚

            pipeline.run_base_feature_extraction(
                start_week=0, # æˆ–ä»configè¯»å–
                end_week=current_run_config.data.max_weeks, # æˆ–ä»configè¯»å–
                # max_users å’Œ sample_ratio ä¸åœ¨è¿™é‡Œä¼ é€’ï¼Œè®© CERTDatasetPipeline ä½¿ç”¨å…¶å†…éƒ¨é€šè¿‡ config è®¾ç½®çš„å€¼
            )
            
            training_data = pipeline.prepare_training_data(
                start_week=0, # æˆ–ä»configè¯»å–
                end_week=current_run_config.data.max_weeks, # æˆ–ä»configè¯»å–
                max_users=current_run_config.data.max_users, # ç¡®ä¿ MultiModalDataPipeline ä¹Ÿä½¿ç”¨æ­£ç¡®çš„ max_users
                sequence_length=current_run_config.model.sequence_length
            )

            if not training_data or len(training_data.get('labels', [])) == 0:
                logger.warning(f"è½®æ¬¡ {i+1} (ç§å­ {current_seed}): æœªç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè·³è¿‡æ­¤è½®ã€‚")
                all_run_metrics.append({'seed': current_seed, 'error': 'No training data'})
                wandb_run_logger.finish()
                continue
            
            # æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦è‡³å°‘æœ‰ä¸¤ä¸ªç±»åˆ«
            unique_labels = np.unique(training_data['labels'])
            if len(unique_labels) < 2:
                logger.warning(f"è½®æ¬¡ {i+1} (ç§å­ {current_seed}): è®­ç»ƒæ•°æ®åªåŒ…å«ä¸€ä¸ªç±»åˆ« ({unique_labels})ï¼Œæ— æ³•è¿›è¡Œæœ‰æ„ä¹‰çš„è®­ç»ƒ/è¯„ä¼°ã€‚è·³è¿‡æ­¤è½®ã€‚")
                all_run_metrics.append({'seed': current_seed, 'error': 'Single class in training data'})
                wandb_run_logger.finish()
                continue

            # 2. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° (åªè®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹)
            logger.info(f"  è½®æ¬¡ {i+1}: è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹...")
            run_output_dir = os.path.join(output_dir, f"run_{i+1}_seed_{current_seed}")
            os.makedirs(run_output_dir, exist_ok=True)
            
            # ç¡®ä¿ MultiModalTrainer ä¹Ÿä½¿ç”¨å½“å‰è½®æ¬¡çš„ç§å­ï¼Œä»¥ä¾¿å…¶å†…éƒ¨çš„train/val/teståˆ’åˆ†æ˜¯ä¸€è‡´çš„ï¼ˆå¦‚æœå®ƒä¹Ÿç”¨äº†éšæœºç§å­ï¼‰
            # MultiModalTrainer çš„ __init__ æ¥æ”¶ configï¼Œå®ƒå†…éƒ¨çš„ _prepare_dataloaders ä¼šä½¿ç”¨ config.seed
            multimodal_trainer = MultiModalTrainer(config=current_run_config, output_dir=run_output_dir)
            # ä¿®æ”¹ï¼šæ¥æ”¶æ¨¡å‹å’Œæµ‹è¯•æŒ‡æ ‡
            trained_model, final_test_metrics = multimodal_trainer.train(training_data)

            # æ”¶é›†æŒ‡æ ‡ - ç›´æ¥ä½¿ç”¨ final_test_metrics
            # final_test_metrics å­—å…¸çš„é”®å¦‚ 'f1', 'auc' ç­‰ï¼Œè€Œä¸æ˜¯ 'test_f1'
            current_run_results = {
                'seed': current_seed,
                'f1_score': final_test_metrics.get('f1', np.nan),
                'auc': final_test_metrics.get('auc', np.nan),
                'precision': final_test_metrics.get('precision', np.nan),
                'recall': final_test_metrics.get('recall', np.nan),
                'accuracy': final_test_metrics.get('accuracy', np.nan),
                'error': None
            }
            all_run_metrics.append(current_run_results)

            # è®°å½•åˆ°WandB
            wandb_run_logger.log_metrics({
                'generalization_test_f1': current_run_results['f1_score'],
                'generalization_test_auc': current_run_results['auc'],
                'generalization_test_precision': current_run_results['precision'],
                'generalization_test_recall': current_run_results['recall'],
                'generalization_test_accuracy': current_run_results['accuracy']
            })
            logger.info(f"  è½®æ¬¡ {i+1} ç»“æœ: F1={current_run_results['f1_score']:.4f}, AUC={current_run_results['auc']:.4f}")

        except Exception as e:
            logger.error(f"âŒ æ³›åŒ–å®éªŒè½®æ¬¡ {i+1} (ç§å­ {current_seed}) å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            all_run_metrics.append({'seed': current_seed, 'f1_score': np.nan, 'auc': np.nan, 'precision': np.nan, 'recall': np.nan, 'accuracy': np.nan, 'error': str(e)})
        finally:
            wandb_run_logger.finish()

    # ç»“æœèšåˆä¸ä¿å­˜
    results_df = pd.DataFrame(all_run_metrics)
    summary_stats = results_df[['f1_score', 'auc', 'precision', 'recall', 'accuracy']].agg(['mean', 'std']).reset_index()

    logger.info("\n--- æ³›åŒ–å®éªŒæ€»ç»“ ---")
    logger.info("è¯¦ç»†è½®æ¬¡ç»“æœ:")
    logger.info(results_df.to_string())
    logger.info("\næ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡ (å¹³å‡å€¼ Â± æ ‡å‡†å·®):")
    for col in ['f1_score', 'auc', 'precision', 'recall', 'accuracy']:
        mean_val = results_df[col].mean()
        std_val = results_df[col].std()
        logger.info(f"  {col.replace('_', ' ').title()}: {mean_val:.4f} Â± {std_val:.4f}")

    # ä¿å­˜ç»“æœ
    summary_file_path = os.path.join(output_dir, "gen_eval_summary.csv")
    results_df.to_csv(summary_file_path, index=False)
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {summary_file_path}")
    
    summary_stats_path = os.path.join(output_dir, "gen_eval_mean_std.csv")
    summary_stats.to_csv(summary_stats_path, index=False)
    logger.info(f"ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_stats_path}")

    # å¯é€‰ï¼šç”Ÿæˆç®±çº¿å›¾ (éœ€è¦ matplotlib å’Œ seaborn)
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        sns.boxplot(data=results_df['f1_score'])
        plt.title('F1 Score Distribution (Generalization Runs)')
        plt.ylabel('F1 Score')
        
        plt.subplot(1, 2, 2)
        sns.boxplot(data=results_df['auc'])
        plt.title('AUC Distribution (Generalization Runs)')
        plt.ylabel('AUC')
        
        boxplot_path = os.path.join(output_dir, "gen_eval_f1_auc_boxplot.png")
        plt.tight_layout()
        plt.savefig(boxplot_path)
        logger.info(f"F1å’ŒAUCç®±çº¿å›¾å·²ä¿å­˜åˆ°: {boxplot_path}")
        plt.close()

    except ImportError:
        logger.warning("matplotlib æˆ– seaborn æœªå®‰è£…ï¼Œè·³è¿‡ç”Ÿæˆç®±çº¿å›¾ã€‚")
    except Exception as e_plot:
        logger.error(f"ç”Ÿæˆç®±çº¿å›¾æ—¶å‡ºé”™: {e_plot}")


    return {
        'experiment_type': 'generalization',
        'num_runs': num_runs,
        'all_run_metrics': all_run_metrics,
        'summary_statistics': summary_stats.to_dict('records')
    }

def save_results(results: Dict[str, Any], output_dir: str, experiment_name: str):
    """
    ä¿å­˜å®éªŒç»“æœ
    
    Args:
        results: å®éªŒç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        experiment_name: å®éªŒåç§°
    """
    results_file = os.path.join(output_dir, f"{experiment_name}_results.json")
    
    # å¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
    def json_serializable(obj):
        if hasattr(obj, '__dict__'):
            return obj.__dict__
        return str(obj)
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=json_serializable)
    
    print(f"ğŸ“ å®éªŒç»“æœå·²ä¿å­˜: {results_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å¤šæ¨¡æ€å†…éƒ¨å¨èƒæ£€æµ‹ç³»ç»Ÿ - ä¸»å®éªŒæ§åˆ¶è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å®éªŒç±»å‹è¯´æ˜:
  baseline  - ä¼ ç»ŸMLæ–¹æ³•å¯¹æ¯”å®éªŒ (RandomForest vs XGBoost vs å¤šæ¨¡æ€)
  tune      - è¶…å‚æ•°ä¼˜åŒ–å®éªŒ (ä½¿ç”¨Optuna)
  ablation  - æ¶ˆèå®éªŒï¼Œæµ‹è¯•ä¸åŒæ¨¡æ€ç»„åˆçš„æ•ˆæœ
  imbalance - æ•°æ®ä¸å¹³è¡¡é€‚åº”æ€§å®éªŒ
  realtime  - å®æ—¶æ£€æµ‹å®éªŒ (å¾…å®ç°)
  generalization - æ¨¡å‹æ³›åŒ–èƒ½åŠ›è¯„ä¼°å®éªŒ

ç¤ºä¾‹:
  python main_experiment.py --run_type baseline --max_users 100 --epochs 5
  python main_experiment.py --run_type tune --config_file configs/tune_config.yaml --n_trials 30
  python main_experiment.py --run_type ablation --output_dir ./results/ablation_exp
  python main_experiment.py --run_type imbalance --max_users 200
  python main_experiment.py --run_type generalization --config_file configs/gen_config.yaml
        """
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--run_type', type=str, required=True,
                       choices=['baseline', 'tune', 'ablation', 'imbalance', 'realtime', 'generalization'],
                       help='å®éªŒç±»å‹')
    parser.add_argument('--config_file', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (JSON/YAML)')
    parser.add_argument('--output_dir', type=str, default='./results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--experiment_name', type=str, default=None,
                       help='å®éªŒåç§° (é»˜è®¤ä½¿ç”¨run_type + æ—¶é—´æˆ³)')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--max_users', type=int, default=100,
                       help='æœ€å¤§ç”¨æˆ·æ•°')
    parser.add_argument('--data_version', type=str, default='r4.2',
                       help='æ•°æ®é›†ç‰ˆæœ¬')
    parser.add_argument('--sample_ratio', type=float, default=1.0,
                       help='æ•°æ®é‡‡æ ·æ¯”ä¾‹')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=3,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'cuda', 'mps'],
                       help='è®­ç»ƒè®¾å¤‡')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_dim', type=int, default=256,
                       help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--num_heads', type=int, default=8,
                       help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--num_layers', type=int, default=6,
                       help='Transformerå±‚æ•°')
    
    # è¶…å‚æ•°ä¼˜åŒ–å‚æ•°
    parser.add_argument('--n_trials', type=int, default=20,
                       help='Optunaä¼˜åŒ–è¯•éªŒæ¬¡æ•°')
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument('--num_cores', type=int, default=8,
                       help='CPUæ ¸å¿ƒæ•°')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    # æ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹å‚æ•°
    parser.add_argument('--use_improved_baseline', action='store_true',
                       help='ä½¿ç”¨æ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹ï¼ˆå·®å¼‚åŒ–ç‰¹å¾å·¥ç¨‹å’Œäº¤å‰éªŒè¯ï¼‰')
    parser.add_argument('--baseline_cv_folds', type=int, default=5,
                       help='åŸºçº¿æ¨¡å‹äº¤å‰éªŒè¯æŠ˜æ•°')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆå®éªŒåç§°
    if args.experiment_name is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        args.experiment_name = f"{args.run_type}_{timestamp}"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(args.output_dir, args.experiment_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(output_dir, args.experiment_name)
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(
            config_file=args.config_file,
            max_users=args.max_users,
            data_version=args.data_version,
            sample_ratio=args.sample_ratio,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            device=args.device,
            hidden_dim=args.hidden_dim,
            num_heads=args.num_heads,
            num_layers=args.num_layers,
            num_cores=args.num_cores,
            seed=args.seed,
            n_trials=args.n_trials,
            use_improved_baseline=args.use_improved_baseline,
            baseline_cv_folds=args.baseline_cv_folds
        )
        
        logger.info(f"ğŸ¯ å¼€å§‹å®éªŒ: {args.experiment_name}")
        logger.info(f"ğŸ“‹ å®éªŒç±»å‹: {args.run_type}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # æ ¹æ®å®éªŒç±»å‹è¿è¡Œç›¸åº”çš„å®éªŒ
        if args.run_type == 'baseline':
            results = run_baseline_experiment(config, output_dir, logger)
        elif args.run_type == 'tune':
            results = run_tune_experiment(config, output_dir, logger)
        elif args.run_type == 'ablation':
            results = run_ablation_experiment(config, output_dir, logger)
        elif args.run_type == 'imbalance':
            results = run_imbalance_experiment(config, output_dir, logger)
        elif args.run_type == 'realtime':
            results = run_realtime_experiment(config, output_dir, logger)
        elif args.run_type == 'generalization':
            results = run_generalization_eval(config, output_dir, logger)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å®éªŒç±»å‹: {args.run_type}")
        
        # ä¿å­˜ç»“æœ
        save_results(results, output_dir, args.experiment_name)
        
        logger.info(f"ğŸ‰ å®éªŒ {args.experiment_name} å®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main() 