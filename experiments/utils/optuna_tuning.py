#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optuna Hyperparameter Tuning Utilities
Optunaè¶…å‚æ•°ä¼˜åŒ–å·¥å…·æ¨¡å—
"""

import optuna
import numpy as np
import pandas as pd
from typing import Dict, Any, List, Optional, Callable
import os
import json
from datetime import datetime
import logging

# è®¾ç½®Optunaæ—¥å¿—çº§åˆ«
optuna.logging.set_verbosity(optuna.logging.WARNING)

class OptunaOptimizer:
    """Optunaè¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 study_name: str = None,
                 direction: str = "maximize",
                 storage: str = None,
                 random_state: int = 42):
        """
        åˆå§‹åŒ–Optunaä¼˜åŒ–å™¨
        
        Args:
            study_name: ç ”ç©¶åç§°
            direction: ä¼˜åŒ–æ–¹å‘ ("maximize" æˆ– "minimize")
            storage: å­˜å‚¨åç«¯ (å¯é€‰)
            random_state: éšæœºç§å­
        """
        self.study_name = study_name or f"study_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.direction = direction
        self.storage = storage
        self.random_state = random_state
        
        # åˆ›å»ºé‡‡æ ·å™¨
        self.sampler = optuna.samplers.TPESampler(seed=random_state)
        
        # åˆ›å»ºç ”ç©¶
        self.study = optuna.create_study(
            study_name=self.study_name,
            direction=direction,
            sampler=self.sampler,
            storage=storage,
            load_if_exists=True
        )
        
        self.best_params = None
        self.best_value = None
        self.optimization_history = []
    
    def define_search_space(self, trial: optuna.Trial, search_space: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®šä¹‰æœç´¢ç©ºé—´
        
        Args:
            trial: Optunaè¯•éªŒå¯¹è±¡
            search_space: æœç´¢ç©ºé—´å®šä¹‰
            
        Returns:
            é‡‡æ ·çš„å‚æ•°å­—å…¸
        """
        params = {}
        
        for param_name, param_config in search_space.items():
            param_type = param_config.get('type', 'float')
            
            if param_type == 'float':
                params[param_name] = trial.suggest_float(
                    param_name,
                    param_config['low'],
                    param_config['high'],
                    log=param_config.get('log', False)
                )
            elif param_type == 'int':
                params[param_name] = trial.suggest_int(
                    param_name,
                    param_config['low'],
                    param_config['high'],
                    log=param_config.get('log', False)
                )
            elif param_type == 'categorical':
                params[param_name] = trial.suggest_categorical(
                    param_name,
                    param_config['choices']
                )
            elif param_type == 'discrete_uniform':
                params[param_name] = trial.suggest_discrete_uniform(
                    param_name,
                    param_config['low'],
                    param_config['high'],
                    param_config['q']
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å‚æ•°ç±»å‹: {param_type}")
        
        return params
    
    def optimize_multimodal_model(self,
                                 training_data: Dict[str, Any],
                                 model_trainer,
                                 search_space: Dict[str, Any],
                                 n_trials: int = 50,
                                 timeout: Optional[int] = None,
                                 wandb_logger=None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„è¶…å‚æ•°
        
        Args:
            training_data: è®­ç»ƒæ•°æ®
            model_trainer: æ¨¡å‹è®­ç»ƒå™¨
            search_space: æœç´¢ç©ºé—´
            n_trials: è¯•éªŒæ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            wandb_logger: WandBè®°å½•å™¨
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        def objective(trial):
            try:
                # é‡‡æ ·è¶…å‚æ•°
                params = self.define_search_space(trial, search_space)
                
                # æ›´æ–°æ¨¡å‹é…ç½®
                config = model_trainer.config
                for param_name, param_value in params.items():
                    if param_name in ['learning_rate', 'batch_size', 'weight_decay']:
                        setattr(config.training, param_name, param_value)
                    elif param_name in ['hidden_dim', 'num_heads', 'num_layers', 'dropout']:
                        setattr(config.model, param_name, param_value)
                    else:
                        # å…¶ä»–å‚æ•°è®¾ç½®åˆ°ç›¸åº”çš„é…ç½®æ®µ
                        if hasattr(config.model, param_name):
                            setattr(config.model, param_name, param_value)
                        elif hasattr(config.training, param_name):
                            setattr(config.training, param_name, param_value)
                
                # è®­ç»ƒæ¨¡å‹
                model = model_trainer.train(training_data)
                
                # è·å–éªŒè¯æŒ‡æ ‡
                train_history = model_trainer.train_history
                if 'val_f1' in train_history and len(train_history['val_f1']) > 0:
                    # ä½¿ç”¨æœ€ä½³éªŒè¯F1åˆ†æ•°
                    objective_value = max(train_history['val_f1'])
                elif 'val_accuracy' in train_history and len(train_history['val_accuracy']) > 0:
                    # å¤‡é€‰ï¼šä½¿ç”¨éªŒè¯å‡†ç¡®ç‡
                    objective_value = max(train_history['val_accuracy'])
                else:
                    # æœ€åå¤‡é€‰ï¼šä½¿ç”¨è®­ç»ƒF1
                    objective_value = max(train_history.get('train_f1', [0.0]))
                
                # è®°å½•åˆ°WandB
                if wandb_logger:
                    wandb_logger.log_metrics({
                        'trial_number': trial.number,
                        'objective_value': objective_value,
                        **{f'param_{k}': v for k, v in params.items()}
                    })
                
                # è®°å½•ä¼˜åŒ–å†å²
                self.optimization_history.append({
                    'trial': trial.number,
                    'params': params,
                    'value': objective_value
                })
                
                return objective_value
                
            except Exception as e:
                print(f"âš ï¸ Trial {trial.number} å¤±è´¥: {e}")
                return 0.0  # è¿”å›æœ€å·®åˆ†æ•°
        
        # å¼€å§‹ä¼˜åŒ–
        print(f"ğŸ¯ å¼€å§‹Optunaè¶…å‚æ•°ä¼˜åŒ–...")
        print(f"   æœç´¢ç©ºé—´: {search_space}")
        print(f"   è¯•éªŒæ¬¡æ•°: {n_trials}")
        
        self.study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        # è·å–æœ€ä½³ç»“æœ
        self.best_params = self.study.best_params
        self.best_value = self.study.best_value
        
        print(f"âœ… ä¼˜åŒ–å®Œæˆ!")
        print(f"   æœ€ä½³å‚æ•°: {self.best_params}")
        print(f"   æœ€ä½³åˆ†æ•°: {self.best_value:.4f}")
        
        return {
            'best_params': self.best_params,
            'best_value': self.best_value,
            'n_trials': len(self.study.trials),
            'optimization_history': self.optimization_history,
            'study': self.study
        }
    
    def optimize_traditional_model(self,
                                 multimodal_data: Dict[str, Any],
                                 model_trainer,
                                 search_space: Dict[str, Any],
                                 n_trials: int = 50,
                                 timeout: Optional[int] = None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¶…å‚æ•°
        
        Args:
            multimodal_data: å¤šæ¨¡æ€æ•°æ®
            model_trainer: åŸºçº¿æ¨¡å‹è®­ç»ƒå™¨
            search_space: æœç´¢ç©ºé—´
            n_trials: è¯•éªŒæ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        def objective(trial):
            try:
                # é‡‡æ ·è¶…å‚æ•°
                params = self.define_search_space(trial, search_space)
                
                # æ›´æ–°æ¨¡å‹å‚æ•°
                if model_trainer.model_type == "random_forest":
                    from sklearn.ensemble import RandomForestClassifier
                    model_trainer.model = RandomForestClassifier(
                        random_state=model_trainer.random_state,
                        n_jobs=-1,
                        **params
                    )
                elif model_trainer.model_type == "xgboost":
                    import xgboost as xgb
                    model_trainer.model = xgb.XGBClassifier(
                        random_state=model_trainer.random_state,
                        n_jobs=-1,
                        eval_metric='logloss',
                        **params
                    )
                
                # è®­ç»ƒæ¨¡å‹
                results = model_trainer.train(multimodal_data)
                
                # è·å–æµ‹è¯•F1åˆ†æ•°ä½œä¸ºç›®æ ‡
                objective_value = results['test_metrics']['f1']
                
                # è®°å½•ä¼˜åŒ–å†å²
                self.optimization_history.append({
                    'trial': trial.number,
                    'params': params,
                    'value': objective_value
                })
                
                return objective_value
                
            except Exception as e:
                print(f"âš ï¸ Trial {trial.number} å¤±è´¥: {e}")
                return 0.0
        
        # å¼€å§‹ä¼˜åŒ–
        print(f"ğŸ¯ å¼€å§‹ä¼˜åŒ– {model_trainer.model_type} æ¨¡å‹...")
        
        self.study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        # è·å–æœ€ä½³ç»“æœ
        self.best_params = self.study.best_params
        self.best_value = self.study.best_value
        
        print(f"âœ… {model_trainer.model_type} ä¼˜åŒ–å®Œæˆ!")
        print(f"   æœ€ä½³å‚æ•°: {self.best_params}")
        print(f"   æœ€ä½³F1åˆ†æ•°: {self.best_value:.4f}")
        
        return {
            'best_params': self.best_params,
            'best_value': self.best_value,
            'n_trials': len(self.study.trials),
            'optimization_history': self.optimization_history
        }
    
    def get_optimization_history_df(self) -> pd.DataFrame:
        """è·å–ä¼˜åŒ–å†å²çš„DataFrame"""
        if not self.optimization_history:
            return pd.DataFrame()
        
        # å±•å¼€å‚æ•°å­—å…¸
        rows = []
        for record in self.optimization_history:
            row = {'trial': record['trial'], 'value': record['value']}
            row.update(record['params'])
            rows.append(row)
        
        return pd.DataFrame(rows)
    
    def save_results(self, output_dir: str, filename: str = "optuna_results.json"):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        results = {
            'study_name': self.study_name,
            'direction': self.direction,
            'best_params': self.best_params,
            'best_value': self.best_value,
            'n_trials': len(self.study.trials),
            'optimization_history': self.optimization_history
        }
        
        filepath = os.path.join(output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        
        # ä¿å­˜ä¼˜åŒ–å†å²CSV
        history_df = self.get_optimization_history_df()
        if not history_df.empty:
            csv_path = os.path.join(output_dir, "optimization_history.csv")
            history_df.to_csv(csv_path, index=False)
            print(f"ğŸ“Š ä¼˜åŒ–å†å²å·²ä¿å­˜åˆ°: {csv_path}")

def get_multimodal_search_space() -> Dict[str, Any]:
    """è·å–å¤šæ¨¡æ€æ¨¡å‹çš„é»˜è®¤æœç´¢ç©ºé—´"""
    return {
        'learning_rate': {
            'type': 'float',
            'low': 1e-5,
            'high': 1e-2,
            'log': True
        },
        'hidden_dim': {
            'type': 'categorical',
            'choices': [64, 128, 256, 512]
        },
        'num_heads': {
            'type': 'categorical',
            'choices': [4, 8, 16]
        },
        'num_layers': {
            'type': 'int',
            'low': 2,
            'high': 8
        },
        'dropout': {
            'type': 'float',
            'low': 0.1,
            'high': 0.5
        },
        'batch_size': {
            'type': 'categorical',
            'choices': [16, 32, 64, 128]
        },
        'weight_decay': {
            'type': 'float',
            'low': 1e-6,
            'high': 1e-3,
            'log': True
        }
    }

def get_random_forest_search_space() -> Dict[str, Any]:
    """è·å–éšæœºæ£®æ—çš„æœç´¢ç©ºé—´"""
    return {
        'n_estimators': {
            'type': 'int',
            'low': 50,
            'high': 300
        },
        'max_depth': {
            'type': 'int',
            'low': 3,
            'high': 20
        },
        'min_samples_split': {
            'type': 'int',
            'low': 2,
            'high': 20
        },
        'min_samples_leaf': {
            'type': 'int',
            'low': 1,
            'high': 10
        },
        'max_features': {
            'type': 'categorical',
            'choices': ['sqrt', 'log2', None]
        }
    }

def get_xgboost_search_space() -> Dict[str, Any]:
    """è·å–XGBoostçš„æœç´¢ç©ºé—´"""
    return {
        'n_estimators': {
            'type': 'int',
            'low': 50,
            'high': 300
        },
        'max_depth': {
            'type': 'int',
            'low': 3,
            'high': 10
        },
        'learning_rate': {
            'type': 'float',
            'low': 0.01,
            'high': 0.3,
            'log': True
        },
        'subsample': {
            'type': 'float',
            'low': 0.6,
            'high': 1.0
        },
        'colsample_bytree': {
            'type': 'float',
            'low': 0.6,
            'high': 1.0
        },
        'reg_alpha': {
            'type': 'float',
            'low': 0.0,
            'high': 1.0
        },
        'reg_lambda': {
            'type': 'float',
            'low': 0.0,
            'high': 1.0
        }
    }

def run_optuna_tuning(model_type: str,
                     data: Dict[str, Any],
                     trainer,
                     output_dir: str,
                     n_trials: int = 50,
                     custom_search_space: Dict[str, Any] = None,
                     wandb_logger=None) -> Dict[str, Any]:
    """
    è¿è¡ŒOptunaè¶…å‚æ•°è°ƒä¼˜
    
    Args:
        model_type: æ¨¡å‹ç±»å‹ ("multimodal", "random_forest", "xgboost")
        data: æ•°æ®
        trainer: è®­ç»ƒå™¨
        output_dir: è¾“å‡ºç›®å½•
        n_trials: è¯•éªŒæ¬¡æ•°
        custom_search_space: è‡ªå®šä¹‰æœç´¢ç©ºé—´
        wandb_logger: WandBè®°å½•å™¨
        
    Returns:
        ä¼˜åŒ–ç»“æœ
    """
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = OptunaOptimizer(
        study_name=f"{model_type}_tuning_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        direction="maximize"
    )
    
    # è·å–æœç´¢ç©ºé—´
    if custom_search_space:
        search_space = custom_search_space
    elif model_type == "multimodal":
        search_space = get_multimodal_search_space()
    elif model_type == "random_forest":
        search_space = get_random_forest_search_space()
    elif model_type == "xgboost":
        search_space = get_xgboost_search_space()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    # è¿è¡Œä¼˜åŒ–
    if model_type == "multimodal":
        results = optimizer.optimize_multimodal_model(
            data, trainer, search_space, n_trials, wandb_logger=wandb_logger
        )
    else:
        results = optimizer.optimize_traditional_model(
            data, trainer, search_space, n_trials
        )
    
    # ä¿å­˜ç»“æœ
    optimizer.save_results(output_dir, f"{model_type}_optuna_results.json")
    
    return results 