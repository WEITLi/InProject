#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved Traditional Machine Learning Baseline Models
æ”¹è¿›ç‰ˆä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, 
                           f1_score, precision_recall_curve, auc, average_precision_score,
                           precision_score, recall_score, accuracy_score, make_scorer)
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import xgboost as xgb
import shap
from typing import Dict, Any, Tuple, List, Optional
import pickle
import os
import logging
from itertools import combinations
import warnings
warnings.filterwarnings('ignore')

class ImprovedBaselineModelTrainer:
    """æ”¹è¿›ç‰ˆä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_type: str = "random_forest", random_state: int = 42):
        """
        åˆå§‹åŒ–æ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹è®­ç»ƒå™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ("random_forest" æˆ– "xgboost")
            random_state: éšæœºç§å­
        """
        self.model_type = model_type
        self.random_state = random_state
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_names = None
        self.best_params = None
        
        # ä¸åœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºæ¨¡å‹ï¼Œè€Œæ˜¯åœ¨å‚æ•°ä¼˜åŒ–ååˆ›å»º
        
    def extract_rf_features(self, multimodal_data: Dict[str, Any]) -> pd.DataFrame:
        """
        ä¸ºRandom Forestæå–ç‰¹å¾ - ä½¿ç”¨æ›´å¤šåŸå§‹ç‰¹å¾å’Œäº¤äº’ç‰¹å¾
        
        Args:
            multimodal_data: å¤šæ¨¡æ€æ•°æ®å­—å…¸
            
        Returns:
            ç‰¹å¾DataFrame
        """
        features_list = []
        user_ids = multimodal_data.get('users', [])
        
        print("ğŸŒ² ä¸ºRandom Forestæå–ä¸°å¯Œç‰¹å¾...")
        
        # 1. è¯¦ç»†çš„è¡Œä¸ºåºåˆ—ç‰¹å¾
        if 'behavior_sequences' in multimodal_data:
            behavior_data = multimodal_data['behavior_sequences']
            if isinstance(behavior_data, np.ndarray) and len(behavior_data.shape) == 3:
                for i, user_id in enumerate(user_ids):
                    if i < behavior_data.shape[0]:
                        user_sequence = behavior_data[i]  # [sequence_length, feature_dim]
                        
                        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
                        features = {
                            'user_id': user_id,
                            # å…¨å±€ç»Ÿè®¡
                            'seq_mean': np.mean(user_sequence),
                            'seq_std': np.std(user_sequence),
                            'seq_max': np.max(user_sequence),
                            'seq_min': np.min(user_sequence),
                            'seq_median': np.median(user_sequence),
                            'seq_q25': np.percentile(user_sequence, 25),
                            'seq_q75': np.percentile(user_sequence, 75),
                            'seq_iqr': np.percentile(user_sequence, 75) - np.percentile(user_sequence, 25),
                            'seq_skew': self._calculate_skewness(user_sequence.flatten()),
                            'seq_kurtosis': self._calculate_kurtosis(user_sequence.flatten()),
                            'seq_range': np.max(user_sequence) - np.min(user_sequence),
                            
                            # æ´»åŠ¨æ¨¡å¼ç‰¹å¾
                            'seq_activity_rate': np.mean(user_sequence > 0),
                            'seq_zero_ratio': np.mean(user_sequence == 0),
                            'seq_high_activity_ratio': np.mean(user_sequence > np.mean(user_sequence)),
                            
                            # å˜å¼‚æ€§ç‰¹å¾
                            'seq_cv': np.std(user_sequence) / (np.mean(user_sequence) + 1e-8),
                            'seq_mad': np.mean(np.abs(user_sequence - np.median(user_sequence))),
                        }
                        
                        # æ—¶é—´åºåˆ—ç‰¹å¾ - æ›´è¯¦ç»†
                        daily_activity = np.mean(user_sequence, axis=1)
                        features.update({
                            'daily_activity_mean': np.mean(daily_activity),
                            'daily_activity_std': np.std(daily_activity),
                            'daily_activity_max': np.max(daily_activity),
                            'daily_activity_min': np.min(daily_activity),
                            'daily_activity_trend': self._calculate_trend(daily_activity),
                            'daily_activity_autocorr': self._calculate_autocorr(daily_activity),
                            'peak_activity_day': np.argmax(daily_activity),
                            'activity_consistency': 1 / (1 + np.std(daily_activity)),
                            'activity_burst_count': self._count_bursts(daily_activity),
                            'activity_plateau_count': self._count_plateaus(daily_activity),
                        })
                        
                        # ç‰¹å¾ç»´åº¦çº§åˆ«çš„ç»Ÿè®¡
                        for dim in range(min(user_sequence.shape[1], 10)):  # é™åˆ¶ç»´åº¦æ•°é‡
                            dim_data = user_sequence[:, dim]
                            features.update({
                                f'dim_{dim}_mean': np.mean(dim_data),
                                f'dim_{dim}_std': np.std(dim_data),
                                f'dim_{dim}_max': np.max(dim_data),
                                f'dim_{dim}_trend': self._calculate_trend(dim_data),
                            })
                        
                        features_list.append(features)
        
        # 2. ç»“æ„åŒ–ç‰¹å¾ - æ›´è¯¦ç»†
        if 'structured_features' in multimodal_data:
            structured_data = multimodal_data['structured_features']
            if isinstance(structured_data, np.ndarray):
                for i, user_id in enumerate(user_ids):
                    if i < structured_data.shape[0]:
                        user_features = structured_data[i]
                        
                        struct_features = {
                            'struct_mean': np.mean(user_features),
                            'struct_std': np.std(user_features),
                            'struct_max': np.max(user_features),
                            'struct_min': np.min(user_features),
                            'struct_median': np.median(user_features),
                            'struct_nonzero_count': np.count_nonzero(user_features),
                            'struct_sparsity': 1 - (np.count_nonzero(user_features) / len(user_features)),
                            'struct_entropy': self._calculate_entropy(user_features),
                            'struct_energy': np.sum(user_features ** 2),
                        }
                        
                        if i < len(features_list):
                            features_list[i].update(struct_features)
                        else:
                            struct_features['user_id'] = user_id
                            features_list.append(struct_features)
        
        # 3. å›¾ç‰¹å¾
        if 'node_features' in multimodal_data:
            node_features = multimodal_data['node_features']
            if isinstance(node_features, np.ndarray):
                for i, user_id in enumerate(user_ids):
                    if i < node_features.shape[0]:
                        user_node_features = node_features[i]
                        
                        graph_features = {
                            'node_feature_mean': np.mean(user_node_features),
                            'node_feature_std': np.std(user_node_features),
                            'node_centrality_proxy': np.sum(user_node_features),
                            'node_feature_max': np.max(user_node_features),
                            'node_feature_energy': np.sum(user_node_features ** 2),
                        }
                        
                        if i < len(features_list):
                            features_list[i].update(graph_features)
                        else:
                            graph_features['user_id'] = user_id
                            features_list.append(graph_features)
        
        # 4. æ–‡æœ¬ç‰¹å¾
        if 'text_content' in multimodal_data:
            text_data = multimodal_data['text_content']
            if isinstance(text_data, list):
                for i, user_id in enumerate(user_ids):
                    if i < len(text_data):
                        user_text = text_data[i] if text_data[i] else ""
                        
                        text_features = {
                            'text_length': len(user_text),
                            'text_word_count': len(user_text.split()) if user_text else 0,
                            'text_char_diversity': len(set(user_text.lower())) if user_text else 0,
                            'text_avg_word_length': np.mean([len(word) for word in user_text.split()]) if user_text else 0,
                            'text_sentence_count': user_text.count('.') + user_text.count('!') + user_text.count('?'),
                            'text_uppercase_ratio': sum(1 for c in user_text if c.isupper()) / (len(user_text) + 1),
                        }
                        
                        if i < len(features_list):
                            features_list[i].update(text_features)
                        else:
                            text_features['user_id'] = user_id
                            features_list.append(text_features)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶åˆ›å»ºäº¤äº’ç‰¹å¾
        if features_list:
            df = pd.DataFrame(features_list)
            df = df.fillna(0)
            
            # ä¸ºRandom Forestæ·»åŠ äº¤äº’ç‰¹å¾
            print("ğŸ”— ä¸ºRandom Foreståˆ›å»ºäº¤äº’ç‰¹å¾...")
            df = self._create_interaction_features(df)
            
            return df
        else:
            return pd.DataFrame()
    
    def extract_xgb_features(self, multimodal_data: Dict[str, Any]) -> pd.DataFrame:
        """
        ä¸ºXGBoostæå–ç‰¹å¾ - åˆ©ç”¨å…¶å¯¹ç¼ºå¤±å€¼å’Œç‰¹å¾é€‰æ‹©çš„ä¼˜åŠ¿
        
        Args:
            multimodal_data: å¤šæ¨¡æ€æ•°æ®å­—å…¸
            
        Returns:
            ç‰¹å¾DataFrame
        """
        features_list = []
        user_ids = multimodal_data.get('users', [])
        
        print("ğŸš€ ä¸ºXGBoostæå–ä¼˜åŒ–ç‰¹å¾...")
        
        # 1. ä¿ç•™åŸå§‹ç‰¹å¾ï¼ˆXGBoostèƒ½è‡ªåŠ¨å¤„ç†ç‰¹å¾é€‰æ‹©ï¼‰
        if 'behavior_sequences' in multimodal_data:
            behavior_data = multimodal_data['behavior_sequences']
            if isinstance(behavior_data, np.ndarray) and len(behavior_data.shape) == 3:
                for i, user_id in enumerate(user_ids):
                    if i < behavior_data.shape[0]:
                        user_sequence = behavior_data[i]
                        
                        # åŸºç¡€ç‰¹å¾ï¼ˆè®©XGBoostè‡ªå·±é€‰æ‹©é‡è¦çš„ï¼‰
                        features = {
                            'user_id': user_id,
                            'seq_mean': np.mean(user_sequence),
                            'seq_std': np.std(user_sequence),
                            'seq_max': np.max(user_sequence),
                            'seq_min': np.min(user_sequence),
                            'seq_median': np.median(user_sequence),
                            'seq_skew': self._calculate_skewness(user_sequence.flatten()),
                            'seq_kurtosis': self._calculate_kurtosis(user_sequence.flatten()),
                            'seq_activity_rate': np.mean(user_sequence > 0),
                            'seq_zero_ratio': np.mean(user_sequence == 0),
                            'seq_range': np.max(user_sequence) - np.min(user_sequence),
                        }
                        
                        # æ—¶é—´åºåˆ—ç‰¹å¾
                        daily_activity = np.mean(user_sequence, axis=1)
                        features.update({
                            'daily_activity_mean': np.mean(daily_activity),
                            'daily_activity_std': np.std(daily_activity),
                            'daily_activity_trend': self._calculate_trend(daily_activity),
                            'peak_activity_day': np.argmax(daily_activity),
                            'activity_consistency': 1 / (1 + np.std(daily_activity)),
                        })
                        
                        # æ·»åŠ åŸå§‹åºåˆ—çš„å±•å¹³ç‰ˆæœ¬ï¼ˆéƒ¨åˆ†ï¼‰- XGBoostèƒ½å¤„ç†é«˜ç»´
                        flattened = user_sequence.flatten()
                        for j in range(min(len(flattened), 50)):  # é™åˆ¶ç‰¹å¾æ•°é‡
                            features[f'raw_seq_{j}'] = flattened[j]
                        
                        # æ•…æ„å¼•å…¥ä¸€äº›ç¼ºå¤±å€¼ï¼ˆæµ‹è¯•XGBoostçš„ç¼ºå¤±å€¼å¤„ç†èƒ½åŠ›ï¼‰
                        if np.random.random() < 0.1:  # 10%çš„æ¦‚ç‡
                            features['seq_std'] = np.nan
                        if np.random.random() < 0.05:  # 5%çš„æ¦‚ç‡
                            features['daily_activity_trend'] = np.nan
                        
                        features_list.append(features)
        
        # 2. ç»“æ„åŒ–ç‰¹å¾ï¼ˆä¿æŒç®€æ´ï¼Œè®©XGBoostè‡ªåŠ¨é€‰æ‹©ï¼‰
        if 'structured_features' in multimodal_data:
            structured_data = multimodal_data['structured_features']
            if isinstance(structured_data, np.ndarray):
                for i, user_id in enumerate(user_ids):
                    if i < structured_data.shape[0]:
                        user_features = structured_data[i]
                        
                        struct_features = {
                            'struct_mean': np.mean(user_features),
                            'struct_std': np.std(user_features),
                            'struct_max': np.max(user_features),
                            'struct_min': np.min(user_features),
                            'struct_nonzero_count': np.count_nonzero(user_features),
                            'struct_sparsity': 1 - (np.count_nonzero(user_features) / len(user_features)),
                        }
                        
                        # æ·»åŠ åŸå§‹ç»“æ„åŒ–ç‰¹å¾
                        for j, val in enumerate(user_features[:20]):  # é™åˆ¶æ•°é‡
                            struct_features[f'struct_raw_{j}'] = val
                        
                        if i < len(features_list):
                            features_list[i].update(struct_features)
                        else:
                            struct_features['user_id'] = user_id
                            features_list.append(struct_features)
        
        # 3. å…¶ä»–ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if 'node_features' in multimodal_data:
            node_features = multimodal_data['node_features']
            if isinstance(node_features, np.ndarray):
                for i, user_id in enumerate(user_ids):
                    if i < node_features.shape[0]:
                        user_node_features = node_features[i]
                        
                        graph_features = {
                            'node_feature_mean': np.mean(user_node_features),
                            'node_feature_std': np.std(user_node_features),
                            'node_centrality_proxy': np.sum(user_node_features),
                        }
                        
                        if i < len(features_list):
                            features_list[i].update(graph_features)
                        else:
                            graph_features['user_id'] = user_id
                            features_list.append(graph_features)
        
        if 'text_content' in multimodal_data:
            text_data = multimodal_data['text_content']
            if isinstance(text_data, list):
                for i, user_id in enumerate(user_ids):
                    if i < len(text_data):
                        user_text = text_data[i] if text_data[i] else ""
                        
                        text_features = {
                            'text_length': len(user_text),
                            'text_word_count': len(user_text.split()) if user_text else 0,
                            'text_char_diversity': len(set(user_text.lower())) if user_text else 0,
                            'text_avg_word_length': np.mean([len(word) for word in user_text.split()]) if user_text else 0,
                        }
                        
                        if i < len(features_list):
                            features_list[i].update(text_features)
                        else:
                            text_features['user_id'] = user_id
                            features_list.append(text_features)
        
        # è½¬æ¢ä¸ºDataFrameï¼ˆä¸åˆ›å»ºäº¤äº’ç‰¹å¾ï¼Œè®©XGBoostè‡ªå·±å­¦ä¹ ï¼‰
        if features_list:
            df = pd.DataFrame(features_list)
            # å¯¹äºXGBoostï¼Œä¸å¡«å……ç¼ºå¤±å€¼ï¼Œè®©å®ƒè‡ªå·±å¤„ç†
            return df
        else:
            return pd.DataFrame()
    
    def _create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¸ºRandom Foreståˆ›å»ºäº¤äº’ç‰¹å¾"""
        if 'user_id' in df.columns:
            df_features = df.drop('user_id', axis=1)
        else:
            df_features = df.copy()
        
        # é€‰æ‹©æœ€é‡è¦çš„å‡ ä¸ªç‰¹å¾è¿›è¡Œäº¤äº’
        important_features = ['seq_mean', 'seq_std', 'daily_activity_mean', 'struct_mean', 'seq_activity_rate']
        available_features = [f for f in important_features if f in df_features.columns]
        
        if len(available_features) >= 2:
            # åˆ›å»ºäºŒé˜¶äº¤äº’ç‰¹å¾
            for i, feat1 in enumerate(available_features):
                for feat2 in available_features[i+1:]:
                    # ä¹˜ç§¯äº¤äº’
                    df[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]
                    # æ¯”å€¼äº¤äº’ï¼ˆé¿å…é™¤é›¶ï¼‰
                    df[f'{feat1}_div_{feat2}'] = df[feat1] / (df[feat2] + 1e-8)
        
        return df
    
    def _calculate_skewness(self, data: np.ndarray) -> float:
        """è®¡ç®—ååº¦"""
        if len(data) == 0 or np.std(data) == 0:
            return 0
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 3)
    
    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """è®¡ç®—å³°åº¦"""
        if len(data) == 0 or np.std(data) == 0:
            return 0
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 4) - 3
    
    def _calculate_trend(self, data: np.ndarray) -> float:
        """è®¡ç®—è¶‹åŠ¿ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰"""
        if len(data) < 2:
            return 0
        x = np.arange(len(data))
        return np.polyfit(x, data, 1)[0]
    
    def _calculate_autocorr(self, data: np.ndarray, lag: int = 1) -> float:
        """è®¡ç®—è‡ªç›¸å…³"""
        if len(data) <= lag:
            return 0
        return np.corrcoef(data[:-lag], data[lag:])[0, 1] if not np.isnan(np.corrcoef(data[:-lag], data[lag:])[0, 1]) else 0
    
    def _count_bursts(self, data: np.ndarray, threshold_factor: float = 1.5) -> int:
        """è®¡ç®—æ´»åŠ¨çªå‘æ¬¡æ•°"""
        if len(data) == 0:
            return 0
        threshold = np.mean(data) * threshold_factor
        return np.sum(data > threshold)
    
    def _count_plateaus(self, data: np.ndarray, tolerance: float = 0.1) -> int:
        """è®¡ç®—å¹³å°æœŸæ¬¡æ•°"""
        if len(data) < 3:
            return 0
        diff = np.abs(np.diff(data))
        plateau_mask = diff < tolerance
        # è®¡ç®—è¿ç»­å¹³å°æœŸçš„æ•°é‡
        plateaus = 0
        in_plateau = False
        for is_plateau in plateau_mask:
            if is_plateau and not in_plateau:
                plateaus += 1
                in_plateau = True
            elif not is_plateau:
                in_plateau = False
        return plateaus
    
    def _calculate_entropy(self, data: np.ndarray) -> float:
        """è®¡ç®—ç†µ"""
        if len(data) == 0:
            return 0
        # å°†æ•°æ®ç¦»æ•£åŒ–
        hist, _ = np.histogram(data, bins=10)
        hist = hist[hist > 0]  # ç§»é™¤é›¶å€¼
        if len(hist) == 0:
            return 0
        prob = hist / np.sum(hist)
        return -np.sum(prob * np.log2(prob))
    
    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """
        ä¼˜åŒ–è¶…å‚æ•°
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾
            
        Returns:
            æœ€ä½³å‚æ•°å­—å…¸
        """
        print(f"ğŸ”§ ä¸º {self.model_type} ä¼˜åŒ–è¶…å‚æ•°...")
        
        if self.model_type == "random_forest":
            # Random Forestå‚æ•°ç½‘æ ¼
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None],
                'bootstrap': [True, False]
            }
            
            base_model = RandomForestClassifier(
                random_state=self.random_state,
                n_jobs=-1
            )
            
        elif self.model_type == "xgboost":
            # XGBoostå‚æ•°ç½‘æ ¼ - æ›´æ¿€è¿›çš„å‚æ•°
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [6, 8, 10, 12],  # æ›´æ·±çš„æ ‘
                'learning_rate': [0.1, 0.2, 0.3],  # æ›´é«˜çš„å­¦ä¹ ç‡
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 0.5],
                'reg_lambda': [1, 1.5, 2],
            }
            
            base_model = xgb.XGBClassifier(
                random_state=self.random_state,
                n_jobs=-1,
                eval_metric='logloss'
            )
        
        # ä½¿ç”¨åˆ†å±‚äº¤å‰éªŒè¯è¿›è¡Œç½‘æ ¼æœç´¢
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        # ä½¿ç”¨F1ä½œä¸ºè¯„åˆ†æ ‡å‡†ï¼ˆäºŒåˆ†ç±»ï¼‰
        grid_search = GridSearchCV(
            base_model,
            param_grid,
            cv=cv,
            scoring='f1',  # äºŒåˆ†ç±»F1
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X, y)
        
        print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"âœ… æœ€ä½³CV F1åˆ†æ•°: {grid_search.best_score_:.4f}")
        
        self.best_params = grid_search.best_params_
        self.model = grid_search.best_estimator_
        
        return grid_search.best_params_
    
    def prepare_data(self, multimodal_data: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ® - ä½¿ç”¨å·®å¼‚åŒ–ç‰¹å¾æå–
        
        Args:
            multimodal_data: å¤šæ¨¡æ€æ•°æ®
            
        Returns:
            (X, y, feature_names)
        """
        # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„ç‰¹å¾æå–æ–¹æ³•
        if self.model_type == "random_forest":
            features_df = self.extract_rf_features(multimodal_data)
        elif self.model_type == "xgboost":
            features_df = self.extract_xgb_features(multimodal_data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        if features_df.empty:
            raise ValueError("æ— æ³•æå–ç‰¹å¾ï¼Œæ•°æ®å¯èƒ½ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
        
        # è·å–æ ‡ç­¾
        labels = multimodal_data.get('labels', [])
        if len(labels) == 0:
            raise ValueError("ç¼ºå°‘æ ‡ç­¾æ•°æ®")
        
        # ç¡®ä¿ç‰¹å¾å’Œæ ‡ç­¾æ•°é‡åŒ¹é…
        min_samples = min(len(features_df), len(labels))
        features_df = features_df.iloc[:min_samples]
        labels = labels[:min_samples]
        
        # ç§»é™¤user_idåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'user_id' in features_df.columns:
            features_df = features_df.drop('user_id', axis=1)
        
        # è·å–ç‰¹å¾åç§°
        self.feature_names = list(features_df.columns)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = features_df.values
        y = np.array(labels)
        
        # å¯¹äºRandom Forestï¼Œè¿›è¡Œæ ‡å‡†åŒ–ï¼›å¯¹äºXGBoostï¼Œä¿æŒåŸå§‹å€¼
        if self.model_type == "random_forest":
            X = self.scaler.fit_transform(X)
        elif self.model_type == "xgboost":
            # XGBoostä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œä½†éœ€è¦å¤„ç†æ— ç©·å€¼
            X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)
        
        return X, y, self.feature_names
    
    def train_with_cv(self, multimodal_data: Dict[str, Any], cv_folds: int = 5) -> Dict[str, Any]:
        """
        ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹
        
        Args:
            multimodal_data: å¤šæ¨¡æ€æ•°æ®
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        # å‡†å¤‡æ•°æ®
        X, y, feature_names = self.prepare_data(multimodal_data)
        
        print(f"ğŸ“Š æ•°æ®å‡†å¤‡å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
        
        # æ£€æŸ¥ç±»åˆ«å¹³è¡¡
        if len(np.unique(y)) < 2:
            raise ValueError("æ ‡ç­¾ä¸­åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è¿›è¡ŒäºŒåˆ†ç±»")
        
        # è¶…å‚æ•°ä¼˜åŒ–
        best_params = self.optimize_hyperparameters(X, y)
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œäº¤å‰éªŒè¯
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        # å®šä¹‰è¯„ä¼°æŒ‡æ ‡
        scoring = {
            'accuracy': 'accuracy',
            'f1': 'f1',  # äºŒåˆ†ç±»F1
            'precision': 'precision',  # äºŒåˆ†ç±»ç²¾ç¡®ç‡
            'recall': 'recall',  # äºŒåˆ†ç±»å¬å›ç‡
            'roc_auc': 'roc_auc',
            'average_precision': 'average_precision'  # PR-AUC
        }
        
        print(f"ğŸ”„ å¼€å§‹ {cv_folds} æŠ˜äº¤å‰éªŒè¯...")
        cv_results = cross_validate(
            self.model, X, y,
            cv=cv,
            scoring=scoring,
            return_train_score=True,
            n_jobs=-1
        )
        
        # è®¡ç®—äº¤å‰éªŒè¯ç»Ÿè®¡
        cv_stats = {}
        for metric in scoring.keys():
            test_scores = cv_results[f'test_{metric}']
            train_scores = cv_results[f'train_{metric}']
            
            cv_stats[f'{metric}_test_mean'] = np.mean(test_scores)
            cv_stats[f'{metric}_test_std'] = np.std(test_scores)
            cv_stats[f'{metric}_train_mean'] = np.mean(train_scores)
            cv_stats[f'{metric}_train_std'] = np.std(train_scores)
        
        # åœ¨å…¨éƒ¨æ•°æ®ä¸Šé‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        print("ğŸš€ åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        self.model.fit(X, y)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = self._get_feature_importance()
        
        results = {
            'model_type': self.model_type,
            'best_params': best_params,
            'cv_results': cv_stats,
            'feature_importance': feature_importance,
            'feature_names': feature_names,
            'n_samples': X.shape[0],
            'n_features': X.shape[1],
            'class_distribution': np.bincount(y).tolist()
        }
        
        # æ‰“å°ç»“æœ
        print(f"âœ… {self.model_type} äº¤å‰éªŒè¯å®Œæˆ")
        print(f"   æµ‹è¯•é›† F1: {cv_stats['f1_test_mean']:.4f} Â± {cv_stats['f1_test_std']:.4f}")
        print(f"   æµ‹è¯•é›† AUC: {cv_stats['roc_auc_test_mean']:.4f} Â± {cv_stats['roc_auc_test_std']:.4f}")
        print(f"   æµ‹è¯•é›† PR-AUC: {cv_stats['average_precision_test_mean']:.4f} Â± {cv_stats['average_precision_test_std']:.4f}")
        print(f"   æµ‹è¯•é›† ç²¾ç¡®ç‡: {cv_stats['precision_test_mean']:.4f} Â± {cv_stats['precision_test_std']:.4f}")
        print(f"   æµ‹è¯•é›† å¬å›ç‡: {cv_stats['recall_test_mean']:.4f} Â± {cv_stats['recall_test_std']:.4f}")
        
        return results
    
    def _get_feature_importance(self) -> np.ndarray:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if hasattr(self.model, 'feature_importances_'):
            return self.model.feature_importances_
        else:
            return np.zeros(len(self.feature_names))
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'model_type': self.model_type,
            'best_params': self.best_params
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ æ”¹è¿›æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

def run_improved_baseline_comparison(multimodal_data: Dict[str, Any], 
                                   output_dir: str,
                                   models: List[str] = None,
                                   cv_folds: int = 5) -> Dict[str, Any]:
    """
    è¿è¡Œæ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹å¯¹æ¯”å®éªŒ
    
    Args:
        multimodal_data: å¤šæ¨¡æ€æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
        models: è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
        cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
        
    Returns:
        å¯¹æ¯”ç»“æœ
    """
    if models is None:
        models = ["random_forest", "xgboost"]
    
    results = {}
    
    for model_type in models:
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ æµ‹è¯•æ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹: {model_type}")
        print(f"{'='*60}")
        
        try:
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = ImprovedBaselineModelTrainer(model_type=model_type)
            
            # ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹
            model_results = trainer.train_with_cv(multimodal_data, cv_folds=cv_folds)
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(output_dir, f"improved_{model_type}_model.pkl")
            trainer.save_model(model_path)
            
            # ä¿å­˜ç»“æœ
            results[model_type] = model_results
            
        except Exception as e:
            print(f"âŒ {model_type} æ”¹è¿›æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results[model_type] = {'error': str(e)}
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š æ”¹è¿›ç‰ˆåŸºçº¿æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print(f"{'='*60}")
    
    for model_type, result in results.items():
        if 'error' not in result:
            cv_results = result['cv_results']
            print(f"\nğŸ”¬ {model_type.upper()}:")
            print(f"   F1 Score: {cv_results['f1_test_mean']:.4f} Â± {cv_results['f1_test_std']:.4f}")
            print(f"   ROC-AUC:  {cv_results['roc_auc_test_mean']:.4f} Â± {cv_results['roc_auc_test_std']:.4f}")
            print(f"   PR-AUC:   {cv_results['average_precision_test_mean']:.4f} Â± {cv_results['average_precision_test_std']:.4f}")
            print(f"   ç²¾ç¡®ç‡:   {cv_results['precision_test_mean']:.4f} Â± {cv_results['precision_test_std']:.4f}")
            print(f"   å¬å›ç‡:   {cv_results['recall_test_mean']:.4f} Â± {cv_results['recall_test_std']:.4f}")
            print(f"   ç‰¹å¾æ•°:   {result['n_features']}")
        else:
            print(f"\nâŒ {model_type.upper()}: {result['error']}")
    
    return results 