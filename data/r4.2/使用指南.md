# CERT r4.2 数据集特征提取与异常检测使用指南

## 概述

本指南介绍如何使用CERT r4.2数据集进行内部威胁检测，包括特征提取、时间表示转换和异常检测的完整流水线。

## 目录结构

```
r4.2/
├── feature_extraction.py              # 主要特征提取脚本
├── temporal_data_representation_fixed.py  # 时间表示转换脚本
├── anomaly_detection_r4.2.py         # 异常检测脚本
├── demo_anomaly_detection.py         # 演示脚本
├── DataByWeek/                        # 按周合并的原始数据
├── NumDataByWeek/                     # 数值化特征数据
├── ExtractedData/                     # 最终特征CSV文件
└── *.pkl                             # 时间表示特征文件
```

## 步骤1: 特征提取

### 基本语法
```bash
python feature_extraction.py <CPU核心数> <开始周> <结束周> <最大用户数> <模式> <子会话>
```

### 参数说明
- **CPU核心数**: 并行处理的核心数量（建议8-16）
- **开始周**: 处理的起始周数（通常从0开始）
- **结束周**: 处理的结束周数（r4.2最大73周）
- **最大用户数**: 限制处理的用户数量（或'all'处理所有用户）
- **模式**: 时间粒度选择
  - `"week"`: 只生成周级别特征
  - `"day"`: 只生成日级别特征  
  - `"session"`: 只生成会话级别特征
  - `"week,day,session"`: 生成所有级别特征
- **子会话**: 是否生成子会话特征（0或1）

### 实际示例

#### 快速测试（小数据集）
```bash
# 处理前5周，200个用户，生成所有类型特征
python feature_extraction.py 8 0 5 200 "week,day,session" 1
```

#### 中等规模处理
```bash
# 处理前20周，500个用户，只生成day和session特征
python feature_extraction.py 16 0 20 500 "day,session" 1
```

#### 完整数据集处理
```bash
# 处理所有73周，所有用户，所有特征
python feature_extraction.py 16 0 73 all "week,day,session" 1
```

### 输出文件
- **DataByWeek/**: `0.pickle`, `1.pickle`, ... (按周原始数据)
- **ExtractedData/**: 
  - `weekr4.2_<配置>.csv`: 周级别特征
  - `dayr4.2_<配置>.csv`: 日级别特征
  - `sessionr4.2_<配置>.csv`: 会话级别特征
  - `session<类型><参数>r4.2_<配置>.csv`: 子会话特征

## 步骤2: 时间表示转换（可选）

时间表示转换可以将静态特征转换为捕捉时间模式的特征，适用于时序异常检测。

### 支持的转换类型

1. **percentile**: 百分位排名（推荐用于异常检测）
2. **concat**: 特征连接（适用于序列学习）
3. **meandiff**: 均值差异
4. **meddiff**: 中位数差异

### 使用示例

```bash
# 生成百分位特征（7天窗口）
python temporal_data_representation_fixed.py \
  --representation percentile \
  --file_input ExtractedData/dayr4.2_u200_w0-19_mweekdaysession_s1.csv \
  --window_size 7

# 生成连接特征（3个时间点）
python temporal_data_representation_fixed.py \
  --representation concat \
  --file_input ExtractedData/sessionr4.2_u200_w0-19_mweekdaysession_s1.csv \
  --num_concat 3

# 生成所有类型的时间表示
python temporal_data_representation_fixed.py \
  --representation all \
  --file_input ExtractedData/dayr4.2_u200_w0-19_mweekdaysession_s1.csv \
  --window_size 14 \
  --num_concat 5
```

### 输出文件
- `文件名-percentile<窗口>.pkl`: 百分位特征
- `文件名-concat<连接数>.pkl`: 连接特征
- `文件名-meandiff<窗口>.pkl`: 均值差异特征
- `文件名-meddiff<窗口>.pkl`: 中位数差异特征

## 步骤3: 异常检测

使用自编码器方法进行无监督异常检测。

### 基本用法

```bash
# 使用原始特征进行异常检测
python anomaly_detection_r4.2.py \
  --input ExtractedData/dayr4.2_u500_w0-30_mweekdaysession_s1.csv \
  --plot

# 使用时间表示特征进行异常检测
python anomaly_detection_r4.2.py \
  --input dayr4.2_u500_w0-30_mweekdaysession_s1-percentile7.pkl \
  --plot

# 自定义参数
python anomaly_detection_r4.2.py \
  --input 输入文件.pkl \
  --test_ratio 0.3 \
  --max_iter 100 \
  --output_dir results/ \
  --plot
```

### 参数说明
- `--input`: 输入数据文件（CSV或PKL）
- `--test_ratio`: 测试集比例（默认0.5）
- `--max_iter`: 训练迭代次数（默认25）
- `--output_dir`: 输出目录（默认当前目录）
- `--plot`: 是否生成可视化图表

### 输出结果
- **控制台输出**: AUC分数、检测率、精确率
- **图表文件**: `anomaly_detection_results.png`
  - 重构误差分布图
  - ROC曲线（如有异常样本）或统计信息

## 步骤4: 演示脚本

运行演示脚本可以自动测试多种数据类型：

```bash
python demo_anomaly_detection.py
```

演示脚本会：
1. 扫描所有可用的数据文件
2. 自动选择不同类型的文件进行测试
3. 生成比较报告和可视化结果

## 步骤5: 清理生成的数据 (安全清理脚本)

为了方便清理特征提取过程中生成的大量数据文件和文件夹，可以使用 `cleanup_data.py` 脚本。这个脚本会扫描并列出可以清理的文件和目录，并提供了安全删除选项。

### 基本语法

```bash
python cleanup_data.py [options]
```

### 参数说明

- `--dry-run`: 模拟运行，只显示将要删除的文件，不实际执行删除操作。
- `--force`: 强制删除，跳过确认提示，直接执行删除。使用此选项请务必小心。
- `--exclude <filename1> <filename2> ...`: 排除指定的文件，不进行删除。可以指定多个文件名，用空格分隔。

### 清理范围

脚本会扫描当前目录下（即 `Huawei/Anomaly_Detection/InProject/data/r4.2/`）并清理以下内容：
- `DataByWeek/` 文件夹及其所有内容
- `NumDataByWeek/` 文件夹及其所有内容
- `ExtractedData/` 文件夹及其所有内容
- 当前目录下的所有 `.pkl` 文件，但会**保留**在脚本内部 `exclude_files` 列表中指定的重要 `.pkl` 文件（例如示例数据文件）。

### 使用示例

#### 模拟清理 (强烈推荐先进行模拟运行)
```bash
python cleanup_data.py --dry-run
```
运行此命令会列出所有符合清理条件的文件和文件夹，但不会真正删除它们。

#### 交互式清理 (带确认提示)
```bash
python cleanup_data.py
```
运行此命令后，脚本会先扫描并列出可清理项，然后会询问你是否确认删除。输入 `y` 或 `yes` 确认，输入 `n` 或 `no` 取消。

#### 强制清理 (无确认提示)
```bash
python cleanup_data.py --force
```
此命令会直接执行删除操作，**不会询问确认**。请谨慎使用！

#### 清理时排除特定文件
```bash
python cleanup_data.py --exclude week-r5.2-percentile30.pkl another_important_file.pkl
```
此命令会清理除 `week-r5.2-percentile30.pkl` 和 `another_important_file.pkl` 之外的可清理文件。

### 注意事项
- 脚本默认在当前目录运行，请确保你在 `Huawei/Anomaly_Detection/InProject/data/r4.2/` 目录下执行脚本。
- 删除操作不可逆，请在执行清理前仔细检查模拟运行结果。

## 最佳实践

### 1. 数据规模建议
- **开发测试**: 0-5周，200用户
- **中等实验**: 0-20周，500用户  
- **完整研究**: 0-73周，全部用户

### 2. 特征选择建议
- **快速实验**: 使用day或session级别特征
- **深度分析**: 结合week、day、session三个粒度
- **时序检测**: 使用percentile时间表示

### 3. 异常检测参数调优
- **训练样本少**: 增加`max_iter`到100-200
- **数据不平衡**: 调整`test_ratio`到0.3-0.7
- **内存不足**: 减少用户数量和周数范围

### 4. 性能优化
- **并行处理**: 使用多核CPU（8-16核）
- **内存管理**: 分批处理大数据集
- **缓存利用**: 复用已生成的中间文件

## 常见问题

### Q: 没有检测到异常样本怎么办？
A: 
1. 扩大时间范围（增加周数）
2. 增加用户数量
3. 检查数据集中的恶意用户分布
4. 调整训练/测试分割比例

### Q: 内存不足如何处理？
A:
1. 减少用户数量限制
2. 缩短处理的时间范围
3. 使用单一模式而非多模式处理
4. 关闭子会话生成

### Q: 处理速度太慢？
A:
1. 增加CPU核心数
2. 使用SSD存储
3. 减少特征维度（只选择必要的活动类型）
4. 分阶段处理（先生成基础特征，再做时间表示）

### Q: 如何比较不同方法的效果？
A:
1. 使用相同的数据集和参数
2. 比较AUC分数和检测率
3. 分析重构误差分布
4. 使用交叉验证验证结果稳定性

## 配置文件命名规则

生成的文件使用标准化命名：
```
<类型><数据集>_u<用户数>_w<开始周>-<结束周>_m<模式>_s<子会话>.csv
```

例如：
- `dayr4.2_u200_w0-19_mweekdaysession_s1.csv`
- `sessionr4.2_u500_w0-50_msession_s0.csv`

时间表示文件：
```
<原文件名>-<表示类型><参数>.pkl
```

例如：
- `dayr4.2_u200_w0-19_mweekdaysession_s1-percentile7.pkl`
- `sessionr4.2_u500_w0-50_msession_s0-concat3.pkl`

## 结论

这个完整的流水线提供了从原始CERT数据到异常检测结果的端到端解决方案。通过调整不同的参数和方法，可以适应各种研究需求和计算资源约束。

建议从小规模数据开始实验，验证流程正确性后再扩展到完整数据集。 